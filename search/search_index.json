{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Obsidian Notes","text":"<p>Publish your public notes with MkDocs</p>"},{"location":"#hello-world","title":"Hello World!","text":"<p>The <code>index.md</code> in the <code>/docs</code> folder is the homepage you see here.</p> <p>The folders in <code>/docs</code> appear as the main sections on the navigation bar.</p> <p>The notes appear as pages within these sections. For example, [[Note 1]] in <code>Topic 1</code></p>"},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/","title":"Neural Network Compression (Quantization ) for NLP","text":""},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/#task","title":"Task:","text":"<ul> <li>Language modeling </li> <li>Name entity recognition </li> <li>NMT (translation) </li> <li>Question answering</li> <li>Sentiment analysis </li> <li>Speech recognition </li> <li>Textual entailment</li> <li>Word similarity </li> </ul>"},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/#model","title":"Model:","text":"<p>Before transformer </p> <ul> <li>LSTM, GRU, RNN based model</li> </ul> <p>After transformer</p> <ul> <li>Bert-base (encoder), Transformer based</li> <li>LLM </li> </ul>"},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/#quantization-method","title":"Quantization Method:","text":""},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/#ptq","title":"PTQ:","text":"<p>A quantization technique that is applied after the training process is completed. </p> <p>This means the model is first trained using high-precision (e.g., 32-bit) floating-point numbers, and then the weights of the trained model are quantized to lower precision, such as 8-bit integers.</p>"},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/#qat","title":"QAT:","text":"<p>A method where the quantization process is incorporated into the training stage itself. </p> <p>This means the model is aware of the quantization process during training, and the loss function is modified to take into account the quantization error.</p>"},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/#common-setting","title":"Common Setting:","text":"<p>8-bit quantization -&gt; classic setting </p> <p>lower-bit quantization -&gt; binary, ternary </p>"},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/#post-training-quantization-for-large-language-model","title":"Post Training Quantization for Large Language Model","text":""},{"location":"Neural%20Network%20Compression%20%28Quantization%20%29%20for%20NLP/#some-takeaway","title":"some takeaway","text":"<ol> <li> <p>asymmetric quantization &gt; symmetric quantization </p> </li> <li> <p>WQ (weight-only quantization) &gt; WAQ (weight and activation quantization) </p> </li> <li> <p>Robustness of Weight-only Quantization for Large Models</p> </li> <li> <p>Challenge of Activation Quantization for Large Models</p> </li> </ol> <p>Quantization Method: (comparison in Microsoft paper. Full experiments)</p> <ol> <li>RTN -&gt; naive round-to-nearest baseline</li> <li>GPTQ  https://arxiv.org/pdf/2210.17323.pdf</li> <li>ZeroQuant  https://arxiv.org/pdf/2206.01861.pdf</li> </ol> <p>Other methods:</p> <ol> <li> <p>Smooth Quant https://arxiv.org/pdf/2211.10438.pdf</p> </li> <li> <p>LLM.int8() https://arxiv.org/pdf/2208.07339.pdf</p> </li> <li> <p>RPTQ https://arxiv.org/pdf/2304.01089.pdf</p> </li> </ol> <p>A Comprehensive Study on Post-Training Quantization for Large Language Models</p> <p>Microsoft </p> <p>https://arxiv.org/pdf/2303.08302.pdf</p>"},{"location":"Paper%20Todo%20List/","title":"Paper Todo List","text":""},{"location":"Paper%20Todo%20List/#pre-transformer","title":"Pre-Transformer","text":"<p>TODO</p>"},{"location":"Paper%20Todo%20List/#transformer","title":"Transformer","text":""},{"location":"Paper%20Todo%20List/#pre-llm","title":"Pre-LLM:","text":"<p>TODO </p>"},{"location":"Paper%20Todo%20List/#next","title":"Next:","text":"<ol> <li>I-BERT: Integer-only BERT Quantization https://arxiv.org/pdf/2101.01321.pdf</li> <li>Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT https://ojs.aaai.org/index.php/AAAI/article/view/6409</li> <li>Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model https://arxiv.org/pdf/1906.00532.pdf</li> </ol>"},{"location":"Paper%20Todo%20List/#llm","title":"LLM:","text":""},{"location":"Paper%20Todo%20List/#finished","title":"Finished:","text":"<ol> <li>Survery of PTQ for LLM (Microsoft): https://arxiv.org/pdf/2303.08302.pdf </li> <li>GPTQ https://arxiv.org/pdf/2210.17323.pdf</li> <li>ZeroQuant https://arxiv.org/pdf/2206.01861.pdf</li> <li>LLM.int8() https://arxiv.org/pdf/2208.07339.pdf</li> <li>RPTQ https://arxiv.org/pdf/2304.01089.pdf</li> <li>SmoothQuant https://arxiv.org/pdf/2211.10438.pdf</li> </ol>"},{"location":"Paper%20Todo%20List/#next_1","title":"Next:","text":"<ol> <li>Intriguing Properties of Quantization at Scale (Cohere): publish yesterday https://arxiv.org/pdf/2305.19268v1.pdf</li> </ol> <p>QAT:  1. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models https://arxiv.org/pdf/2305.17888.pdf</p> <p>Finetuning:  1. QLORA: Efficient Finetuning of Quantized LLMs:  https://arxiv.org/pdf/2305.14314.pdf</p>"},{"location":"draft/","title":"Introduction","text":"<p>Natural Language Processing (NLP) has witnessed dramatic progress over the last few decades, moving from simple rule-based systems to complex deep learning models capable of understanding and generating human language with impressive accuracy. As these models have evolved, so too has the need for efficient ways to deploy them. One such method is quantization, a technique used to reduce the computational and memory requirements of these models without significantly sacrificing their performance.</p> <p>Quantization in NLP involves converting the continuous weights of a model into discrete counterparts. This reduces the memory footprint and computational requirements, making the models faster and more efficient, particularly important for deployment on devices with limited resources. The process, however, is non-trivial as it poses a challenge: maintaining the balance between the model's efficiency and its performance.</p> <p>This document explores the evolution of quantization techniques in NLP, focusing on the era before and after the introduction of Transformer models. It further dissects the timeline after the advent of Large Language Models (LLMs), as these massive models pose their unique challenges and opportunities for quantization.</p> <p>We will delve into the various quantization techniques and their evolution, performance, and associated research, starting from pre-Transformer NLP models like Long Short-Term Memory (LSTM) to Transformer-based models like BERT, and moving towards the recent and more complex LLMs. The document also includes exploration of some notable studies and research papers for a detailed understanding of these techniques.</p> <p>By providing an organized compilation of these techniques and their chronological evolution, this document aims to offer a comprehensive guide to the landscape of quantization in NLP. The goal is not just to reflect on the strides that have been made in the field, but also to anticipate future trends and opportunities. Let's embark on this journey through the past, present, and potential future of quantization methods in NLP.</p>"},{"location":"draft/#pre-transformer","title":"Pre-Transformer","text":"<p>In the Pre-Transformer era, several methods were used to process and understand text data. Techniques such as Bag of Words (BoW), TF-IDF, and n-grams were early text representation methods, which were then significantly enhanced by the introduction of word embeddings like Word2Vec and GloVe. The introduction of Recurrent Neural Networks (RNNs), and more specifically, architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), brought about a revolutionary change in the NLP field, enabling models to understand the sequence and context in language data.</p>"},{"location":"draft/#model-structure","title":"Model Structure","text":""},{"location":"draft/#rnn","title":"RNN","text":"<p>List out RNN </p>"},{"location":"draft/#lstm-in-nlp","title":"LSTM in NLP","text":"<p>LSTM, a special kind of RNN, was capable of learning long-term dependencies, which was a significant leap forward from the simple, feed-forward networks. It was particularly effective in applications such as language modeling, machine translation, text generation, and named entity recognition.</p> <ul> <li>\"Long Short-Term Memory\", Hochreiter &amp; Schmidhuber (1997): The paper introduced the LSTM architecture. Link</li> <li>\"Sequence to Sequence Learning with Neural Networks\", Sutskever et al. (2014): This paper demonstrated the application of LSTMs in sequence-to-sequence tasks, providing the backbone for numerous NLP tasks. Link</li> </ul>"},{"location":"draft/#limitations-of-lstm","title":"Limitations of LSTM","text":"<p>Despite LSTM's effectiveness in understanding sequence and context, it came with its own set of challenges. The recurrent nature of LSTM made it computationally expensive and slow to train. Furthermore, LSTM's memory cell could capture dependencies over a certain range but struggled with very long sequences, leading to difficulties in learning overall structure in the data.</p>"},{"location":"draft/#quantization-methods-application","title":"Quantization Methods Application","text":"<p>As LSTM models and other neural networks grew in complexity, the need for memory-efficient and computationally efficient models became evident. This need led to the exploration of quantization methods, which aimed at reducing the precision of the weights in a neural network, hence reducing the memory footprint and computational requirements.</p> <p>Some of the early quantization methods that came into the picture  K-means Quantization, and Binary Quantization. Here we divided the section in binary/ternary quantization and general k-bit quantization </p>"},{"location":"draft/#binary-and-ternary-quantization","title":"Binary and Ternary Quantization","text":"<ul> <li>Binary quantization uses only 1 bit to represent weights. Methods include deterministic binarization, stochastic binarization, and loss-aware binarization. However, binarization generally leads to large drops in accuracy.</li> <li>Ternary quantization uses 2 bits to represent weights as -1, 0, or +1. Methods include simple threshold-based ternarization, trainable ternarization, and hybrid schemes like HitNets. Ternary quantization works better than binary for many models.</li> </ul>"},{"location":"draft/#general-k-bit-quantization","title":"General k-bit Quantization","text":"<ul> <li>Uniform quantization divides the weight range into equal intervals. It is easy to implement but suboptimal for non-uniform weight distributions.</li> <li>Balanced quantization partitions weights into equal sized bins based on percentiles. This better utilizes the full range.</li> <li>K-means clustering is used in techniques like product quantization and residual quantization to perform non-uniform quantization.</li> <li>Loss-aware quantization directly optimizes the quantization parameters to minimize the overall network loss. This works better than static post-training quantization.</li> </ul>"},{"location":"draft/#results","title":"Results","text":"<p>Ref. Compression of Deep Learning Models for Text: A Survey[https://arxiv.org/pdf/2008.05221.pdf]</p>"},{"location":"draft/#summary","title":"Summary","text":"<ul> <li>Binary quantization does not work well for text models like RNNs and LSTMs. It leads to exploding/vanishing gradients.</li> <li>Ternary quantization with 2 bits provides a good balance of compression and accuracy for text models. Hybrid schemes like HitNets combine ternary and binary quantization across layers.</li> <li>For RNNs, threshold-based ternary quantization works better for weights while stochastic ternary quantization works better for activations based on their distribution.</li> <li>Quantizing both embeddings and weights during training works better than post-training quantization for text models.</li> <li>Loss-aware quantization optimized during training outperforms static post-training quantization for text models.</li> </ul> <p>In summary, quantization can effectively compress text models but RNNs/LSTMs require more than 1-bit quantization. Ternary quantization works well for general text models while mixed-precision schemes optimized during training give best results for Transformers. This will be discussed in the transformer era section. </p>"},{"location":"draft/#transformer-era","title":"Transformer Era","text":"<p>The introduction of the Transformer architecture in 2017 marked a turning point in the field of NLP. Vaswani et al. proposed this novel architecture in the paper \"Attention is All You Need\" as a remedy to the limitations faced by its RNN predecessors like LSTM and GRU. The advent of Transformer models in NLP introduced architectures with much higher complexity and significantly larger parameter space compared to their predecessors. </p>"},{"location":"draft/#introduction-of-transformer-architecture","title":"Introduction of Transformer Architecture","text":"<p>Transformer models deviate from the recurrent nature of LSTMs and GRUs, introducing the concept of \"attention\" which allows the model to focus on different parts of the input sequence when producing an output. This attention mechanism, specifically the self-attention or scaled dot-product attention, enables Transformers to handle long-term dependencies effectively and makes them highly parallelizable, speeding up training times significantly.</p> <p>However, despite these improvements, Transformer models, due to their increased complexity and large number of parameters, require substantial computational resources. This has implications for their deployment, especially in resource-constrained environments.</p> <p>Reference </p> <ul> <li>\"Attention is All You Need\", Vaswani et al. (2017): The paper that introduced the Transformer model. Link</li> </ul> <p>Impact on Quantization Methods</p> <p>With the advent of Transformer models, the need for efficient quantization methods became even more pronounced. The large number of parameters in these models necessitated the development of novel quantization techniques that could reduce the model size without significantly affecting performance.</p>"},{"location":"draft/#emergence-of-bert-and-its-implications","title":"Emergence of BERT and Its Implications","text":"<p>One notable Transformer-based model is BERT (Bidirectional Encoder Representations from Transformers), which revolutionized NLP by providing significant improvements in performance across various tasks. BERT's large size, particularly in its larger variants, presented further challenges for deployment, providing an impetus for the development of specific quantization techniques for Transformer models.</p> <p>Reference </p> <ul> <li>\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", Devlin et al. (2018): The original paper introducing BERT. Link</li> </ul>"},{"location":"draft/#quantization-techniques-for-transformers","title":"Quantization Techniques for Transformers","text":"<p>Before the advent of LLMs, several quantization techniques were proposed to deal with the increased size and complexity of Transformer models. These techniques took into account the specific architecture of these models, optimizing different aspects to balance efficiency and performance.</p> <p>Here are </p>"},{"location":"draft/#i-bert-integer-only-bert-quantization","title":"I-BERT: Integer-only BERT Quantization","text":"<p>I-BERT converts all the floating-point weights in the BERT model into integer values. This method simplifies the hardware requirements and reduces memory usage. I-BERT uses a non-uniform quantization algorithm that performs comparably to full-precision BERT on GLUE benchmark tasks.</p>"},{"location":"draft/#q-bert-hessian-based-ultra-low-precision-quantization-of-bert","title":"Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT","text":"<p>Q-BERT introduces Hessian-based quantization, which uses second-order information to identify optimal bit-width allocation across layers. This method ensures efficient use of memory while maintaining a competitive performance.</p>"},{"location":"draft/#efficient-8-bit-quantization-of-transformer-neural-machine-language-translation-model","title":"Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model","text":"<p>This paper proposes an 8-bit quantization method specific to Transformer models, reducing the model size and accelerating the inference speed without significant performance drop. This method is particularly effective for NMT (Neural Machine Translation) tasks.</p>"},{"location":"draft/#mkq-bert-quantized-bert-with-4bits-weights-and-activations","title":"MKQ-BERT: QUANTIZED BERT WITH 4BITS WEIGHTS AND ACTIVATIONS","text":""},{"location":"draft/#binarybert-pushing-the-limit-of-bert-quantization","title":"BinaryBERT: Pushing the Limit of BERT Quantization","text":"<p>Reference</p> <ul> <li> <p>\"Integer-only BERT Quantization\", Kim et al. (2021): Link</p> </li> <li> <p>\"Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT\", Shen et al. (2020): Link</p> </li> <li>\"Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model\", Lin et al. (2019): Link</li> </ul>"},{"location":"draft/#summary_1","title":"Summary","text":"<p>Each of these methods has its strengths and weaknesses. For instance, I-BERT and Q-BERT can reduce model size significantly, but they might experience a slight performance degradation. On the other hand, the efficient 8-bit quantization method for Transformer models achieves good balance between model size reduction and performance preservation, but it may not achieve as high a compression ratio as the other two methods.</p> <p>Ultimately, the choice between these methods depends on the specific needs of the application, including available resources and performance requirements.</p> <p>Despite the introduction of quantization, these methods have demonstrated that it's possible to maintain a competitive performance level. For example, I-BERT achieves performance comparable to full-precision BERT on the GLUE benchmark, Q-BERT shows only minimal performance degradation across several NLP tasks, and the 8-bit quantization approach maintains strong performance in NMT tasks.</p> <ul> <li>For Transformers, mixed precision quantization with different bits for different layers/heads works better than blanket quantization.</li> <li>Word embeddings can be quantized to 2-4 bits with minimal impact on downstream tasks.</li> <li>Overall, ternary quantization provides a good tradeoff for general text models. Loss-aware mixed precision quantization gives state-of-the-art results for Transformers like BERT.</li> <li>The Transformer architecture is highly robust to quantization compared to other text models like RNNs.</li> <li>more keypoints....</li> </ul> <p>In summary, the Transformer era brought about a new set of challenges and opportunities in quantization methods for NLP. As these models continue to evolve and increase in complexity, the development and refinement of quantization methods remain an active area of research.</p>"},{"location":"draft/#llm-era","title":"LLM Era","text":"<p>LLM, standed for Large Language Model, refers to a category of NLP models that have a large number of parameters. They are trained on extensive text corpora and are capable of understanding context, generating human-like text, and demonstrating a deep understanding of many different topics.</p> <p>The term 'Large' in LLMs is indicative of the sheer size of these models in terms of their parameter counts. For instance, GPT-3, one of the largest models to date, has 175 billion parameters. This significant increase in parameters has resulted in models with much higher complexity and accuracy compared to their predecessors, leading to a new era in NLP, often referred to as the 'LLM Era'.</p> <p>The LLM Era is generally considered to have begun around the time when models started to scale beyond a billion parameters, which happened around 2018-2019 with the advent of models like GPT-2, MegatronLM, and subsequently, GPT-3.</p> <p>Nowadays, with the rapid incresed popularity of chatgpt, there are a list of LLMs which contains  hundreds of billions of parameters come up in this field such as Jurassic-1, Megatron-Turing, LaMDA, BLOOM, Claude, and so on. </p> <p>Their large size allows them to capture more nuanced patterns in the data and generate more contextual outputs. They have been shown to perform exceptionally well on a range of NLP tasks, from translation to text generation, and even tasks they were not explicitly trained on, a phenomenon known as 'zero-shot' learning.</p> <p>However, the larger the model, the more computational resources it requires. This includes both the resources needed to train the model and to use the model for inference. The large size also makes it challenging to deploy these models on resource-constrained devices, such as mobile phones or embedded systems. This is why quantization methods have become increasingly important in the LLM Era, as they allow for the reduction of model size and computational requirements while attempting to maintain high model performance.</p>"},{"location":"draft/#general-quantization-setup-for-llm","title":"General quantization setup for LLM","text":"<p>A general quantization setup for language model can be divided into PTQ and QAT.</p>"},{"location":"draft/#post-training-quantization-ptq","title":"Post-Training Quantization (PTQ):","text":"<p>Post-training quantization, as the name suggests, is a technique where quantization is applied after the model has been fully trained using high-precision floating point weights and activations. The key steps in PTQ are:</p> <ul> <li>Train the model to completion using floating point (32-bit or higher precision) numbers. This allows the model to converge to an accurate set of weights and activations.</li> <li>Analyze the distribution of weights and activations after training to select optimal quantization parameters like bit-widths, clipping values etc.</li> <li>Quantize the weights and activations by replacing the floating point values with low-precision integer equivalents using the selected quantization parameters. For example, converting 32-bit floats to 8-bit integers.</li> <li>Fine-tune the quantized model to recover any small loss in accuracy. This fine-tuning is usually fast and uses low learning rates.</li> </ul> <p>The benefits of post-training quantization include simplicity and convenience, since quantization is treated as an after-thought. But the drawback is that there can be a larger loss in model accuracy compared to quantization-aware training.</p>"},{"location":"draft/#quantization-aware-training-qat","title":"Quantization-Aware Training (QAT):","text":"<p>In QAT, the model is aware of quantization and its effects right from the start of training. The key aspects are:</p> <ul> <li>Quantization operations like rounding and clamping are simulated during training. This allows the model to learn parameters that are robust to quantization loss.</li> <li>The training loss function is modified to include quantization error as a term, so the model can directly learn to minimize this error.</li> <li>Quantization parameters like bit-widths are dynamically learned during training by observing activation distributions.</li> <li>Batch normalization folding merges batch norm parameters into weights before quantization.</li> </ul>"},{"location":"draft/#ptq-or-qat-which-one-is-better","title":"PTQ or QAT? Which one is better?","text":"<p>The benefits of QAT include better accuracy compared to PTQ, and the ability to learn optimal quantization bit-widths. The challenges include increased training time and complexity. Thus in the context of LLM, receby research has favored post-training quantization (PTQ) over quantization-aware training (QAT) for compressing large language models (LLMs). PTQ allows quantizing a pre-trained floating point model to lower precisions without additional training. In contrast, QAT jointly trains the model to learn lower precision weights and activations. While QAT can achieve better accuracy, it requires full re-training which is prohibitively expensive for large models. For example, training a model like GPT-3 with 175 billion parameters on thousands of GPUs for months would need to be repeated for QAT. PTQ avoids this computational burden by quantizing the already trained model with minimal data and tuning. Consequently, PTQ has emerged as a more practical solution for deploying production LLMs with reduced precision. The focus is now on developing PTQ methods that can maximize compression without significant accuracy loss.</p>"},{"location":"draft/#quantization-method-for-llm","title":"Quantization Method for LLM:","text":"<p>Recent research has explored techniques to optimize large language models (LLMs) using post-training quantization to reduce model size and computational costs. A comprehensive study by Yao et al. (2023) evaluates different quantization schemes and granularities using methods like GPTQ and ZeroQuant across a range of model sizes.</p>"},{"location":"draft/#gptq","title":"GPTQ","text":"<p>GPTQ introduces a new quantization method specifically designed for GPT models. This technique ensures that quantization can be applied without significant loss of performance.</p>"},{"location":"draft/#zeroquant","title":"ZeroQuant","text":"<p>ZeroQuant presents a novel approach to quantization that focuses on zero-valued weights in neural networks. By doing so, it enables more efficient use of memory and computational resources.</p>"},{"location":"draft/#llmint8","title":"LLM.int8()","text":"<p>This paper proposes a method to reduce the precision of weights in LLMs to 8 bits, which allows for substantial reduction in the memory footprint of these models.</p>"},{"location":"draft/#rptq","title":"RPTQ","text":"<p>RPTQ offers a method for Re-training Post-training Quantization, a two-step process that first quantizes the model weights, and then retrains the quantized model to restore the performance.</p>"},{"location":"draft/#smoothquant","title":"SmoothQuant","text":"<p>SmoothQuant introduces a new approach to quantization, which uses probabilistic rounding schemes to maintain high performance while reducing the memory and computational needs.</p> <p>Reference</p> <ul> <li> <p>\"GPTQ: Quantization for Large Language Models\", Sharma et al. (2022): Link</p> </li> <li> <p>\"ZeroQuant: Rethinking Quantization for Large Language Models\", Shen et al. (2022): Link</p> </li> <li>\"LLM.int8(): Low Precision Large Language Models\", Bhandare et al. (2022): Link</li> <li>\"Re-training Post-training Quantization for Large Language Models\", Zhang et al. (2023): Link)</li> <li>\"SmoothQuant: Smoothing the path to low-precision large language models\", Ambroladze et al. (2022): Link</li> </ul>"},{"location":"draft/#summary_2","title":"Summary","text":"<p>These quantization methods offer a variety of approaches to reducing the resource requirements of LLMs, making these models more accessible for deployment on various hardware platforms. As these models continue to evolve and increase in size and complexity, the need for efficient and effective quantization methods will only continue to grow.</p> <p>Some key findings in comparison between all methods:</p> <ul> <li> <ul> <li>Activation quantization is more challenging than weight quantization, with smaller models showing better tolerance.</li> </ul> </li> <li>Different model families behave differently - BLOOM has no divergence issues but OPT struggles beyond 6.7B parameters.</li> <li>Current methods can't achieve full model quality with 4-bit weights or 4-bit weights + 8-bit activations.</li> <li>Fine-grained quantization recovers model quality for &gt;13B models but still not optimal.</li> <li>LoRC provides substantial gains, especially for lower bit precision, and recovers close to full model quality.</li> </ul> <p>NOTICE THERE ARE MORE PAPER IN THE TODO LIST.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/A_zero-shot_learning_application_in_deep_drawing_process_using_hyper-process_model/","title":"A zero-shot learning application in deep drawing process using hyper-process model","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/A_zero-shot_learning_application_in_deep_drawing_process_using_hyper-process_model/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Zero-Shot Learning based approach, known as hyper-process model (HPM), to learn the relation among multiple tasks in order to shorten the machine calibration phase in high-mix/low-volume production. This approach enables the generation of new process models for process optimization, and defines a regression problem in the domain of ZSL. The results indicate that the method can learn new tasks without any available data, thereby speeding up the calibration phase and allowing quicker integration of new products into manufacturing systems.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/A_zero-shot_learning_application_in_deep_drawing_process_using_hyper-process_model/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/A_zero-shot_learning_application_in_deep_drawing_process_using_hyper-process_model/#content","title":"Content","text":"<p>One of the consequences of passing from mass production to mass customization paradigm in the nowadays industrialized world is the need to increase flexibility and responsiveness of manufacturing companies. The high-mix / low-volume production forces constant accommodations of unknown product variants, which ultimately leads to high periods of machine calibration. Therefore, a Zero-Shot Learning (ZSL) based approach called hyper-process model (HPM) to learn the relation among multiple tasks is used as a way to shorten the calibration phase. Ultimately, the present work has two main contributions: 1) Formulation of an industrial problem into a ZSL setting where new process models can be generated for process optimization and 2) the definition of a regression problem in the domain of ZSL. The obtained results show that is possible to learn new tasks without any available data (both labeled and unlabeled) by leveraging information about already existing tasks, allowing to speed up the calibration phase and make a quicker integration of new products into manufacturing systems."},{"location":"Quantization-Survey-Paper-Notes/any_task/Adapt%3A_fast_emulation_of_approximate_dnn_accelerators_in_pytorch/","title":"Adapt: fast emulation of approximate dnn accelerators in pytorch","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Adapt%3A_fast_emulation_of_approximate_dnn_accelerators_in_pytorch/#summary","title":"Summary","text":"<p>The paper introduces AdaPT, a PyTorch-based emulation framework that enables approximate inference and approximation-aware retraining. AdaPT is shown to be compatible with various DNN models and multipliers with different bitwidth values. The results demonstrated significant error recovery from approximate retraining and a reduced inference time of up to 53.9% compared to the baseline approximate implementation."},{"location":"Quantization-Survey-Paper-Notes/any_task/Adapt%3A_fast_emulation_of_approximate_dnn_accelerators_in_pytorch/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Adapt%3A_fast_emulation_of_approximate_dnn_accelerators_in_pytorch/#content","title":"Content","text":"<p> In this paper, the authors present AdaPT, a fast emulation framework that extends PyTorch to support approximate inference as well as approximation-aware retraining. AdaPT can be seamlessly deployed and is compatible with the most DNNs. The authors evaluate the framework on several DNN models and application fields including CNNs, LSTMs, and GANs for a number of approximate multipliers with distinct bitwidth values. The results show substantial error recovery from approximate retraining and reduced inference time up to 53:9 with respect to the baseline approximate implementation."},{"location":"Quantization-Survey-Paper-Notes/any_task/BppAttack%3A_Stealthy_and_Efficient_Trojan_Attacks_against_Deep_Neural_Networks_via_Image_Quantization_and_Contrastive_Adversarial_Learning/","title":"BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/BppAttack%3A_Stealthy_and_Efficient_Trojan_Attacks_against_Deep_Neural_Networks_via_Image_Quantization_and_Contrastive_Adversarial_Learning/#summary","title":"Summary","text":"<p>Summary: This research paper proposes a new stealthy and efficient Trojan attack method called BPPATTACK which uses image quantization and dithering as the trigger, and a contrastive learning approach to improve precision. The attack bypasses existing Trojan defenses and human inspection with high success rates.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/BppAttack%3A_Stealthy_and_Efficient_Trojan_Attacks_against_Deep_Neural_Networks_via_Image_Quantization_and_Contrastive_Adversarial_Learning/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/BppAttack%3A_Stealthy_and_Efficient_Trojan_Attacks_against_Deep_Neural_Networks_via_Image_Quantization_and_Contrastive_Adversarial_Learning/#content","title":"Content","text":"<p> In this research paper, the authors propose a new Trojan attack method called BPPATTACK that is both stealthy and efficient. By exploiting human visual system vulnerabilities, the trigger of the attack is imperceptible to human inspection. The authors propose using image quantization and dithering as the Trojan trigger, without the need for training auxiliary models. To improve the attack's precision, the authors propose a contrastive learning based approach that leverages adversarial attacks to generate negative sample pairs. Evaluation on four benchmark datasets shows that the proposed method achieves high attack success rates and effectively bypasses existing Trojan defenses and human inspection."},{"location":"Quantization-Survey-Paper-Notes/any_task/Confounding_tradeoffs_for_neural_network_quantization/","title":"Confounding tradeoffs for neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Confounding_tradeoffs_for_neural_network_quantization/#summary","title":"Summary","text":"<p>Summary: The authors analyze various neural network quantization techniques to reduce computational and memory usage in deep learning. They find that tradeoffs associated with these techniques have a greater impact on network accuracy than the quantization methods themselves. To help researchers compare methods and determine the effectiveness of quantization techniques for their hardware, the authors propose the use of \"quantization cards\" to report design decisions.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Confounding_tradeoffs_for_neural_network_quantization/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Confounding_tradeoffs_for_neural_network_quantization/#content","title":"Content","text":"<p>  In this research paper, the authors discuss the tradeoffs associated with various neural network quantization techniques that are developed to decrease the computational and memory footprint of deep learning. The authors empirically analyze the impact of these confounding tradeoffs on different use cases and find that they may have a larger impact on quantized network accuracy than the actual quantization methods. To address this problem, the authors propose the use of \"quantization cards\" to report these design decisions, which can help researchers compare methods more effectively and determine the applicability of quantization techniques for their hardware."},{"location":"Quantization-Survey-Paper-Notes/any_task/Conversion_of_synchronous_artificial_neural_network_to_asynchronous_spiking_neural_network_using_sigma-delta_quantization/","title":"Conversion of synchronous artificial neural network to asynchronous spiking neural network using sigma-delta quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Conversion_of_synchronous_artificial_neural_network_to_asynchronous_spiking_neural_network_using_sigma-delta_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method to convert artificial neural networks into spiking neural networks that use asynchronous spikes for communication, which reduces power consumption and allows scalability. The proposed method outperforms previous implementations in terms of accuracy and energy consumption. The focus is on introducing a straightforward and hardware-efficient method to implement ANNs asynchronously.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Conversion_of_synchronous_artificial_neural_network_to_asynchronous_spiking_neural_network_using_sigma-delta_quantization/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Conversion_of_synchronous_artificial_neural_network_to_asynchronous_spiking_neural_network_using_sigma-delta_quantization/#content","title":"Content","text":"<p>Artificial Neural Networks (ANNs) have great potential for data analysis tasks, but their direct implementation requires high processing power, consumes a lot of energy, and suffers from scalability issues. Asynchronous processing and communication, inspired by biology, is a promising solution to reduce power consumption and allow scalability. In this work, we propose a simple and efficient method to convert an ANN to a Spiking Neural Network (SNN) which uses asynchronous spikes to communicate the quantized output activations of neurons. This method outperforms previous implementations in terms of accuracy and energy consumption. Asynchronous processing is a key feature of scalable neuromorphic platforms, which can be easily scaled up without losing performance. Our focus is to introduce a hardware-efficient and straightforward method to implement an ANN in an asynchronous way."},{"location":"Quantization-Survey-Paper-Notes/any_task/Deep_learning_methods_for_joint_optimization_of_beamforming_and_fronthaul_quantization_in_cloud_radio_access_networks/","title":"Deep learning methods for joint optimization of beamforming and fronthaul quantization in cloud radio access networks","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Deep_learning_methods_for_joint_optimization_of_beamforming_and_fronthaul_quantization_in_cloud_radio_access_networks/#summary","title":"Summary","text":"<p> This paper investigates a deep learning approach to optimize cooperative beamforming and quantization strategies in cloud radio access network (C-RAN) systems. A well-trained deep neural network (DNN) is used to replace the optimization module to reduce computational complexity. The proposed learning solution produces a low-dimensional representation of optimal beamforming and quantization strategies, and numerical results show advantages over iterative algorithms."},{"location":"Quantization-Survey-Paper-Notes/any_task/Deep_learning_methods_for_joint_optimization_of_beamforming_and_fronthaul_quantization_in_cloud_radio_access_networks/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Deep_learning_methods_for_joint_optimization_of_beamforming_and_fronthaul_quantization_in_cloud_radio_access_networks/#content","title":"Content","text":"<p> Cooperative beamforming across access points (APs) and fro nthaul quantization strategies are essential for cloud radio access network (C-RAN) systems. The nonconvexity of the C-RAN optimization problems, which is stemmed from per-AP power and fronthaul c apacity constraints, requires high computational complexity for executing iterative algorit hms. To resolve this issue, we investigate a deep learning approach where the optimization module is rep laced with a well-trained deep neural network (DNN). An ef\ufb01cient learning solution is proposed wh ich constructs a DNN to produce a low-dimensional representation of optimal beamforming an d quantization strategies. Numerical results validate the advantages of the proposed learning solution."},{"location":"Quantization-Survey-Paper-Notes/any_task/Effective_quantization_approaches_for_recurrent_neural_networks/","title":"Effective quantization approaches for recurrent neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Effective_quantization_approaches_for_recurrent_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper proposes effective quantization approaches such as Binary, Ternary and Quaternary Connect for RNN approaches, along with LSTM, GRU, and ConvLSTM techniques. These approaches are evaluated on different datasets for sentiment analysis and video frame prediction and compared against full precision versions, showing promising results.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Effective_quantization_approaches_for_recurrent_neural_networks/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Effective_quantization_approaches_for_recurrent_neural_networks/#content","title":"Content","text":"<p> Deep learning approaches have shown superior accuracy in different application domains; however, they are very expensive in terms of computation. Recurrent Neural Networks (RNNs) are particularly expensive, as they are used in a large variety of tasks, including machine translation, language understanding, and movie frames generation. To address this issue, researchers have proposed energy-efficient RNN approaches for deploying solutions on special-purpose hardware. This paper proposes several effective quantization approaches, including Binary Connect {-1,1}, Ternary Connect {-1,0,1}, and Quaternary Connect {-1,-0.5,0.5,1}, for RNN techniques such as Long Short Term Memory (LSTM), Gated Recurrent Units (GRU), and Convolutional Long Short Term Memory (ConvLSTM). These proposed approaches are evaluated on different datasets for sentiment analysis on IMDB and on video frame predictions on the moving MNIST dataset, and are compared against the full precision versions of the LSTM, GRU, and ConvLSTM. The experimental results show promising results for both sentiment analysis and video frame prediction."},{"location":"Quantization-Survey-Paper-Notes/any_task/Fast_quantized_average_consensus_over_static_and_dynamic_directed_graphs/","title":"Fast quantized average consensus over static and dynamic directed graphs","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Fast_quantized_average_consensus_over_static_and_dynamic_directed_graphs/#summary","title":"Summary","text":"<p>The paper presents an algorithm for distributed average consensus in multi-agent systems with directed communication links, using quantized values and event-driven updates. It allows each agent to reach a fixed state within one quantization level to the average of initial states and can achieve finite-time convergence in dynamic directed communication topologies, with examples to illustrate its operation, performance, and potential advantages."},{"location":"Quantization-Survey-Paper-Notes/any_task/Fast_quantized_average_consensus_over_static_and_dynamic_directed_graphs/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Fast_quantized_average_consensus_over_static_and_dynamic_directed_graphs/#content","title":"Content","text":"<p> In this paper, the problem of distributed average consensus in multi-agent systems with directed communication links is studied. The proposed algorithm operates exclusively with quantized values and relies on event-driven updates, where each node models its initial state as two quantized fractions and transmits one fraction randomly while it keeps the other stored. The algorithm's execution, on any static and strongly connected digraph, allows each agent to reach infinite time a fixed state within one quantisation level to the average of the initial states. Additionally, the operation of the algorithm is extended to achieve finite-time convergence in the presence of a dynamic directed communication topology subject to some connectivity conditions. Finally, there are examples to illustrate the operation, performance, and potential advantages of the proposed algorithm."},{"location":"Quantization-Survey-Paper-Notes/any_task/High_Rate_Communication_over_One-Bit_Quantized_Channels_via_Deep_Learning_and_LDPC_Codes/","title":"High Rate Communication over One-Bit Quantized Channels via Deep Learning and LDPC Codes","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/High_Rate_Communication_over_One-Bit_Quantized_Channels_via_Deep_Learning_and_LDPC_Codes/#summary","title":"Summary","text":"<p>Summary: This paper presents a novel methodology to design efficient error correction codes for one-bit quantization nonlinear channels. It proposes the combination of an LDPC encoder and a trained autoencoder. The LDPC encoder reduces the input space and enables the autoencoder to learn effectively. The method performs well for high-order modulation formats and has been theoretically proven to provide approximately Gaussian distributed data to the LDPC decoder despite the non-Gaussian statistics of the received signal.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/High_Rate_Communication_over_One-Bit_Quantized_Channels_via_Deep_Learning_and_LDPC_Codes/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/High_Rate_Communication_over_One-Bit_Quantized_Channels_via_Deep_Learning_and_LDPC_Codes/#content","title":"Content","text":"<p>This paper proposes a methodology for designing efficient error correction codes for one-bit quantization nonlinear channels by combining an LDPC encoder with a trained autoencoder. The LDPC encoder reduces the input space of the autoencoder and enables it to learn more easily. The proposed error correction code shows promising results for one-bit quantization, even with high-order modulation formats such as 16-QAM and 64-QAM, and has been theoretically proven to provide approximately Gaussian distributed data to the LDPC decoder despite the non-Gaussian statistics of the received signal. The paper also discusses the history of employing neural networks for error correction codes and the contributions of the proposed method."},{"location":"Quantization-Survey-Paper-Notes/any_task/Hybrid_Precoding_Based_on_Non-Uniform_Quantization_Codebook_to_Reduce_Feedback_Overhead_in_Millimeter_Wave_MIMO_Systems/","title":"Hybrid Precoding Based on Non-Uniform Quantization Codebook to Reduce Feedback Overhead in Millimeter Wave MIMO Systems","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Hybrid_Precoding_Based_on_Non-Uniform_Quantization_Codebook_to_Reduce_Feedback_Overhead_in_Millimeter_Wave_MIMO_Systems/#summary","title":"Summary","text":"<p>This paper proposes two non-uniform quantization (NUQ) codebook-based hybrid precoding schemes for millimeter wave MIMO systems. The proposed methods use effective and ineffective coverage angles for quantization and achieve near-optimal spectral efficiencies while reducing feedback overhead compared to uniform quantization (UQ) methods. The paper proposes two low-complexity hybrid analog/digital precoding schemes based on NUQ codebooks. The simulation results show the effectiveness of the proposed methods in reducing feedback overhead."},{"location":"Quantization-Survey-Paper-Notes/any_task/Hybrid_Precoding_Based_on_Non-Uniform_Quantization_Codebook_to_Reduce_Feedback_Overhead_in_Millimeter_Wave_MIMO_Systems/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Hybrid_Precoding_Based_on_Non-Uniform_Quantization_Codebook_to_Reduce_Feedback_Overhead_in_Millimeter_Wave_MIMO_Systems/#content","title":"Content","text":"<p>In this paper, we focus on the design of the hybrid analog/digital precoding in millimeter wave multiple-input multiple-output (MIMO) systems. To reduce the feedback overhead, we propose two non-uniform quantization (NUQ) codebook based hybrid precoding schemes for two main hybrid precoding implementations, i.e., the full-connected structure and the sub-connected structure. Speci\ufb01cally, we \ufb01rstly group the angles of the arrive/departure (AOAs/AODs) of the scattering paths into several spatial lobes by exploiting the sparseness property of the millimeter wave in the angular domain, which divides the total angular domain into effective spatial lobes\u2019 coverage angles and ineffective coverage angles. Then, we map the quantization bits non-uniformly to different coverage angles and construct NUQ codebooks, where high numbers of quantization bits are employed for the effective coverage angles to quantize AoAs/AoDs and zero quantization bit is employed for ineffective coverage angles. Finally, two low-complexity hybrid analog/digital precoding schemes are proposed based on NUQ codebooks. Simulation results demonstrate that, the proposed two NUQ codebook based hybrid precoding schemes achieve near-optimal spectral ef\ufb01ciencies and show the superiority in reducing the feedback overhead compared with the uniform quantization (UQ) codebook based works, e.g., at least 12:5%feedback overhead could be reduced for a system with 144=36transmitting/receiving antennas."},{"location":"Quantization-Survey-Paper-Notes/any_task/Loss-aware_weight_quantization_of_deep_networks/","title":"Loss-aware weight quantization of deep networks","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Loss-aware_weight_quantization_of_deep_networks/#summary","title":"Summary","text":"<p>This paper suggests a method for compressing deep neural networks by weight quantization. They propose a scheme that extends loss-aware weight binarization to ternarization and m-bit quantization. Experimental results demonstrate that this approach outperforms existing weight quantization algorithms while maintaining accuracy comparable to full-precision networks."},{"location":"Quantization-Survey-Paper-Notes/any_task/Loss-aware_weight_quantization_of_deep_networks/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Loss-aware_weight_quantization_of_deep_networks/#content","title":"Content","text":"<p>The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m &gt; 2) quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network."},{"location":"Quantization-Survey-Paper-Notes/any_task/Max%E2%80%93min_rate_of_cell-free_massive_MIMO_uplink_with_optimal_uniform_quantization/","title":"Max\u2013min rate of cell-free massive MIMO uplink with optimal uniform quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Max%E2%80%93min_rate_of_cell-free_massive_MIMO_uplink_with_optimal_uniform_quantization/#summary","title":"Summary","text":"<p> This paper analyzes cell-free massive multiple-input multiple-output (MIMO) in the uplink with optimal uniform quantization. The effect of quantization is modeled using the Bussgang decomposition. The max-min problem is studied and the original problem is decomposed into two sub-problems for receiver filter design and power allocation, respectively. Geometric programming (GP) is used to solve the power allocation problem, a generalized eigenvalue problem is solved for designing the receiver filter, and a user assignment algorithm is proposed to improve performance. Numerical results demonstrate the superiority of the proposed schemes."},{"location":"Quantization-Survey-Paper-Notes/any_task/Max%E2%80%93min_rate_of_cell-free_massive_MIMO_uplink_with_optimal_uniform_quantization/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Max%E2%80%93min_rate_of_cell-free_massive_MIMO_uplink_with_optimal_uniform_quantization/#content","title":"Content","text":"<p>Cell-free Massive multiple-input multiple-output (MIMO) is analyzed in this paper, focusing on the uplink with optimal uniform quantization. The effect of quantization is modeled using the Bussgang decomposition. The max-min problem is studied and the original problem is decomposed into two sub-problems for receiver filter design and power allocation, respectively. Geometric programming (GP) is used to solve the power allocation problem and a generalized eigenvalue problem is solved for designing the receiver filter. A user assignment algorithm is proposed which improves the performance. Numerical results demonstrate the superiority of the proposed schemes."},{"location":"Quantization-Survey-Paper-Notes/any_task/Neural_network_distiller%3A_A_python_package_for_dnn_compression_research/","title":"Neural network distiller: A python package for dnn compression research","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Neural_network_distiller%3A_A_python_package_for_dnn_compression_research/#summary","title":"Summary","text":"<p> This paper introduces Neural Network Distiller, an open-source Python package for DNN compression research. It is a library of DNN compression algorithms implementations, with tools, tutorials, and sample applications for various learning tasks. Target users of this package are both engineers and researchers, and its design-for-extensibility facilitates new research. Distiller is open-source and is available on Github."},{"location":"Quantization-Survey-Paper-Notes/any_task/Neural_network_distiller%3A_A_python_package_for_dnn_compression_research/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Neural_network_distiller%3A_A_python_package_for_dnn_compression_research/#content","title":"Content","text":"<p>This paper presents the philosophy, design and feature-set of Neural Network Distiller, an open-source Python package for DNN compression research. Distiller is a library of DNN compression algorithms implementations, with tools, tutorials and sample applications for various learning tasks. Its target users are both engineers and researchers, and the rich content is complemented by a design-for-extensibility to facilitate new research. Distiller is open-source and is available on Github at https://github.com/NervanaSystems/distiller."},{"location":"Quantization-Survey-Paper-Notes/any_task/On_the_tradeoff_between_energy%2C_precision%2C_and_accuracy_in_federated_quantized_neural_networks/","title":"On the tradeoff between energy, precision, and accuracy in federated quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/On_the_tradeoff_between_energy%2C_precision%2C_and_accuracy_in_federated_quantized_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a framework for deploying federated learning over wireless networks with resource-constrained devices using quantized neural networks (QNNs) to achieve a finite level of precision in both local training and uplink transmission. Energy models for local training and transmission with quantization are derived, and an energy minimization problem is formulated that optimizes the level of precision while ensuring convergence. Simulation results show that the proposed framework can reduce energy consumption by up to 53% compared to a standard FL model while balancing the tradeoff between precision, energy, and accuracy in FL over wireless networks.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/On_the_tradeoff_between_energy%2C_precision%2C_and_accuracy_in_federated_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/On_the_tradeoff_between_energy%2C_precision%2C_and_accuracy_in_federated_quantized_neural_networks/#content","title":"Content","text":"<p>Deploying federated learning over wireless networks with resource-constrained devices requires balancing between accuracy, energy efficiency, and precision. In this work, quantized neural networks (QNNs) are proposed to achieve a finite level of precision in both local training and uplink transmission, with weights and activations quantized in fixed-precision format. Energy models for local training and transmission with quantization are derived, and an energy minimization problem is formulated that optimizes the level of precision while ensuring convergence. Simulation results show that the proposed framework can reduce energy consumption by up to 53% compared to a standard FL model while balancing the tradeoff between precision, energy, and accuracy in FL over wireless networks."},{"location":"Quantization-Survey-Paper-Notes/any_task/Prototype-based_neural_network_layers%3A_incorporating_vector_quantization/","title":"Prototype-based neural network layers: incorporating vector quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Prototype-based_neural_network_layers%3A_incorporating_vector_quantization/#summary","title":"Summary","text":"<p>This paper proposes techniques to merge neural networks with prototype-based vector quantization methods, highlighting the similarities between the two approaches and outlining how to construct prototype-based classification layers for multilayer networks. Additionally, an alternative prototype-based approach to the classical convolution operation is also presented. The authors emphasize on establishing a theoretical framework and hope to jump-start the incorporation of prototype-based learning in neural networks."},{"location":"Quantization-Survey-Paper-Notes/any_task/Prototype-based_neural_network_layers%3A_incorporating_vector_quantization/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Prototype-based_neural_network_layers%3A_incorporating_vector_quantization/#content","title":"Content","text":"<p>Neural networks are highly accurate in complex tasks, but they lack robustness and interpretability. On the other hand, prototype-based vector quantization methods are known for being highly robust and interpretable. In this paper, the authors propose techniques and strategies to merge both approaches, especially prototype-based learning in neural networks and vice versa. They highlight the similarities between the two approaches and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, the authors provide an alternative, prototype-based, approach to the classical convolution operation. The authors focus on establishing a strong theoretical framework rather than presenting numerical results. They hope that by publishing their framework, they will jump-start the incorporation of prototype-based learning in neural networks."},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantization_loss_in_convolutional_decoding/","title":"Quantization loss in convolutional decoding","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantization_loss_in_convolutional_decoding/#summary","title":"Summary","text":"<p> The paper presents a new quantization scheme and branch metric calculation method for coded symbols from the AWGN channel with BPSK or QPSK modulation. The authors use cutoff rate to determine the stepsize and the smallest number of quantization bits needed for a given bit signal-to-noise ratio (Eb/NO) loss. The paper presents a 9-level quantizer and 3-bit branch metrics for a rate 1/2 code, with an Eb/NO loss of only 0.14dB. The calculations are verified by simulations of several convolutional codes, including the new memory 14, rate 1/4 or 1/6 codes used by the big Viterbi decoders at JPL."},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantization_loss_in_convolutional_decoding/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantization_loss_in_convolutional_decoding/#content","title":"Content","text":"<p>We study the loss in quantizing coded symbols from the AWGN channel with BPSK or QPSK modulation. A new quantization scheme and branch metric calculation method are presented. For the uniformly quantized AWGN channel, cutoff rate is used to determine the stepsize and the smallest number of quantization bits needed for a given bit signal-to-noise ratio (&amp;/NO) loss. A 9-level quantizer is presented, along with 3-bit branch metrics for a rate 1/2 code, which causes anEb/.\u2019Vo loss of only 0.14 dB. These results also apply to soft-decision decoding of block codes. A tight upper bound is derived for the range of path metrics in a Viterbi decoder. The calculations are verified by simulations of several convolutional codes, including the new memory 14, rate 1/4 or 1/6 codes used by the big Viterbi decoders at JPL."},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantized_control_for_a_class_of_neural_networks_with_adaptive_event%E2%80%90triggered_scheme_and_complex_cyber%E2%80%90attacks/","title":"Quantized control for a class of neural networks with adaptive event\u2010triggered scheme and complex cyber\u2010attacks","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantized_control_for_a_class_of_neural_networks_with_adaptive_event%E2%80%90triggered_scheme_and_complex_cyber%E2%80%90attacks/#summary","title":"Summary","text":"<p>The paper proposes a solution to the quantized control problem for neural networks in the presence of complex cyber-attacks, employing an adaptive event-triggered scheme and a quantization mechanism to reduce communication costs. Stability of the neural network is guaranteed using Lyapunov stability theory, and controller gain is obtained from solving linear matrix inequalities. A numerical example is provided to demonstrate the effectiveness of the proposed method."},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantized_control_for_a_class_of_neural_networks_with_adaptive_event%E2%80%90triggered_scheme_and_complex_cyber%E2%80%90attacks/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantized_control_for_a_class_of_neural_networks_with_adaptive_event%E2%80%90triggered_scheme_and_complex_cyber%E2%80%90attacks/#content","title":"Content","text":"<p>This article focuses on the quantized control problem for neural networks with adaptive event-triggered scheme (AETS) and complex cyber-attacks. It presents a mathematical model of complex cyber-attacks which includes replay attacks, deception attacks, and denial-of-service (DoS) attacks. The AETS and a quantization mechanism are employed to relieve the pressure under limited communication resources. By utilizing Lyapunov stability theory, the stability of neural networks is guaranteed, and the controller gain is derived by solving a set of linear matrix inequalities. The proposed method is verified by a numerical example."},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantized_state_estimation_for_neural_networks_with_cyber_attacks_and_hybrid_triggered_communication_scheme/","title":"Quantized state estimation for neural networks with cyber attacks and hybrid triggered communication scheme","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantized_state_estimation_for_neural_networks_with_cyber_attacks_and_hybrid_triggered_communication_scheme/#summary","title":"Summary","text":"<p>This paper discusses the issue of quantized state estimation for neural networks with the use of a hybrid triggered communication scheme that combines event and time triggered schemes. A mathematical model including both the hybrid triggered scheme and quantization is proposed, and sufficient conditions for ensuring stability of the estimating error system through the use of linear matrix inequality techniques and Lyapunov stability theory are presented. An algorithm based on LMIs is also introduced for designing a desired state estimator."},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantized_state_estimation_for_neural_networks_with_cyber_attacks_and_hybrid_triggered_communication_scheme/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Quantized_state_estimation_for_neural_networks_with_cyber_attacks_and_hybrid_triggered_communication_scheme/#content","title":"Content","text":"<p>This paper presents the issue of quantized state estimation for neural networks with cyber attacks and hybrid triggered communication scheme. The hybrid triggered scheme and quantization are introduced to reduce the network transmission pressure and save network resources. The hybrid triggered scheme consists of time triggered scheme and event triggered scheme. A mathematical model for estimating the state of neural networks is constructed by taking the effect of hybrid triggered scheme and quantization into consideration. The sufficient conditions are given using linear matrix inequality (LMI) techniques and Lyapunov stability theory, which can ensure the stability of estimating error system under hybrid triggered scheme. The designing algorithm of desired state estimator is also presented in terms of LMIs."},{"location":"Quantization-Survey-Paper-Notes/any_task/Reconstruction-Computation-Quantization_%28RCQ%29%3A_A_Paradigm_for_Low_Bit_Width_LDPC_Decoding/","title":"Reconstruction-Computation-Quantization (RCQ): A Paradigm for Low Bit Width LDPC Decoding","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Reconstruction-Computation-Quantization_%28RCQ%29%3A_A_Paradigm_for_Low_Bit_Width_LDPC_Decoding/#summary","title":"Summary","text":"<p> The paper proposes the Reconstruction-Computation-Quantization (RCQ) paradigm for decoding LDPC codes with low message precision. The RCQ parameters are designed using discrete density evolution, which allows for dynamic non-uniform quantization. An extension of RCQ decoding for layered architectures is also introduced, called layer-specific RCQ. The use of layer-specific message representations helps achieve better FER performance. The paper also explores FPGA implementations of RCQ decoders, which resulted in reduced hardware usage while maintaining decoding accuracy."},{"location":"Quantization-Survey-Paper-Notes/any_task/Reconstruction-Computation-Quantization_%28RCQ%29%3A_A_Paradigm_for_Low_Bit_Width_LDPC_Decoding/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Reconstruction-Computation-Quantization_%28RCQ%29%3A_A_Paradigm_for_Low_Bit_Width_LDPC_Decoding/#content","title":"Content","text":"<p> This paper proposes the Reconstruction-Computation-Quantization (RCQ) paradigm to decode low-density parity-check (LDPC) codes. The RCQ parameters are designed by discrete density evolution. This approach facilitates dynamic non-uniform quantization to achieve good frame error rate (FER) performance with very low message precision. The paper also introduces layer-specific RCQ, an extension of RCQ decoding for layered architectures. Layer-specific RCQ uses layer-specific message representations to achieve the best possible FER performance. Finally, the paper studies field-programmable gate array (FPGA) implementations of RCQ decoders, achieving more than a 10% reduction in LUTs and routed nets and more than a 6% decrease in register usage while maintaining comparable decoding performance."},{"location":"Quantization-Survey-Paper-Notes/any_task/Resource-efficient_deep_neural_networks_for_automotive_radar_interference_mitigation/","title":"Resource-efficient deep neural networks for automotive radar interference mitigation","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Resource-efficient_deep_neural_networks_for_automotive_radar_interference_mitigation/#summary","title":"Summary","text":"<p>The authors investigate quantization techniques for DNN-based denoising and interference mitigation of radar signals to reduce memory requirements. The study compares models with fixed and learned bit-widths, achieving an 80% memory reduction compared to the real-valued baseline. The authors recommend using 8 bits for weights and activations resulting in models that require only 0.2 megabytes of memory."},{"location":"Quantization-Survey-Paper-Notes/any_task/Resource-efficient_deep_neural_networks_for_automotive_radar_interference_mitigation/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Resource-efficient_deep_neural_networks_for_automotive_radar_interference_mitigation/#content","title":"Content","text":"<p>Radar sensors are crucial for environment perception in autonomous vehicles and driver assistance systems. However, with the increasing number of radar sensors and unregulated automotive radar frequency bands, mutual interference is inevitable and must be dealt with. Deep neural networks (DNNs) have emerged for radar spectra denoising and interference mitigation, but they require high memory and computational resources. In this study, the authors investigate quantization techniques for DNN-based denoising and interference mitigation of radar signals to reduce memory requirements for model storage and during inference. They compare models with fixed and learned bit-widths and illustrate the importance of structurally small real-valued base models for quantization. The authors achieve a memory reduction of around 80% compared to the real-valued baseline and recommend the use of 8 bits for weights and activations, which results in models that require only 0.2 megabytes of memory."},{"location":"Quantization-Survey-Paper-Notes/any_task/SmaQ%3A_Smart_quantization_for_DNN_training_by_exploiting_value_clustering/","title":"SmaQ: Smart quantization for DNN training by exploiting value clustering","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/SmaQ%3A_Smart_quantization_for_DNN_training_by_exploiting_value_clustering/#summary","title":"Summary","text":"<p>Summary: The paper discusses the issue of memory requirement during neural network training and introduces Smart Quantization (SmaQ), a quantization scheme that exploits the observed normal distribution to quantize data structures resulting in up to 6.7x reduction in memory usage during training with minimal accuracy loss.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/SmaQ%3A_Smart_quantization_for_DNN_training_by_exploiting_value_clustering/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/SmaQ%3A_Smart_quantization_for_DNN_training_by_exploiting_value_clustering/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/The_effect_of_uniform_quantization_on_parameter_estimation_of_compound_distributions/","title":"The effect of uniform quantization on parameter estimation of compound distributions","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/The_effect_of_uniform_quantization_on_parameter_estimation_of_compound_distributions/#summary","title":"Summary","text":"<p>This paper discusses the impact of uniform quantization on the maximum likelihood estimation of probability density function parameters for a compound distribution. The Fisher information loss is used as a measure of the loss of information due to quantization. An approximation is proposed to characterize the loss's asymptotic behavior; this reduces computational complexity and investigates selecting the quantization interval for a pre-defined loss of Fisher information. Guidelines for selecting a quantization interval with a pre-defined maximum loss of Fisher information are presented, and numerical simulations demonstrate the approximation's efficiency."},{"location":"Quantization-Survey-Paper-Notes/any_task/The_effect_of_uniform_quantization_on_parameter_estimation_of_compound_distributions/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/The_effect_of_uniform_quantization_on_parameter_estimation_of_compound_distributions/#content","title":"Content","text":"<p>This paper discusses the impact of uniform quantization on the maximum likelihood estimation of probability density function parameters for a compound distribution. The Fisher information loss is used as a measure of the loss of information due to quantization. The paper proposes an approximation to characterize the loss's asymptotic behavior to reduce computational complexity and investigate selecting the quantization interval for a pre-defined loss of Fisher information. Numerical simulations demonstrate the approximation's efficiency. The paper presents guidelines for selecting a quantization interval with a pre-defined maximum loss of Fisher information. The proposed approximation significantly reduces the computational complexity."},{"location":"Quantization-Survey-Paper-Notes/any_task/Uniform_Vs._Non-Uniform_Coarse_Quantization_in_Mutual_Information_Maximizing_LDPC_Decoding/","title":"Uniform Vs. Non-Uniform Coarse Quantization in Mutual Information Maximizing LDPC Decoding","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Uniform_Vs._Non-Uniform_Coarse_Quantization_in_Mutual_Information_Maximizing_LDPC_Decoding/#summary","title":"Summary","text":"<p>Summary: The paper proposes to use uniform quantization with a simple hardware structure in low-resolution LDPC decoders, which reduces the complexity of individual node operations, shortens decoding delay, and causes only minor performance degradation within 0.01 dB compared to the non-uniform alternative. The proposed 3-bit decoder has the potential to replace 4-bit conventional decoders due to the complexity reduction.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Uniform_Vs._Non-Uniform_Coarse_Quantization_in_Mutual_Information_Maximizing_LDPC_Decoding/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Uniform_Vs._Non-Uniform_Coarse_Quantization_in_Mutual_Information_Maximizing_LDPC_Decoding/#content","title":"Content","text":"<p>Recently, low-resolution LDPC decoders have been introduced that perform mutual information maximizing signal processing. However, the optimal quantization in variable and check nodes requires expensive non-uniform operations. Instead, we propose to use uniform quantization with a simple hardware structure, which reduces the complexity of individual node operations approximately by half and shortens the decoding delay significantly. Our analysis shows that the loss of preserved mutual information resulting from restriction to uniform quantization is very small. Furthermore, the error rate simulations with regular LDPC codes confirm that the uniform quantization causes only minor performance degradation within 0.01 dB compared to the non-uniform alternative. Due to the complexity reduction, especially the proposed 3-bit decoder is a promising candidate to replace 4-bit conventional decoders."},{"location":"Quantization-Survey-Paper-Notes/any_task/Variational_bayesian_quantization/","title":"Variational bayesian quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/any_task/Variational_bayesian_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes an algorithm for quantizing continuous latent representations in trained models. This algorithm is applicable to deep probabilistic models such as VAEs and enables both data and model compression. Unlike current end-to-end neural compression methods, this approach allows for a variable rate-distortion trade-off using a single trained model. This algorithm is based on arithmetic coding and uses adaptive quantization accuracy based on estimates of posterior uncertainty. Therefore, it allows for \"plug-and-play\" compression without the need for fixed quantization schemes.</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Variational_bayesian_quantization/#target-task","title":"Target Task","text":"<p>any task</p>"},{"location":"Quantization-Survey-Paper-Notes/any_task/Variational_bayesian_quantization/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/3DQ%3A_Compact_quantized_neural_networks_for_volumetric_whole_brain_segmentation/","title":"3DQ: Compact quantized neural networks for volumetric whole brain segmentation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/3DQ%3A_Compact_quantized_neural_networks_for_volumetric_whole_brain_segmentation/#summary","title":"Summary","text":"<p> The paper proposes 3DQ, a ternary quantization method for 3D Fully Convolutional Neural Networks (F-CNNs), which significantly compresses the model by up to 16x while maintaining comparable accuracy with full precision models, especially for whole brain segmentation. The method is applicable to common 3D architectures such as 3D U-Net and V-Net and can effectively reduce the storage requirements of space-critical applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/3DQ%3A_Compact_quantized_neural_networks_for_volumetric_whole_brain_segmentation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/3DQ%3A_Compact_quantized_neural_networks_for_volumetric_whole_brain_segmentation/#content","title":"Content","text":"<p> Model architectures have been dramatically increasing in size, improving performance at the cost of resource requirements. In this paper we propose 3DQ, a ternary quantization method, applied for the first time to 3D Fully Convolutional Neural Networks (F-CNNs), enabling 16x model compression while maintaining performance on par with full precision models. We extensively evaluate 3DQ on two datasets for the challenging task of whole brain segmentation. Additionally, we showcase our method's ability to generalize on two common 3D architectures, namely 3D U-Net and V-Net. Outperforming a variety of baselines, the proposed method is capable of compressing large 3D models to a few MBytes, alleviating the storage needs in space-critical applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ACIQ%3A_analytical_clipping_for_integer_quantization_of_neural_networks/","title":"ACIQ: analytical clipping for integer quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ACIQ%3A_analytical_clipping_for_integer_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method to minimize quantization effects at the tensor level rather than the network level. The authors analyze the trade-off between quantization noise and clipping distortion in low precision networks, identify tensor statistics, and derive approximate analytical expressions for mean-square-error degradation due to clipping. By optimizing these expressions, they show significant improvements over standard quantization schemes, and provide applications for training and inference of low-precision neural networks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ACIQ%3A_analytical_clipping_for_integer_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ACIQ%3A_analytical_clipping_for_integer_quantization_of_neural_networks/#content","title":"Content","text":"<p> Unlike traditional approaches that focus on the quantization at the network level, in this work we propose to minimize the quantization effect at the tensor level. We analyze the trade-off between quantization noise and clipping distortion in low precision networks. We identify the statistics of various tensors, and derive approximate analytical expressions for the mean-square-error degradation due to clipping. By optimizing these expressions, we show marked improvements over standard quantization schemes that normally avoid clipping. For example, just by choosing the accurate clipping values, more than 40% accuracy improvement is obtained for the quantization of VGG16-BN to 4-bits of precision. Our results have many applications for the quantization of neural networks at both training and inference time. One immediate application is for a rapid deployment of neural networks to low-precision accelerators without time-consuming fine-tuning or the availability of the full datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Generalized_Zero-Shot_Quantization_of_Deep_Convolutional_Neural_Networks_via_Learned_Weights_Statistics/","title":"A Generalized Zero-Shot Quantization of Deep Convolutional Neural Networks via Learned Weights Statistics","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Generalized_Zero-Shot_Quantization_of_Deep_Convolutional_Neural_Networks_via_Learned_Weights_Statistics/#summary","title":"Summary","text":"<p>Summary: The paper discusses the limitations of existing zero-shot quantization methods that rely on batch normalization parameters and proposes a generalized zero-shot quantization (GZSQ) framework that does not require original data or BN layer statistics. The authors use data distillation to estimate enriched data for range calibration of activations and demonstrate improved classification accuracy (33%) for various models with and without BN layers. The proposed model is also applicable for unnormalized deep neural networks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Generalized_Zero-Shot_Quantization_of_Deep_Convolutional_Neural_Networks_via_Learned_Weights_Statistics/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Generalized_Zero-Shot_Quantization_of_Deep_Convolutional_Neural_Networks_via_Learned_Weights_Statistics/#content","title":"Content","text":"<p>Quantizing floating-point weights and activations of deep convolutional neural networks to fixed-point representation yields reduced memory footprints and inference time. Recently, zero-shot quantization methods that do not require original unlabelled training samples of a given task have been proposed. However, most of these methods heavily rely on the learned batch normalization (BN) parameters to infer the range of activations for quantization, and their performance severely degrades when presented with a network that does not accommodate BN layers. In this work, we propose a generalized zero-shot quantization (GZSQ) framework that neither requires original data nor relies on BN layer statistics. We have utilized the data distillation approach and leverage only the pre-trained weights of the model to estimate enriched data for the range calibration of activations. Our scheme has significantly outperformed existing zero-shot works, an improvement of 33% in classification accuracy for MobileNetV2 and several other models with and without BN layers, for a variety of tasks. We have also demonstrated the efficacy of the proposed work across multiple open-source quantization frameworks. Importantly, our work is the first attempt towards the post-training zero-shot quantization of futuristic unnormalized deep neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Learning_Framework_for_n-Bit_Quantized_Neural_Networks_Toward_FPGAs/","title":"A Learning Framework for n-Bit Quantized Neural Networks Toward FPGAs","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Learning_Framework_for_n-Bit_Quantized_Neural_Networks_Toward_FPGAs/#summary","title":"Summary","text":"<p>Summary: The paper proposes a learning framework for n-bit quantized neural networks with weights constrained to power of two which includes a reconstructed gradient function and a novel QNN structure called n-BQ-NN using shift operations suitable for FPGA implementation. The paper introduces a shift vector processing element array to replace all 16-bit multiplications, reducing energy consumption and increasing execution speed. Experiments show that the quantized models of ResNet, DenseNet, and AlexNet can achieve the same accuracy as the original full-precision models, achieving state-of-the-art results compared to typical low-precision QNNs.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Learning_Framework_for_n-Bit_Quantized_Neural_Networks_Toward_FPGAs/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Learning_Framework_for_n-Bit_Quantized_Neural_Networks_Toward_FPGAs/#content","title":"Content","text":"<p>This paper proposes a learning framework for n-bit quantized neural networks (QNNs) with weights constrained to the power of two. The framework includes a reconstructed gradient function to solve the gradient vanishing problem in back-propagation algorithm and a novel QNN structure called n-BQ-NN that uses shift operations to replace multiplication, making it more suitable for FPGA implementation. The paper also introduces a shift vector processing element (SVPE) array to replace all 16-bit multiplications, reducing energy consumption and increasing execution speed. Experiments show that the quantized models of ResNet, DenseNet, and AlexNet can achieve the same accuracy as the original full-precision models. Furthermore, the proposed learning framework can achieve state-of-the-art results compared to typical low-precision QNNs. Index terms include deep learning, quantum neural network, deep compression, and FPGA."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Low_Memory_Footprint_Quantized_Neural_Network_for_Depth_Completion_of_Very_Sparse_Time-of-Flight_Depth_Maps/","title":"A Low Memory Footprint Quantized Neural Network for Depth Completion of Very Sparse Time-of-Flight Depth Maps","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Low_Memory_Footprint_Quantized_Neural_Network_for_Depth_Completion_of_Very_Sparse_Time-of-Flight_Depth_Maps/#summary","title":"Summary","text":"<p>Summary: The paper proposes a quantized convolutional encoder-decoder network for depth completion, achieving optimal depth map quality through input pre-processing and carefully tuned training with a geometry-preserving loss function. The models are quantized using mixed precision quantization-at-training techniques, resulting in low memory footprint for weights and activations. The quantized models achieve significant reductions in GPU times and memory size with minimal impact on quality metrics.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Low_Memory_Footprint_Quantized_Neural_Network_for_Depth_Completion_of_Very_Sparse_Time-of-Flight_Depth_Maps/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_Low_Memory_Footprint_Quantized_Neural_Network_for_Depth_Completion_of_Very_Sparse_Time-of-Flight_Depth_Maps/#content","title":"Content","text":"<p> We propose a quantized convolutional encoder-decoder network for depth completion of very sparse time-of-flight depth maps. Our model achieves optimal depth map quality by means of input pre-processing and carefully tuned training with a geometry-preserving loss function. We also achieve low memory footprint for weights and activations by means of mixed precision quantization-at-training techniques. The resulting quantized models are comparable to the state of the art in terms of quality, but they require very low GPU times and achieve up to 14-fold memory size reduction for the weights with minimal impact on quality metrics."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_compression_pipeline_for_one-stage_object_detection_model/","title":"A compression pipeline for one-stage object detection model","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_compression_pipeline_for_one-stage_object_detection_model/#summary","title":"Summary","text":"<p>Summary: This paper proposes a compression pipeline for one-stage object detection networks to meet real-time requirements. The proposed pipeline compresses the model size and inference time to 10% or less of the original while maintaining mAP reduction to 2.5% or less.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_compression_pipeline_for_one-stage_object_detection_model/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_compression_pipeline_for_one-stage_object_detection_model/#content","title":"Content","text":"<p>Deep neural networks have strong fitting ability on a variety of computer vision tasks, but they also require intensive computing power and large storage space, which are not always available in portable smart devices. Although a lot of studies have contributed to the compression of image classification networks, there are few model compression algorithms for object detection models. In this paper, we propose a general compression pipeline for one-stage object detection networks to meet the real-time requirements. With this pipeline, the model size and inference time are compressed to 10% or less of the original, while the mAP is only reduced by 2.5% or less."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_high-throughput_and_energy-efficient_RRAM-based_convolutional_neural_network_using_data_encoding_and_dynamic_quantization/","title":"A high-throughput and energy-efficient RRAM-based convolutional neural network using data encoding and dynamic quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_high-throughput_and_energy-efficient_RRAM-based_convolutional_neural_network_using_data_encoding_and_dynamic_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a high-throughput and energy-efficient Convolutional Neural Network (CNN) using Resistive Random-Access Memory (RRAM). The design utilizes data encoding and dynamic quantization to achieve high energy efficiency and high accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_high-throughput_and_energy-efficient_RRAM-based_convolutional_neural_network_using_data_encoding_and_dynamic_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_high-throughput_and_energy-efficient_RRAM-based_convolutional_neural_network_using_data_encoding_and_dynamic_quantization/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_layer-wise_adversarial-aware_quantization_optimization_for_improving_robustness/","title":"A layer-wise adversarial-aware quantization optimization for improving robustness","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_layer-wise_adversarial-aware_quantization_optimization_for_improving_robustness/#summary","title":"Summary","text":"<p>The paper proposes a layer-wise adversarial-aware quantization method to improve the robustness of quantized adversarially-trained neural networks by minimizing adversarial and quantization losses simultaneously. The method uses Lipschitz constant to choose the best quantization parameter settings for a neural network. The proposed method is consistent with proven metrics and effectively improves the robustness of quantized adversarially-trained neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_layer-wise_adversarial-aware_quantization_optimization_for_improving_robustness/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_layer-wise_adversarial-aware_quantization_optimization_for_improving_robustness/#content","title":"Content","text":"<p>In this work, we propose a layer-wise adversarial-aware quantization method to improve the robustness of quantized adversarially-trained neural networks by simultaneously minimizing adversarial and quantization losses. Recent research has found that adversarially-trained neural networks are more vulnerable to quantization loss than plain models. Our method uses Lipschitz constant to select the best quantization parameter settings for a neural network. We derive the losses and prove the consistency of our metric selection theoretically. Experimental results show that our method can effectively and efficiently improve the robustness of quantized adversarially-trained neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_mixed-precision_RISC-V_processor_for_extreme-edge_DNN_inference/","title":"A mixed-precision RISC-V processor for extreme-edge DNN inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_mixed-precision_RISC-V_processor_for_extreme-edge_DNN_inference/#summary","title":"Summary","text":"<p>The paper proposes a novel RISC-V ISA core MPIC based on the open-source RI5CY core that enables full support for mixed-precision Quantized Neural Network inference. The approach uses status-based SIMD instructions to dynamically set the precision of each operand in a core status register. The results show the MPIC improves both performance and energy efficiency when compared to software-based mixed-precision on RI5CY, and with respect to commercially available Cortex-M4 and M7 microcontrollers, it delivers better performance and higher efficiency."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_mixed-precision_RISC-V_processor_for_extreme-edge_DNN_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_mixed-precision_RISC-V_processor_for_extreme-edge_DNN_inference/#content","title":"Content","text":"<p>Low bit-width Quantized Neural Networks (QNNs) enable deployment of complex machine learning models on constrained devices such as microcontrollers (MCUs) by reducing their memory footprint. Fine-grained asymmetric quantization (i.e., different bit-widths assigned to weights and activations on a tensor-by-tensor basis) is a particularly interesting scheme to maximize accuracy under a tight memory constraint [1]. However, the lack of sub-byte instruction set architecture (ISA) support in SoA microprocessors makes it hard to fully exploit this extreme quantization paradigm in embedded MCUs. Support for sub-byte and asymmetric QNNs would require many precision formats and an exorbitant amount of opcode space. In this work, we attack this problem with status-based SIMD instructions: rather than encoding precision explicitly, each operand\u2019s pre- cision is set dynamically in a core status register. We propose a novel RISC-V ISA core MPIC (Mixed Precision Inference Core) based on the open-source RI5CY core. Our approach enables full support for mixed-precision QNN inference with different combinations of operands at 16-, 8-, 4- and 2-bit precision, without adding any extra opcode or increasing the complexity of the decode stage. Our results show that MPIC improves both performance and energy ef\ufb01ciency by a factor of 1.1\u20134.9 when compared to software-based mixed-precision on RI5CY; with respect to commercially available Cortex-M4 and M7 microcontrollers, it delivers 3.6\u201311.7 better performance and 41\u2013155 higher ef\ufb01ciency."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_quantization-aware_regularized_learning_method_in_multilevel_memristor-based_neuromorphic_computing_system/","title":"A quantization-aware regularized learning method in multilevel memristor-based neuromorphic computing system","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_quantization-aware_regularized_learning_method_in_multilevel_memristor-based_neuromorphic_computing_system/#summary","title":"Summary","text":"<p>Sure, please provide me the complete text and I'll help you summarize the abstract section.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_quantization-aware_regularized_learning_method_in_multilevel_memristor-based_neuromorphic_computing_system/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_quantization-aware_regularized_learning_method_in_multilevel_memristor-based_neuromorphic_computing_system/#content","title":"Content","text":"<p>Sorry, but the text you provided does not contain the abstract section of the research paper. Could you please provide the complete text so that I can accurately extract the abstract section for you?</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_quantization-friendly_separable_convolution_for_mobilenets/","title":"A quantization-friendly separable convolution for mobilenets","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_quantization-friendly_separable_convolution_for_mobilenets/#summary","title":"Summary","text":"<p>Summary: The paper explains the importance of making inference computations efficient for deploying deep learning on mobile/IoT devices. Quantization is an effective approach to achieve this offloading. However, existing network designs are not always quantization-friendly. The paper proposes a quantization-friendly separable convolution architecture for the popular lightweight MobileNetV1 model, to reduce the accuracy gap between quantized and float point models. The proposed modified MobileNetV1 model achieves an 8-bit inference top-1 accuracy of 68.03% on ImageNet2012 dataset, almost closing the gap to the float pipeline.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_quantization-friendly_separable_convolution_for_mobilenets/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_quantization-friendly_separable_convolution_for_mobilenets/#content","title":"Content","text":"<p>As deep learning (DL) is being rapidly pushed to edge computing, researchers invented various ways to make inference computation more efficient on mobile/IoT devices, such as network pruning, parameter compression, and etc. Quantization, as one of the key approaches, can effectively offload GPU and make it possible to deploy DL on fixed-point pipeline. Unfortunately, not all existing networks design are friendly to quantization. For example, the popular light-weight MobileNetV1 [1], while it successfully reduces parameter size and computation latency with separable convolution, our experiment shows its quantized models have large accuracy gap against its float point models. To resolve this, we analyzed the root cause of quantization loss and proposed a quantization-friendly separable convolution architecture. By evaluating the image classification task on ImageNet2012 dataset, our modified MobileNetV1 model can archive 8-bit inference top-1 accuracy in 68.03%, almost closed the gap to the float pipeline. Keywords: Separable Convolution, MobileNetV1, Quantization, Fixed-point Inference"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_robust%2C_quantization-aware_training_method_for_photonic_neural_networks/","title":"A robust, quantization-aware training method for photonic neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_robust%2C_quantization-aware_training_method_for_photonic_neural_networks/#summary","title":"Summary","text":"<p> The paper introduces a new training method for photonic neural networks (PNNs) to compensate for quantization noise caused by ADC and DAC conversions. This method considers quantization during training, leading to performance improvements during inference. The study tests the proposed method on various tasks and neuromorphic architectures, proving its effectiveness and generalization ability for low-bit resolution photonic architectures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_robust%2C_quantization-aware_training_method_for_photonic_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_robust%2C_quantization-aware_training_method_for_photonic_neural_networks/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_spike_in_performance%3A_Training_hybrid-spiking_neural_networks_with_quantized_activation_functions/","title":"A spike in performance: Training hybrid-spiking neural networks with quantized activation functions","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_spike_in_performance%3A_Training_hybrid-spiking_neural_networks_with_quantized_activation_functions/#summary","title":"Summary","text":"<p>Summary: The authors of the paper propose a new method for training hybrid-spiking networks that outperforms state-of-the-art recurrent architectures while maintaining a high level of accuracy. The new approach involves gradually transitioning between non-spiking and spiking regimes and significantly improves energy efficiency by reducing activities to an average of 3.74 bits while multiplying each weight by 1.26 significant bits.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_spike_in_performance%3A_Training_hybrid-spiking_neural_networks_with_quantized_activation_functions/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_spike_in_performance%3A_Training_hybrid-spiking_neural_networks_with_quantized_activation_functions/#content","title":"Content","text":"<p>The machine learning community is interested in the energy efficiency of neural networks, and Spiking Neural Networks (SNNs) are being explored as a promising approach due to their activation levels being quantized into temporally sparse, one-bit values. However, maintaining state-of-the-art accuracy when converting a non-spiking network to an SNN has posed a challenge, primarily due to spikes having only a single bit of precision. In this paper, the authors propose a new method of training hybrid-spiking networks by smoothly interpolating between non-spiking and spiking regimes, and show that it outperforms state-of-the-art recurrent architectures while reducing activities to at most 3.74 bits on average with 1.26 significant bits multiplying each weight. The paper presents a discussion on how these methods can significantly improve the energy efficiency of neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_statistical_framework_for_low-bitwidth_training_of_deep_neural_networks/","title":"A statistical framework for low-bitwidth training of deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_statistical_framework_for_low-bitwidth_training_of_deep_neural_networks/#summary","title":"Summary","text":"<p>The paper presents a statistical framework for analyzing fully quantized training (FQT) algorithms, which use low-bitwidth hardware by quantizing activations, weights, and gradients. They show that the FQT gradient is an unbiased estimator of the quantization-aware training (QAT) gradient and discuss the impact of gradient quantization on variance. Two novel gradient quantizers are developed, which have smaller variance than the existing per-tensor quantizer. For training ResNet-50 on ImageNet, a 5-bit block Householder quantizer achieves only 0.5% validation accuracy loss relative to QAT, comparable to the existing INT8 baseline."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_statistical_framework_for_low-bitwidth_training_of_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_statistical_framework_for_low-bitwidth_training_of_deep_neural_networks/#content","title":"Content","text":"<p>Fully quantized training (FQT), which uses low-bitwidth hardware by quantizing the activations, weights, and gradients of a neural network model, is a promising approach to accelerate the training of deep neural networks. One major challenge with FQT is the lack of theoretical understanding, in particular of how gradient quantization impacts convergence properties. In this paper, we address this problem by presenting a statistical framework for analyzing FQT algorithms. We view the quantized gradient of FQT as a stochastic estimator of its full precision counterpart, a procedure known as quantization-aware training (QAT). We show that the FQT gradient is an unbiased estimator of the QAT gradient, and we discuss the impact of gradient quantization on its variance. Inspired by these theoretical results, we develop two novel gradient quantizers, and we show that these have smaller variance than the existing per-tensor quantizer. For training ResNet-50 on ImageNet, our 5-bit block Householder quantizer achieves only 0.5% validation accuracy loss relative to QAT, comparable to the existing INT8 baseline. Our code is publicly available at https://github.com/cjf00000/StatQuant."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_methods_for_low-power_deep_learning_and_computer_vision/","title":"A survey of methods for low-power deep learning and computer vision","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_methods_for_low-power_deep_learning_and_computer_vision/#summary","title":"Summary","text":"<p>Summary: The paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses four major categories of techniques for compacting and accelerating DNN models: parameter quantization and pruning, compressed convolutional filters and matrix factorization, network architecture search, and knowledge distillation. The paper analyzes the advantages, disadvantages, and potential solutions to the problems with each method and proposes a set of evaluation metrics for future research.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_methods_for_low-power_deep_learning_and_computer_vision/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_methods_for_low-power_deep_learning_and_computer_vision/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_model_compression_and_acceleration_for_deep_neural_networks/","title":"A survey of model compression and acceleration for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_model_compression_and_acceleration_for_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper reviews recent techniques for compressing and accelerating Deep Neural Networks (DNNs) into four categories, namely parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization were thoroughly discussed before introducing the other techniques. The paper offers analysis of the performance, advantages, and drawbacks of each category and concludes by discussing the challenges and possible directions for future work.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_model_compression_and_acceleration_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_model_compression_and_acceleration_for_deep_neural_networks/#content","title":"Content","text":"<p>Deep neural networks (DNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. In this paper, we review the recent techniques for compacting and accelerating DNN models, divided into four categories: parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization are described first, after which the other techniques are introduced. For each category, insightful analysis about the performance, related applications, advantages and drawbacks are provided. Finally, we conclude the paper, discuss remaining the challenges and possible directions for future work."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_product_quantization/","title":"A survey of product quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_product_quantization/#summary","title":"Summary","text":"<p>Summary: The paper provides a review of Product Quantization (PQ) and its derivatives and surveys recent PQ-based methods. It highlights the attractive properties of PQ, including efficient compression, efficient computation, and simplicity of coding algorithms. The paper also introduces generalized representations of PQ encoding, practical large-scale systems with an inverted index structure, and hardware-based accelerations. Additionally, an executable example of Python codes for PQ is included.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_product_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_of_product_quantization/#content","title":"Content","text":"<p> Product Quantization (PQ) and its derivatives are widely used for approximated nearest neighbor search in large-scale data. In this paper, the authors review the fundamental algorithm of PQ and provide executable sample codes, followed by a comprehensive survey of recent PQ-based methods. The authors highlight three attractive properties of PQ: efficient compression, efficient computation of the approximate distance, and simplicity of the data structure and coding algorithms. The paper also introduces generalized representations of PQ encoding, practical large-scale systems with an inverted index structure, hardware-based accelerations, and an executable example of Python codes for PQ. The authors formulate the approximated nearest neighbor search problem, which is finding the database item that minimizes the distance measure to the query vector. The paper assumes the Euclidean norm for the distance measurement."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_on_methods_and_theories_of_quantized_neural_networks/","title":"A survey on methods and theories of quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_on_methods_and_theories_of_quantized_neural_networks/#summary","title":"Summary","text":"<p>Summary: This survey paper discusses the importance of quantization in deep neural networks for minimizing memory requirements. The authors provide a comprehensive review of various aspects of quantized neural networks and also discuss current challenges and trends in this field.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_on_methods_and_theories_of_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_survey_on_methods_and_theories_of_quantized_neural_networks/#content","title":"Content","text":"<p> Deep neural networks are the state-of-the-art methods for many real-world tasks, such as computer vision, natural language processing and speech recognition. Quantization is recognized as one of the most effective approaches to satisfy the extreme memory requirements that deep neural network models demand. In this survey, we give a thorough review of different aspects of quantized neural networks. Current challenges and trends of quantized neural networks are also discussed."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_tiny_CNN_architecture_for_medical_face_mask_detection_for_resource-constrained_endpoints/","title":"A tiny CNN architecture for medical face mask detection for resource-constrained endpoints","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_tiny_CNN_architecture_for_medical_face_mask_detection_for_resource-constrained_endpoints/#summary","title":"Summary","text":"<p>Summary: The paper proposes an architecture for detecting medical face masks that can be deployed on resource-constrained endpoints with extremely low memory footprints. The model is deployed on a small development board with an ARM Cortex-M7 microcontroller clocked at 480 Mhz and having just 496 KB of framebuffer RAM. The model is quantized using the TensorFlow Lite framework to reduce its size further. The proposed model is 138 KB post quantization and runs at the inference speed of 30 FPS.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_tiny_CNN_architecture_for_medical_face_mask_detection_for_resource-constrained_endpoints/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_tiny_CNN_architecture_for_medical_face_mask_detection_for_resource-constrained_endpoints/#content","title":"Content","text":"<p>The world is going through one of the most dangerous pandemics of all time with the rapid spread of the novel coronavirus (COVID-19). According to the World Health Organisation, the most effective way to thwart the transmission of coronavirus is to wear medical face masks. Monitoring the use of face masks in public places has been a challenge because manual monitoring could be unsafe. This paper proposes an architecture for detecting medical face masks for deployment on resource-constrained endpoints having extremely low memory footprints. A small development board with an ARM Cortex-M7 microcontroller clocked at480 Mhz and having just 496 KB of framebuffer RAM, has been used for the deployment of the model. Using the TensorFlow Lite framework, the model is quantized to further reduce its size. The proposed model is 138 KB post quantization and runs at the inference speed of 30 FPS."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_white_paper_on_neural_network_quantization/","title":"A white paper on neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_white_paper_on_neural_network_quantization/#summary","title":"Summary","text":"<p>The paper introduces methods for mitigating the impact of quantization noise on neural network performance while reducing power and latency. Two classes of algorithms, Post-Training Quantization (PTQ) and Quantization-Aware-Training (QAT), are considered, with PTQ requiring no re-training or labelled data and QAT requiring fine-tuning and access to labelled training data. The paper provides tested pipelines based on existing literature and experimental results for achieving state-of-the-art performance for common deep learning models and tasks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_white_paper_on_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/A_white_paper_on_neural_network_quantization/#content","title":"Content","text":"<p>While neural networks have advanced the frontiers in many applications, they often come at a high computational cost. Reducing the power and latency of neural network inference is key if we want to integrate modern networks into edge devices with strict power and compute requirements. Neural network quantization is one of the most effective ways of achieving these savings but the additional noise it induces can lead to accuracy degradation. In this white paper, we introduce state-of-the-art algorithms for mitigating the impact of quantization noise on the network\u2019s performance while maintaining low-bit weights and activations. We start with a hardware motivated introduction to quantization and then consider two main classes of algorithms: Post-Training Quantization (PTQ) and Quantization-Aware-Training (QAT). PTQ requires no re-training or labelled data and is thus a lightweight push-button approach to quantization. In most cases, PTQ is sufficient for achieving 8-bit quantization with close to floating-point accuracy. QAT requires fine-tuning and access to labeled training data but enables lower bit quantization with competitive results. For both solutions, we provide tested pipelines based on existing literature and extensive experimentation that lead to state-of-the-art performance for common deep learning models and tasks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accelerating_convolutional_neural_networks_via_activation_map_compression/","title":"Accelerating convolutional neural networks via activation map compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accelerating_convolutional_neural_networks_via_activation_map_compression/#summary","title":"Summary","text":"<p>This paper proposes a three-stage compression and acceleration pipeline for efficient neural network use in low-powered devices. The pipeline sparsifies, quantizes, and entropy encodes activation maps of Convolutional Neural Networks. The sparsification increases inferencing acceleration and model accuracy. The quantization and entropy encoding of the activation maps lead to a higher compression over the baseline, reducing memory cost. Inception-V3 and MobileNet-V1 can be accelerated by as much as 1.6x with an increase in accuracy on the ImageNet and CIFAR-10 datasets respectively. Activation maps, quantized to 16bits, are compressed by as much as 6x with an increase in accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accelerating_convolutional_neural_networks_via_activation_map_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accelerating_convolutional_neural_networks_via_activation_map_compression/#content","title":"Content","text":"<p>Towards the efficient use of neural networks in low-powered devices, we propose a three-stage compression and acceleration pipeline that sparsifies, quantizes, and entropy encodes activation maps of Convolutional Neural Networks. The sparsification increases the representational power of activation maps leading to both acceleration of inference and higher model accuracy, with Inception-V3 and MobileNet-V1 being able to be accelerated by as much as 1.6x with an increase in accuracy of 0.38% and 0.54% on the ImageNet and CIFAR-10 datasets respectively. The quantizing and entropy encoding of the sparser activation maps lead to a higher compression over the baseline, reducing the memory cost of the network execution. Inception-V3 and MobileNet-V1 activation maps, quantized to 16bits, are compressed by as much as 6x with an increase in accuracy of 0.36% and 0.55% respectively."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accelerating_large-scale_inference_with_anisotropic_vector_quantization/","title":"Accelerating large-scale inference with anisotropic vector quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accelerating_large-scale_inference_with_anisotropic_vector_quantization/#summary","title":"Summary","text":"<p>The paper proposes a new family of score-aware quantization loss functions that aim to minimize the quantization error for database points with higher inner products by penalizing the parallel component of a datapoint's residual relative to its orthogonal component. The researchers demonstrate that this approach leads to significant performance gains over traditional quantization methods in maximum inner product search tasks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accelerating_large-scale_inference_with_anisotropic_vector_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accelerating_large-scale_inference_with_anisotropic_vector_quantization/#content","title":"Content","text":"<p>Quantization based techniques are currently used to scale maximum inner product search to massive databases. The objective of traditional quantization methods is to minimize the reconstruction error of database points. However, this is suboptimal for MIPS, as quantization error for database points with higher inner products is more important. In response, we propose a new family of score-aware quantization loss functions that can work under any weighting function of the inner product and regardless of whether the datapoints vary in norm. We demonstrate that these loss functions result in anisotropic weighting that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. This leads to large MIPS performance gains over reconstruction loss."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accurate_and_efficient_2-bit_quantized_neural_networks/","title":"Accurate and efficient 2-bit quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accurate_and_efficient_2-bit_quantized_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes a novel technique for weight and activation quantization resulting in a quantized neural network. The activation quantization method, PACT, optimizes the activation clipping parameter during training to find the right quantization scale. The weight quantization method, SAWB, finds the optimal scaling factor that minimizes the quantization error based on weight distribution statistics without an exhaustive search. The technique achieves state-of-the-art classification accuracy comparable to full precision networks, using a 2-bit QNN on popular models and datasets, and provides insight for quantization in the presence of shortcut connections."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accurate_and_efficient_2-bit_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accurate_and_efficient_2-bit_quantized_neural_networks/#content","title":"Content","text":"<p>Deep learning algorithms achieve high classification accuracy at the expense of significant computation cost. In order to reduce this cost, several quantization schemes have gained attention recently with some focusing on weight quantization, and others focusing on quantizing activations. This paper proposes novel techniques that individually target weight and activation quantizations resulting in an overall quantized neural network (QNN). Our activation quantization technique, PArameterized Clipping acTivation (PACT), uses an activation clipping parameter that is optimized during training to find the right quantization scale. Our weight quantization scheme, statistics-aware weight binning (SAWB), finds the optimal scaling factor that minimizes the quantization error based on the statistical characteristics of weight distribution without the need for an exhaustive search. Furthermore, we provide an innovative insight for quantization in the presence of shortcut connections, which motivates the use of high-precision for the shortcuts. The combination of PACT and SAWB results in a 2-bit QNN that achieves state-of-the-art classification accuracy (comparable to full precision networks) across a range of popular models and datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accurate_post_training_quantization_with_small_calibration_sets/","title":"Accurate post training quantization with small calibration sets","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accurate_post_training_quantization_with_small_calibration_sets/#summary","title":"Summary","text":"<p>Summary: The paper proposes a post-training quantization method that optimizes parameters over the calibration set to minimize quantization errors of each layer separately, without significant accuracy degradation. The method suggests two flavors, parallel and sequential, for fixed and flexible bit-width allocation, respectively. The paper also suggests model global statistics tuning to correct biases introduced during quantization, yielding state-of-the-art results for both vision and text models. The suggested methods are much faster than the traditional Quantize Aware Training approach used for lower than 8-bit quantization. The code has been open-sourced.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accurate_post_training_quantization_with_small_calibration_sets/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Accurate_post_training_quantization_with_small_calibration_sets/#content","title":"Content","text":"<p> Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to \ufb01ne-tune the model without signi\ufb01cant over\ufb01tting. Instead, these methods only use the calibration set to set the activations\u2019 dynamic ranges. However, such methods always resulted in signi\ufb01cant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer or block separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over\ufb01tting than the standard \ufb01ne-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations\u2019 dynamic ranges. We suggest two \ufb02avors for our method, parallel and sequential aim for a \ufb01xed and \ufb02exible bit-width allocation. For the latter, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1% accuracy degradation \u2014 with 4-bit weights and activations in all layers, but \ufb01rst and last. The suggested methods are two orders of magnitude faster than the traditional Quantize Aware Training approach used for lower than 8-bit quantization. We open-sourced our code https://github.com/papers-submission/CalibTIP."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Actnn%3A_Reducing_training_memory_footprint_via_2-bit_activation_compressed_training/","title":"Actnn: Reducing training memory footprint via 2-bit activation compressed training","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Actnn%3A_Reducing_training_memory_footprint_via_2-bit_activation_compressed_training/#summary","title":"Summary","text":"<p>Summary: The ActNN framework is proposed in this research paper, which is a memory-efficient training method that reduces the memory footprint required while training neural networks. It quantizes activations randomly for back propagation and utilizes mixed-precision quantization strategies to take advantage of activation's heterogeneity. The framework requires less memory and provides negligible accuracy loss, allowing for a larger batch size during training. The convergence of ActNN is proved for general network architectures, and quantization's impact on convergence is also characterized through gradient variance. It can be applied to existing dynamic graph frameworks like PyTorch by substituting the layers.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Actnn%3A_Reducing_training_memory_footprint_via_2-bit_activation_compressed_training/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Actnn%3A_Reducing_training_memory_footprint_via_2-bit_activation_compressed_training/#content","title":"Content","text":"<p> In this research paper, the authors propose ActNN, a memory-efficient training framework that reduces the training memory footprint of neural networks. ActNN stores randomly quantized activations for back propagation and utilizes mixed-precision quantization strategies that exploit the activation's heterogeneity across feature dimensions, samples, and layers. The proposed framework provides negligible accuracy loss and reduces the activation's memory footprint by 12 times, enabling training with a 6.6 to 14 times larger batch size. The authors prove the convergence of ActNN for general network architectures and characterize the impact of quantization on the convergence via an exact expression for the gradient variance. The proposed method can be readily applied to existing dynamic graph frameworks, such as PyTorch, by substituting the layers."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adabin%3A_Improving_binary_neural_networks_with_adaptive_binary_sets/","title":"Adabin: Improving binary neural networks with adaptive binary sets","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adabin%3A_Improving_binary_neural_networks_with_adaptive_binary_sets/#summary","title":"Summary","text":"<p> The paper proposes a novel technique named AdaBin that uses binary quantization function based on the center position and distance of 1-bit values and gradient-based optimization method to find the optimal binary sets of activation for each layer, rather than using a fixed set. Using this technique, the paper achieves state-of-the-art performance in Binary Neural Networks, obtaining a 66.4% Top-1 accuracy on ImageNet using ResNet-18 architecture and a 69.4 mAP on PASCAL VOC using SSD300."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adabin%3A_Improving_binary_neural_networks_with_adaptive_binary_sets/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adabin%3A_Improving_binary_neural_networks_with_adaptive_binary_sets/#content","title":"Content","text":"<p>This paper studies Binary Neural Networks (BNNs) that binarize weights and activations into 1-bit values, reducing memory usage and computational complexity. However, conventional sign functions cannot effectively binarize complex architectures with diverse weight and activation distributions. To solve this, this paper presents AdaBin, a simple approach to adaptively obtain optimal binary sets for each layer, rather than a fixed set. The method uses a new binary quantization function based on the center position and distance of 1-bit values, and introduces an equalization method for aligning the symmetrical center of binary distribution to real-valued distribution, and a gradient-based optimization method to find the optimal binary sets of activations. AdaBin achieves state-of-the-art performance, obtaining a 66.4% Top-1 accuracy on the ImageNet using ResNet-18 architecture and a 69.4 mAP on PASCAL VOC using SSD300."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adabits%3A_Neural_network_quantization_with_adaptive_bit-widths/","title":"Adabits: Neural network quantization with adaptive bit-widths","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adabits%3A_Neural_network_quantization_with_adaptive_bit-widths/#summary","title":"Summary","text":"<p> The paper proposes a new concept of adaptive bit-widths of weights and activations in deep neural networks for flexible and efficient deployment on different platforms. They experiment with different approaches and discover that joint training produces comparable performance on the adaptive model as individual models. They also propose a new technique named Switchable Clipping Level (S-CL) to further improve quantized models at the lowest bit-width. The proposed techniques can offer a distinct opportunity for improved accuracy-efficiency trade-off as well as instant adaptation according to the platform constraints in real-world applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adabits%3A_Neural_network_quantization_with_adaptive_bit-widths/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adabits%3A_Neural_network_quantization_with_adaptive_bit-widths/#content","title":"Content","text":"<p>  In this paper, the authors propose a new concept, adaptive bit-widths of weights and activations in deep neural networks, to achieve flexible and efficient deployment on different platforms with resource budgets. They experiment with several approaches, including direct adaptation, progressive training, and joint training, and discover that joint training produces comparable performance on the adaptive model as individual models. They also propose a new technique named Switchable Clipping Level (S-CL) to further improve quantized models at the lowest bit-width. They apply these techniques to several models and demonstrate that bit-width of weights and activations is a new option for adaptively executable deep neural networks, offering a distinct opportunity for improved accuracy-efficiency trade-off as well as instant adaptation according to the platform constraints in real-world applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_Binary_Quantization_for_Fast_Nearest_Neighbor_Search./","title":"Adaptive Binary Quantization for Fast Nearest Neighbor Search.","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_Binary_Quantization_for_Fast_Nearest_Neighbor_Search./#summary","title":"Summary","text":"<p>The paper proposes an adaptive binary quantization approach to generate discriminative binary codes using prototype-based hashing method. The approach learns a hash function that generates small unique binary codes for prototypes, addressing the problem of insufficient coding. Through alternating optimization, the method adapts to different prototype and binary code set sizes and can be generalized to the product space for long hash codes. Experiments on large-scale datasets show significant improvement relative to existing hashing methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_Binary_Quantization_for_Fast_Nearest_Neighbor_Search./#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_Binary_Quantization_for_Fast_Nearest_Neighbor_Search./#content","title":"Content","text":"<p>Hashing has been proved an attractive technique for fast nearest neighbor search over big data. Compared to the projection based hashing methods, prototype based ones own stronger capability of generating discriminative binary codes for the data with complex inherent structure. However, our observation indicates that they still suffer from the insufficient coding that usually utilizes the complete binary codes in a hypercube. To address this problem, we propose an adaptive binary quantization method that learns a discriminative hash function with prototypes correspondingly associated with small unique binary codes. Our alternating optimization adaptively discovers the prototype set and the code set of a varying size in an efficient way, which together robustly approximate the data relations. Our method can be naturally generalized to the product space for long hash codes. We believe that our idea serves as a very helpful insight to hashing research. The extensive experiments on four large-scale (up to 80 million) datasets demonstrate that our method significantly outperforms state-of-the-art hashing methods, with up to 58.84% performance gains relatively."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_Discrete_Communication_Bottlenecks_with_Dynamic_Vector_Quantization/","title":"Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_Discrete_Communication_Bottlenecks_with_Dynamic_Vector_Quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method for dynamically selecting discretization tightness through vector quantization based on the complexity of data to improve model performance on visual reasoning and reinforcement learning tasks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_Discrete_Communication_Bottlenecks_with_Dynamic_Vector_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_Discrete_Communication_Bottlenecks_with_Dynamic_Vector_Quantization/#content","title":"Content","text":"<p>Vector Quantization (VQ) is a method for discretizing latent representations and has become a major part of the deep learning toolkit. In this work, we propose learning to dynamically select discretization tightness conditioned on inputs, based on the hypothesis that data naturally contains variations in complexity that call for different levels of representational coarseness. We show that dynamically varying tightness in communication bottlenecks can improve model performance on visual reasoning and reinforcement learning tasks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_loss-aware_quantization_for_multi-bit_networks/","title":"Adaptive loss-aware quantization for multi-bit networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_loss-aware_quantization_for_multi-bit_networks/#summary","title":"Summary","text":"<p>Summary: The proposed Adaptive Loss-aware Quantization (ALQ) method compresses deep neural networks into multi-bit networks (MBNs) with an average bitwidth below one-bit and minimal loss in inference accuracy. Unlike previous MBN quantization solutions, ALQ directly minimizes the quantization-induced error on the loss function and utilizes strategies such as adaptive bitwidth, smooth bitwidth reduction, and iterative trained quantization. Experimental results on image datasets show that ALQ outperforms state-of-the-art compressed networks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_loss-aware_quantization_for_multi-bit_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_loss-aware_quantization_for_multi-bit_networks/#content","title":"Content","text":"<p> We propose Adaptive Loss-aware Quantization (ALQ) for the compression of deep neural networks into multi-bit networks (MBNs) that achieve an average bitwidth below one-bit without notable loss in inference accuracy. Unlike previous MBN quantization solutions that train a quantizer by minimizing the error to reconstruct full precision weights, ALQ directly minimizes the quantization-induced error on the loss function involving neither gradient approximation nor full precision maintenance. ALQ also exploits strategies including adaptive bitwidth, smooth bitwidth reduction, and iterative trained quantization to allow a smaller network size without loss in accuracy. Experiment results on popular image datasets show that ALQ outperforms state-of-the-art compressed networks in terms of both storage and accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_quantization_of_neural_networks/","title":"Adaptive quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper proposes a new method called adaptive quantization to simplify the trained Deep Neural Network (DNN) models by finding optimal precision for each network parameter. The optimization problem uses linear functions, which makes it computationally cheap and provides a closed-form approximate solution. The experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near state-of-the-art reduction in model size with similar error rates and compressions close to floating-point model compression methods without loss of accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adaptive_quantization_of_neural_networks/#content","title":"Content","text":"<p>Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classi\ufb01cation problems, their deployment onto resource constrained edge computing devices remains challenging due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the changes in the loss function when performing quantization, nor do they take the different importances of DNN model parameters to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simpli\ufb01es a trained DNN model by \ufb01nding a unique, optimal precision for each network parameter such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each parameter and assigns it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rates. Furthermore, it can achieve compressions close to \ufb02oating-point model compression methods without loss of accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Additive_powers-of-two_quantization%3A_An_efficient_non-uniform_discretization_for_neural_networks/","title":"Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Additive_powers-of-two_quantization%3A_An_efficient_non-uniform_discretization_for_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes Additive Powers-of-Two (APoT) quantization to efficiently and effectively quantize the bell-shaped and long-tailed weight and activation distribution in neural networks. The APoT quantization performs well due to high computational efficiency and good match with weight distribution, and uses simple reparameterization of the clipping function and weight normalization to refine weight distribution for more stable and consistent training. Experimental results show the proposed APoT quantization outperforms state-of-the-art methods and achieves competitive accuracy with full-precision models. For example, 4-bit quantized ResNet-50 on ImageNet achieves a 76.6% top-1 accuracy and reduces computational cost by 22% compared with uniformly quantized counterpart, without implementing bells and whistles."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Additive_powers-of-two_quantization%3A_An_efficient_non-uniform_discretization_for_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Additive_powers-of-two_quantization%3A_An_efficient_non-uniform_discretization_for_neural_networks/#content","title":"Content","text":"<p> We propose Additive Powers-of-Two (APoT) quantization, an efficient non-uniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models, demonstrating the effectiveness of our proposed APoT quantization. For example, our 4-bit quantized ResNet-50 on ImageNet achieves 76.6% top-1 accuracy without bells and whistles; meanwhile, our model reduces 22% computational cost compared with the uniformly quantized counterpart."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adversarial_attack_on_deep_product_quantization_network_for_image_retrieval/","title":"Adversarial attack on deep product quantization network for image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adversarial_attack_on_deep_product_quantization_network_for_image_retrieval/#summary","title":"Summary","text":"<p>This paper proposes PQ-AG, a method to generate imperceptible adversarial perturbations for query images to form adversarial queries to investigate the vulnerability of Deep product quantization network (DPQN) to adversarial examples. The method successfully creates adversarial examples to mislead targeted product quantization retrieval models, degrading retrieval performance in both white-box and black-box settings."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adversarial_attack_on_deep_product_quantization_network_for_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Adversarial_attack_on_deep_product_quantization_network_for_image_retrieval/#content","title":"Content","text":"<p>Deep product quantization network (DPQN) has received attention in fast image retrieval tasks due to its efficiency of encoding high-dimensional visual features. However, recent studies show that deep neural networks (DNNs) are vulnerable to adversarial examples. Little effort has been devoted to investigating how adversarial examples affect DPQN. This paper proposes product quantization adversarial generation (PQ-AG), a method to generate imperceptible adversarial perturbations for query images to form adversarial queries. PQ-AG successfully creates adversarial examples to mislead targeted product quantization retrieval models, degrading retrieval performance in both white-box and black-box settings."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/AlignQ%3A_Alignment_Quantization_with_ADMM-based_Correlation_Preservation/","title":"AlignQ: Alignment Quantization with ADMM-based Correlation Preservation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/AlignQ%3A_Alignment_Quantization_with_ADMM-based_Correlation_Preservation/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new quantization method called AlignQ, which minimizes the quantization error caused by distribution differences between training and testing data using the cumulative distribution function. The method uses ADMM optimization to minimize differences in data correlations before and after quantization to retain prediction information. Experimental results show that the AlignQ method performs better, particularly in low-bit models. Code is available on GitHub.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/AlignQ%3A_Alignment_Quantization_with_ADMM-based_Correlation_Preservation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/AlignQ%3A_Alignment_Quantization_with_ADMM-based_Correlation_Preservation/#content","title":"Content","text":"<p>  We propose a new quantization scheme called Alignment Quantization with ADMM-based Correlation Preservation (AlignQ), which aims to minimize the quantization error induced by distribution differences between training and testing data. AlignQ exploits the cumulative distribution function (CDF) to align the data to be independently and identically distributed (i.i.d.) for quantization error minimization. Then, we leverage the Alternating Direction Method of Multipliers (ADMM) optimization to minimize the differences in data correlations before and after the alignment and quantization. Our theoretical analysis indicates that the significant changes in data correlations after the quantization induces a large quantization error; accordingly, we aim to preserve the relationship of data from the original space to the aligned quantization space for retaining the prediction information. Experimental results demonstrated that AlignQ achieves significant performance improvements, especially in low-bit models. Code is available at https://github.com/tinganchen/AlignQ.git."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/All_you_need_is_a_few_shifts%3A_Designing_efficient_convolutional_neural_networks_for_image_classification/","title":"All you need is a few shifts: Designing efficient convolutional neural networks for image classification","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/All_you_need_is_a_few_shifts%3A_Designing_efficient_convolutional_neural_networks_for_image_classification/#summary","title":"Summary","text":"<p>The paper proposes the use of shift operations to create efficient convolutional neural networks. They introduce a new basic component called Sparse Shift Layer (SSL). A shift operation penalty is introduced during optimization, and a quantization-aware shift learning method is proposed. The paper shows that only a few shift operations are enough for spatial communication. Lastly, an improved network architecture named FE-Net is introduced that achieves 75.0% top-1 accuracy on ImageNet with only 563M M-Adds."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/All_you_need_is_a_few_shifts%3A_Designing_efficient_convolutional_neural_networks_for_image_classification/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/All_you_need_is_a_few_shifts%3A_Designing_efficient_convolutional_neural_networks_for_image_classification/#content","title":"Content","text":"<p> Shift operation is introduced in this paper to construct efficient convolutional neural networks. A new and novel basic component named Sparse Shift Layer (SSL) is added to the architecture in which the basic block is only composed of 1x1 convolutional layers with only a few shift operations applied to the intermediate feature maps. They introduce a shift operation penalty during optimization and further propose a quantization-aware shift learning method to make the idea feasible. It is shown that only a few shift operations are sufficient to provide spatial information communication. Furthermore, an improved network architecture named FE-Net is introduced which can achieve 75.0% top-1 accuracy on ImageNet with only 563M M-Adds."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Alps%3A_Adaptive_Quantization_of_Deep_Neural_Networks_with_GeneraLized_PositS/","title":"Alps: Adaptive Quantization of Deep Neural Networks with GeneraLized PositS","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Alps%3A_Adaptive_Quantization_of_Deep_Neural_Networks_with_GeneraLized_PositS/#summary","title":"Summary","text":"<p> The paper presents a new adaptive quantization algorithm using generalized posit format to represent dynamic range and distribution of deep neural network parameters. The algorithm is achieved by minimizing the intra-layer posit quantization error using a compander. The efficacy of the proposed quantization algorithm is studied on ResNet-50 and EfficientNet models for classification tasks within a new low-precision framework called ALPS. Results indicate that the new algorithm outperforms other numerical formats, including standard posits, in terms of accuracy and energy dissipation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Alps%3A_Adaptive_Quantization_of_Deep_Neural_Networks_with_GeneraLized_PositS/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Alps%3A_Adaptive_Quantization_of_Deep_Neural_Networks_with_GeneraLized_PositS/#content","title":"Content","text":"<p>In this paper, a new adaptive quantization algorithm for generalized posit format is presented, to optimally represent the dynamic range and distribution of deep neural network parameters. Adaptation is achieved by minimizing the intra-layer posit quantization error with a compander. The efficacy of the proposed quantization algorithm is studied within a new low-precision framework, ALPS, on ResNet-50 and EfficientNet models for classification tasks. Results assert that the accuracy and energy dissipation of low-precision DNNs using generalized posits outperform other well-known numerical formats, including standard posits."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Alternating_multi-bit_quantization_for_recurrent_neural_networks/","title":"Alternating multi-bit quantization for recurrent neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Alternating_multi-bit_quantization_for_recurrent_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes quantizing both weights and activations into multiple binary codes to compress Recurrent Neural Networks (RNNs) and address issues related to deploying them on portable devices with limited resources and large latency during inference on servers with concurrent requests. The proposed method uses alternating minimization and achieves memory saving and acceleration in real inference without large loss in accuracy. The performance of this method is tested on two RNNs - LSTM and GRU in language models and is further extended to image classification tasks with excellent results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Alternating_multi-bit_quantization_for_recurrent_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Alternating_multi-bit_quantization_for_recurrent_neural_networks/#content","title":"Content","text":"<p> Recurrent neural networks (RNNs) have achieved state-of-art performance in many applications, but they often have too many parameters to deploy on portable devices with limited resources and large latency during inference on servers with concurrent requests. To solve these issues, the authors propose quantizing the network, both weights, and activations, into multiple binary codes f"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/An_efficient_framework_for_zero-shot_sketch-based_image_retrieval/","title":"An efficient framework for zero-shot sketch-based image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/An_efficient_framework_for_zero-shot_sketch-based_image_retrieval/#summary","title":"Summary","text":"<p> The paper discusses Zero-shot Sketch-based Image Retrieval (ZS-SBIR), which is a challenging and realistic variant of SBIR since it involves multiple computer vision problems such as content-based Image Retrieval (CBIR), zero-shot learning and domain adaptation. The proposed method that includes knowledge distillation and similarity learning achieves state-of-the-art results and outperforms related works significantly."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/An_efficient_framework_for_zero-shot_sketch-based_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/An_efficient_framework_for_zero-shot_sketch-based_image_retrieval/#content","title":"Content","text":"<p>Recently, Zero-shot Sketch-based Image Retrieval (ZS-SBIR) has attracted the attention of the computer vision community due to it's real-world applications, and the more realistic and challenging setting than found in SBIR. ZS-SBIR inherits the main challenges of multiple computer vision problems including content- based Image Retrieval (CBIR), zero-shot learning and domain adaptation. The proposed method achieves state-of-the-art results, and outperforms the majority of related works by a large margin. Keywords: Sketch-based Image Retrieval, Zero-shot Learning, Knowledge Distillation, Similarity Learning"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/An_energy-efficient_quantized_and_regularized_training_framework_for_processing-in-memory_accelerators/","title":"An energy-efficient quantized and regularized training framework for processing-in-memory accelerators","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/An_energy-efficient_quantized_and_regularized_training_framework_for_processing-in-memory_accelerators/#summary","title":"Summary","text":"<p>Summary: The paper proposes an energy-efficient quantized and regularized training framework for PIM accelerators, which involves a PIM-based non-uniform activation quantization scheme and an energy-aware weight regularization method. The proposed framework can improve energy efficiency by reducing ADC resolution requirements and training low energy consumption CNN models for PIM without much accuracy loss. Experimental results show that the proposed framework can reduce the resolution of ADCs by 2 bits and computing energy consumption in the analog domain by 35%.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/An_energy-efficient_quantized_and_regularized_training_framework_for_processing-in-memory_accelerators/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/An_energy-efficient_quantized_and_regularized_training_framework_for_processing-in-memory_accelerators/#content","title":"Content","text":"<p>Convolutional Neural Networks (CNNs) have made breakthroughs in various fields, while the energy consumption becomes enormous. Processing-In-Memory (PIM) architectures based on emerging non-volatile memory have demonstrated great potential in improving the energy efficiency of CNN computing. In this paper, we propose an energy-ef\ufb01cient quantized and regularized training framework for PIM accelerators, which consists of a PIM-based non-uniform activation quantization scheme and an energy-aware weight regularization method. The proposed framework can improve the energy ef\ufb01ciency of PIM architectures by reducing the Analog-to-Digital Converters (ADCs) resolution requirements and training low energy consumption CNN models for PIM, with little accuracy loss. The experimental results show that the proposed training framework can reduce the resolution of ADCs by 2bits and the computing energy consumption in the analog domain by 35%. The energy efficiency can be enhanced by 3.4\u00d7 in our proposed training framework."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Analysis_of_quantized_models/","title":"Analysis of quantized models","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Analysis_of_quantized_models/#summary","title":"Summary","text":"<p>Summary: The paper discusses the use of weight and gradient quantization for deep neural networks to increase storage and fast inference on low-end devices. Theoretical analysis and empirical experiments confirm that quantized networks can speed up training while having comparable performance as full-precision networks. Clipping the gradient before quantization allows for the use of fewer bits for gradient quantization.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Analysis_of_quantized_models/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Analysis_of_quantized_models/#content","title":"Content","text":"<p>Deep neural networks are usually huge, which significantly limits the deployment on low-end devices. In recent years, many weight-quantized models have been proposed. They have small storage and fast inference, but training can still be time-consuming. This can be improved with distributed learning. To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. In this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence. We show that (i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension; (ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and (iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization. Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Anchor-based_plain_net_for_mobile_image_super-resolution/","title":"Anchor-based plain net for mobile image super-resolution","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Anchor-based_plain_net_for_mobile_image_super-resolution/#summary","title":"Summary","text":"<p>The paper proposes an efficient 8-bit quantization architecture for image super-resolution on mobile devices called anchor-based plain net (ABPN) that outperforms other quantized models. The authors focus on creating a general SISR network that is beneficial to 8-bit quantization and adopt a quantization-aware training strategy to further boost performance. They also research meta-node latency on mobile hardware to determine what kind of architecture is most effective for INT8 quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Anchor-based_plain_net_for_mobile_image_super-resolution/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Anchor-based_plain_net_for_mobile_image_super-resolution/#content","title":"Content","text":"<p>In this paper, the authors propose an efficient architecture for 8-bit quantization that can be deployed on mobile devices for image super-resolution. They conducted experiments on lightweight SR architectures to determine portable operations that can be utilized, and propose anchor-based plain net (ABPN) which outperforms quantized FSRCNN by nearly 2dB in terms of PSNR. The model satisfies realistic needs by adopting quantization-aware training strategy to further boost the performance. The authors' focus is on creating a general SISR network architecture that is beneficial to 8-bit quantization. Their main contribution is researching meta-node latency on mobile hardware and digging deeper into what kind of architecture can really make sense to INT8 quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/And_the_bit_goes_down%3A_Revisiting_the_quantization_of_neural_networks/","title":"And the bit goes down: Revisiting the quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/And_the_bit_goes_down%3A_Revisiting_the_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper introduces a vector quantization method to reduce the memory usage of convolutional network architectures. It focuses on preserving the quality of the reconstruction of the network outputs instead of its weights. By using byte-aligned codebooks, the method allows for efficient inference on CPU, and it only requires a set of unlabelled data at quantization time. The approach was validated by quantizing a ResNet-50 model to a 5MB memory size while maintaining a top-1 accuracy of 76.1% on ImageNet object classification and compressing a Mask R-CNN with a 26x factor.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/And_the_bit_goes_down%3A_Revisiting_the_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/And_the_bit_goes_down%3A_Revisiting_the_quantization_of_neural_networks/#content","title":"Content","text":"<p>In this paper, we address the problem of reducing the memory footprint of convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. Our method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. We validate our approach by quantizing a high performing ResNet-50 model to a memory size of 5MB while preserving a top-1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26x factor."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ant%3A_Exploiting_adaptive_numerical_data_type_for_low-bit_deep_neural_network_quantization/","title":"Ant: Exploiting adaptive numerical data type for low-bit deep neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ant%3A_Exploiting_adaptive_numerical_data_type_for_low-bit_deep_neural_network_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a fixed-length adaptive numerical data type called ANT that uses flint - a specific data type combining the advantages of float and int to adapt to the importance of different values within a tensor, and an adaptive framework to select the best type for each tensor based on its distribution characteristics. The unified processing element architecture for ANT shows significant improvement in speed and energy efficiency compared to the existing state-of-the-art quantization accelerators.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ant%3A_Exploiting_adaptive_numerical_data_type_for_low-bit_deep_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ant%3A_Exploiting_adaptive_numerical_data_type_for_low-bit_deep_neural_network_quantization/#content","title":"Content","text":"<p>Quantization techniques are used to reduce the computation and memory cost of deep neural network (DNN) models. However, existing quantization solutions have limited benefits and require more bits to maintain the accuracy of original models. To address this, this paper proposes a fixed-length adaptive numerical data type called ANT, which leverages two key innovations to exploit the intra-tensor and inter-tensor adaptive opportunities in DNN models. First, a particular data type called flint is proposed, which combines the advantages of float and int for adapting to the importance of different values within a tensor. Second, an adaptive framework that selects the best type for each tensor according to its distribution characteristics is proposed. The authors design a unified processing element architecture for ANT and show its ease of integration with existing DNN accelerators. Their design results in 2.8 speedup and 2.5 energy efficiency improvement over the state-of-the-art quantization accelerators."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Any-precision_deep_neural_networks/","title":"Any-precision deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Any-precision_deep_neural_networks/#summary","title":"Summary","text":"<p>The paper presents any-precision deep neural networks that are trained with a new method to allow the learned DNNs to be flexible in numerical precision. The same model can be directly set to different bit-widths in runtime, enabling dynamic speed and accuracy trade-offs. The proposed framework achieves this flexibility without any performance degradation and can be applied to multiple vision tasks. This allows flexible deployment of deep learning models in real-world applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Any-precision_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Any-precision_deep_neural_networks/#content","title":"Content","text":"<p>We present any-precision deep neural networks (DNNs), which are trained with a new method that allows the learned DNNs to be \ufb02exible in numerical precision during inference. The same model in runtime can be \ufb02exibly and directly set to different bit-widths, by truncating the least significant bits, to support dynamic speed and accuracy trade-off. When all layers are set to low-bits, we show that the model achieved accuracy comparable to dedicated models trained at the same precision. This nice property facilitates \ufb02exible deployment of deep learning models in real-world applications, where in practice trade-offs between model accuracy and runtime ef\ufb01ciency are often sought. Previous literature presents solutions to train models at each individual \ufb01xed ef\ufb01ciency/accuracy trade-off point. But how to produce a model \ufb02exible in runtime precision is largely unexplored. When the demand of ef\ufb01ciency/accuracy trade-off varies from time to time or even dynamically changes in runtime, it is infeasible to re-train models accordingly, and the storage budget may forbid keeping multiple models. Our proposed framework achieves this \ufb02exibility without performance degradation. More importantly, we demonstrate that this achievement is agnostic to model architectures and applicable to multiple vision tasks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Apq%3A_Joint_search_for_network_architecture%2C_pruning_and_quantization_policy/","title":"Apq: Joint search for network architecture, pruning and quantization policy","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Apq%3A_Joint_search_for_network_architecture%2C_pruning_and_quantization_policy/#summary","title":"Summary","text":"<p>Summary: The paper presents APQ, a design methodology for efficient deep learning deployment. They optimize the neural network architecture, pruning policy, and quantization policy jointly. To deal with the larger design space, they train a quantization-aware accuracy predictor that is fed to the evolutionary search to select the best fit. They propose a predictor-transfer technique to get the quantization-aware predictor to reduce the quantization data collection time.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Apq%3A_Joint_search_for_network_architecture%2C_pruning_and_quantization_policy/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Apq%3A_Joint_search_for_network_architecture%2C_pruning_and_quantization_policy/#content","title":"Content","text":"<p> We present APQ, a novel design methodology for efficient deep learning deployment. Unlike previous methods that separately optimize the neural network architecture, pruning policy, and quantization policy, we design to optimize them in a joint manner. To deal with the larger design space it brings, we devise to train a quantization-aware accuracy predictor that is fed to the evolutionary search to select the best fit. Since directly training such a predictor requires time-consuming quantization data collection, we propose to use predictor-transfer technique to get the quantization-aware predictor: we first generate a large dataset of AN architecture, ImageNet accuracy pair by sampling a pretrained unified once-for-all network and doing direct evaluation; then we use these data to train an accuracy predictor without quantization, followed by transferring its weights to train the quantization-aware predictor, which largely reduces the quantization data collection time."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Arbitrary_Bit-width_Network%3A_A_Joint_Layer-Wise_Quantization_and_Adaptive_Inference_Approach/","title":"Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Arbitrary_Bit-width_Network%3A_A_Joint_Layer-Wise_Quantization_and_Adaptive_Inference_Approach/#summary","title":"Summary","text":"<p>The paper proposes the Arbitrary Bit-width Network (ABN) which suggests using varying quantization schemes for different data samples, which may vary in recognition difficulty. This approach achieves data-dependent dynamic inference at the layer level. ABN utilizes a Markov Decision Process (MDP) to help determine the runtime bit-width for each layer based on well-trained super-network. The proposed technique resulted in 1.1% top1 accuracy improvement while saving 36.2% BitOps on ImageNet classification."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Arbitrary_Bit-width_Network%3A_A_Joint_Layer-Wise_Quantization_and_Adaptive_Inference_Approach/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Arbitrary_Bit-width_Network%3A_A_Joint_Layer-Wise_Quantization_and_Adaptive_Inference_Approach/#content","title":"Content","text":"<p>Conventional model quantization methods use a fixed quantization scheme to different data samples, which ignores the inherent \u201crecognition difficulty\u201d differences between various samples. We propose to feed different data samples with varying quantization schemes to achieve a data-dependent dynamic inference, at a fine-grained layer level. To solve this problem, we present the Arbitrary Bit-width Network (ABN), where the bit-widths of a single deep network can change at runtime for different data samples, with a layer-wise granularity. Based on the well-trained super-network, each layer\u2019s runtime bit-width selection decision is modeled as a Markov Decision Process (MDP) and solved by an adaptive inference strategy accordingly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement while saving 36.2% BitOps."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Asymmetric_gained_deep_image_compression_with_continuous_rate_adaptation/","title":"Asymmetric gained deep image compression with continuous rate adaptation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Asymmetric_gained_deep_image_compression_with_continuous_rate_adaptation/#summary","title":"Summary","text":"<p>This paper proposes a continuously rate adjustable learned image compression framework, Asymmetric Gained Variational Autoencoder (AG-VAE), which achieves discrete rate adaptation in a single model with negligible additional computation. Continuous rate adaptation is achieved by using exponential interpolation without performance compromise. The paper also introduces the asymmetric Gaussian entropy model for more accurate entropy estimation. The proposed method achieves comparable quantitative performance with State-Of-The-Art (SOTA) learned image compression methods and better qualitative performance than classical image codecs. The study confirms the usefulness and superiority of gain units and the asymmetric Gaussian entropy model."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Asymmetric_gained_deep_image_compression_with_continuous_rate_adaptation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Asymmetric_gained_deep_image_compression_with_continuous_rate_adaptation/#content","title":"Content","text":"<p>With the development of deep learning techniques, the combination of deep learning with image compression has drawn lots of attention. Recently, learned image compression methods had exceeded their classical counterparts in terms of rate-distortion performance. However, continuous rate adaptation remains an open question. In this paper, we propose a continuously rate adjustable learned image compression framework, Asymmetric Gained Variational Autoencoder (AG-VAE). AG-VAE utilizes a pair of gain units to achieve discrete rate adaptation in one single model with a negligible additional computation. Then, by using exponential interpolation, continuous rate adaptation is achieved without compromising performance. Besides, we propose the asymmetric Gaussian entropy model for more accurate entropy estimation. Exhaustive experiments show that our method achieves comparable quantitative performance with SOTA learned image compression methods and better qualitative performance than classical image codecs. In the ablation study, we confirm the usefulness and superiority of gain units and the asymmetric Gaussian entropy model."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Asymmetric_mapping_quantization_for_nearest_neighbor_search/","title":"Asymmetric mapping quantization for nearest neighbor search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Asymmetric_mapping_quantization_for_nearest_neighbor_search/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel addition-based vector quantization algorithm, Asymmetric Mapping Quantization (AMQ), for performing approximate nearest neighbor (ANN) search. The proposed method maps the query vector and database vector using different mapping functions, solving the problem caused by the norm of the database vector. The paper also proposes Distributed Asymmetric Mapping Quantization (DAMQ) to allow AMQ to work with large datasets by distributed learning. Experiments conducted on approximate nearest neighbor search and image retrieval demonstrate the effectiveness of the proposed methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Asymmetric_mapping_quantization_for_nearest_neighbor_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Asymmetric_mapping_quantization_for_nearest_neighbor_search/#content","title":"Content","text":"<p> Nearest neighbor search is an important problem in computer vision and machine learning. Although hashing methods are traditionally used for approximate nearest neighbor (ANN) search, vector quantization-based methods have recently gained popularity due to their superior accuracy and efficiency. In this paper, a novel addition-based vector quantization algorithm, Asymmetric Mapping Quantization (AMQ), is proposed to perform ANN search. Unlike existing addition-based quantization methods, AMQ maps the query vector and database vector using different mapping functions, transforming the computation of L-2 distance to inner product similarity, and solves the problem caused by the norm of the database vector. The authors also propose Distributed Asymmetric Mapping Quantization (DAMQ) to allow AMQ to work with large datasets by distributed learning. Experiments conducted on approximate nearest neighbor search and image retrieval demonstrate the effectiveness of the proposed methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attentive_region_embedding_network_for_zero-shot_learning/","title":"Attentive region embedding network for zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attentive_region_embedding_network_for_zero-shot_learning/#summary","title":"Summary","text":"<p>Summary: The paper proposes the attentive region embedding network (AREN) for zero-shot learning (ZSL) which utilizes local image regions (parts) to improve the semantic transfer between seen and unseen classes. AREN uses the attentive region embedding (ARE) and the attentive compressed second-order embedding (ACSE) streams to discover the semantic regions that have more discrimination power than attributes, and ACSE is incorporated to ensure stable semantic transfer. AREN sets a new benchmark for ZSL and achieves compelling results for generalized ZSL.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attentive_region_embedding_network_for_zero-shot_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attentive_region_embedding_network_for_zero-shot_learning/#content","title":"Content","text":"<p>Zero-shot learning (ZSL) is a classification task that aims to recognize images from unseen categories by using only seen class images for training. Previous works on ZSL have mainly focused on global features or global regions to construct embeddings in the semantic space, but they do not explore the discrimination power of local image regions (parts), which can assist semantic transfer between seen and unseen classes. This paper proposes the attentive region embedding network (AREN) to discover semantic regions that correspond to semantic attributes and have stronger discrimination than attributes. AREN is end-to-end trainable and incorporates the attentive region embedding (ARE) stream and the attentive compressed second-order embedding (ACSE) stream. ARE discovers multiple part regions under the guidance of attention and compatibility loss, and an adaptive thresholding mechanism suppresses redundant attention regions. ACSE is also incorporated to guarantee more stable semantic transfer. AREN achieves state-of-the-art performance in evaluations on four benchmarks under ZSL setting and compelling results under generalized ZSL setting."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attribute-based_classification_for_zero-shot_visual_object_categorization/","title":"Attribute-based classification for zero-shot visual object categorization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attribute-based_classification_for_zero-shot_visual_object_categorization/#summary","title":"Summary","text":"<p>Summary: The paper discusses zero-shot learning, in which an object is recognized with no training examples, through attribute-based classification. This approach uses semantic attributes to prelearn classifiers independently and identify new classes based on their attribute representation without requiring a new training phase. The paper also introduces a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes. The experiments on this data set and two more show that attribute-based classification is successful in categorizing images without access to any training images of the target classes.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attribute-based_classification_for_zero-shot_visual_object_categorization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attribute-based_classification_for_zero-shot_visual_object_categorization/#content","title":"Content","text":"<p>We study the problem of object recognition for categories for which we have no training examples, a task also called zero-data or zero-shot learning. This situation has hardly been studied in computer vision research, even though it occurs frequently;the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them. To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object\u2019s color or shape. Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task. Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase. In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes. Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attribute-modulated_generative_meta_learning_for_zero-shot_classification/","title":"Attribute-modulated generative meta learning for zero-shot classification","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attribute-modulated_generative_meta_learning_for_zero-shot_classification/#summary","title":"Summary","text":"<p>The research proposes an Attribute-Modulated generative meta-model for Zero-shot learning (AMAZ) to adaptively modify the generator to adapt to highly diverse tasks through an attribute-aware modulation network, an attribute-augmented generative network, and an attribute-weighted classifier, which outperforms state-of-the-art methods by 3.8% and 3.1% in ZSL and generalized ZSL settings on four widely-used benchmarks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attribute-modulated_generative_meta_learning_for_zero-shot_classification/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Attribute-modulated_generative_meta_learning_for_zero-shot_classification/#content","title":"Content","text":"<p>Zero-shot learning (ZSL) requires transferring knowledge from seen classes to unseen classes. While existing meta generative approaches pursue a common model shared across task distributions, this research proposes an Attribute-Modulated generAtive meta-model for Zero-shot learning (AMAZ). AMAZ constructs a generative network adaptive to task characteristics through an attribute-aware modulation network, an attribute-augmented generative network, and an attribute-weighted classifier. The modulation network adaptively modifies the generator to adapt to highly diverse tasks. The weighted classifier utilizes the data quality to enhance the training procedure. Experimental evaluations on four widely-used benchmarks demonstrate that AMAZ outperforms state-of-the-art methods by 3.8% and 3.1% in ZSL and generalized ZSL settings, respectively, demonstrating its superiority."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Augmentation_network_for_generalised_zero-shot_learning/","title":"Augmentation network for generalised zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Augmentation_network_for_generalised_zero-shot_learning/#summary","title":"Summary","text":"<p> This paper proposed a novel method for Generalised Zero-Shot Learning that involves the combination of visual and semantic classification without any gating mechanisms. They used temperature calibration to balance the classification between seen and unseen classes. The proposed method achieved state-of-the-art Generalised Zero-Shot Learning results in benchmark datasets and competitive results in other datasets by tackling multi-modal and multi-domain inference. An ablation study was conducted to justify each stage of the proposed approach. Keywords include generalised zero-shot learning, multi-modal inference, multi-domain inference."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Augmentation_network_for_generalised_zero-shot_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Augmentation_network_for_generalised_zero-shot_learning/#content","title":"Content","text":"<p>Generalised zero-shot learning (GZSL) is a training process where visual and semantic samples from seen and unseen classes are used to classify visual samples from seen and unseen classes. Current approaches use a single modality classifier and gating mechanisms to balance the classification between seen and unseen classes. In this paper, we propose a novel GZSL method that tackles multi-modal and multi-domain inference by combining visual and semantic classification and automatically balancing the seen and unseen classification using temperature calibration, without requiring any gating mechanisms or external domain classifiers. Our method produces state-of-the-art GZSL results for benchmark datasets and obtains competitive results for other datasets. We provide an ablation study to justify each stage of the proposed approach. Keywords: generalised zero-shot learning, multi-modal inference, multi-domain inference."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ausn%3A_approximately_uniform_quantization_by_adaptively_superimposing_non-uniform_distribution_for_deep_neural_networks/","title":"Ausn: approximately uniform quantization by adaptively superimposing non-uniform distribution for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ausn%3A_approximately_uniform_quantization_by_adaptively_superimposing_non-uniform_distribution_for_deep_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes an advanced quantization method called AUSN that uses a decoder-free coding scheme, superposition quantization algorithm, and rounding scheme to harness the bit-width usage, adapt the coding scheme to diverse DNN layers and tasks, and prevent bit-width overflow and re-quantization errors. It is effective for different DNN models, as supported by theoretical analysis and accuracy assessment. FPGA synthesis results demonstrate a 2x reduction in energy consumption and a 2x-4x reduction in hardware resources."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ausn%3A_approximately_uniform_quantization_by_adaptively_superimposing_non-uniform_distribution_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ausn%3A_approximately_uniform_quantization_by_adaptively_superimposing_non-uniform_distribution_for_deep_neural_networks/#content","title":"Content","text":"<p>Quantization of deep neural networks is essential for reducing the complexity of DNN inference in edge applications. However, existing uniform and non-uniform quantization methods inherently have a conflict between representing range and resolving resolution, resulting in either underutilized bit-width or significant accuracy drop. This paper aims to propose a novel quantization method called AUSN that consists of a decoder-free coding scheme, a superposition quantization algorithm, and a rounding scheme. AUSN successfully exploits the bit-width to its extreme, adapts the coding scheme to different DNN layers, models, and tasks without extra hardware design effort, and eliminates bit-width overflow and re-quantization issues. The effectiveness and generalization of AUSN are supported by theoretical analysis and accuracy evaluation on various DNN models of different tasks. Moreover, the synthesis results on FPGA show a reduction of 2x in energy consumption and a 2x to 4x reduction in hardware resources."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Auto-ViT-Acc%3A_An_FPGA-aware_automatic_acceleration_framework_for_vision_transformer_with_mixed-scheme_quantization/","title":"Auto-ViT-Acc: An FPGA-aware automatic acceleration framework for vision transformer with mixed-scheme quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Auto-ViT-Acc%3A_An_FPGA-aware_automatic_acceleration_framework_for_vision_transformer_with_mixed-scheme_quantization/#summary","title":"Summary","text":"<p>The paper proposes an automatic acceleration framework for Vision Transformer using mixed-scheme quantization, and explores the feasibility of CNN quantization schemes on ViTs. In addition, the paper develops a novel FPGA compute engine for ViT multi-head attention, and summarizes the contributions as an FPGA-aware mixed-scheme ViT quantization algorithm, an automated ViT acceleration framework, and a novel FPGA computing engine for ViT multi-head attention."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Auto-ViT-Acc%3A_An_FPGA-aware_automatic_acceleration_framework_for_vision_transformer_with_mixed-scheme_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Auto-ViT-Acc%3A_An_FPGA-aware_automatic_acceleration_framework_for_vision_transformer_with_mixed-scheme_quantization/#content","title":"Content","text":"<p> This work proposes an FPGA-aware automatic acceleration framework for Vision Transformer, which is based on mixed-scheme quantization. The proposed work explores the feasibility of CNN quantization schemes on ViTs and develops an FPGA-aware automatic ViT acceleration framework based on the mixed-scheme ViT quantization algorithm. The framework also designs a novel FPGA compute engine for ViT multi-head attention. The contributions of the work are summarized as follows: an FPGA-aware mixed-scheme ViT quantization algorithm, an automated ViT acceleration framework, and a novel FPGA computing engine for ViT multi-head attention."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoencoder-based_error_correction_coding_for_one-bit_quantization/","title":"Autoencoder-based error correction coding for one-bit quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoencoder-based_error_correction_coding_for_one-bit_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new error correction coding scheme for AWGN channels using deep learning and one-bit quantization in the receivers. The proposed method gives nearly the same performance as the perfectly trained autoencoder and is as bandwidth efficient as the integrated turbo code. Additionally, the proposed coding outperforms conventional turbo codes even for QPSK modulation.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoencoder-based_error_correction_coding_for_one-bit_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoencoder-based_error_correction_coding_for_one-bit_quantization/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoencoder_based_image_compression%3A_can_the_learning_be_quantization_independent%3F/","title":"Autoencoder based image compression: can the learning be quantization independent?","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoencoder_based_image_compression%3A_can_the_learning_be_quantization_independent%3F/#summary","title":"Summary","text":"<p>This paper proposes the use of a unique learned transform for image compression, rather than learning one transform per rate-distortion point, which saves training time. The approach involves joint learning of the transform and quantization. The paper investigates if compression is affected at test time when quantization step sizes are different from those in the training stage. The work was supported by the French Defense Procurement Agency."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoencoder_based_image_compression%3A_can_the_learning_be_quantization_independent%3F/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoencoder_based_image_compression%3A_can_the_learning_be_quantization_independent%3F/#content","title":"Content","text":"<p>This paper explores the problem of learning transforms for image compression via autoencoders. The authors show that comparable performances can be obtained with a unique learned transform, rather than learning one transform per rate-distortion point at a given quantization step size. The different rate-distortion points are then reached by varying the quantization step size at test time, which saves a lot of training time. The paper proposes an approach where the transform and the quantization are learned jointly and investigates whether, at test time, the compression falls apart when the coefficients obtained with the learned transform are quantized using quantization step sizes which differ from those in the training stage. This work has been supported by the French Defense Procurement Agency (DGA)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_backend-aware_post-training_quantization/","title":"Automated backend-aware post-training quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_backend-aware_post-training_quantization/#summary","title":"Summary","text":"<p>Summary: The paper presents an automated post-training quantization framework called HAGO that provides a set of general quantization graph transformations to find the optimal quantization strategy while satisfying hardware constraints for any model. HAGO achieves a speedup of up to 2.48x on various hardware backends without compromising accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_backend-aware_post-training_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_backend-aware_post-training_quantization/#content","title":"Content","text":"<p>Quantization is a technique used to decrease resource requirements and improve the performance of neural network deployment. However, implementing quantized networks for different hardware backends requires specialized post-training quantization pipelines to be built for each hardware target, an engineering effort that is often too large for developers to keep up with. In this paper, an automated post-training quantization framework called HAGO is presented to tackle this problem. HAGO provides a set of general quantization graph transformations based on a user-defined hardware specification and implements a search mechanism to find the optimal quantization strategy while satisfying hardware constraints for any model. HAGO is shown to achieve speedups of 2.09x, 1.97x, and 2.48x on Intel Xeon Cascade Lake CPUs, NVIDIA Tesla T4 GPUs, ARM Cortex-A CPUs on Raspberry Pi4 relative to full precision, respectively, while maintaining the highest reported post-training quantization accuracy in each case."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_flow_for_compressing_convolution_neural_networks_for_efficient_edge-computation_with_FPGA/","title":"Automated flow for compressing convolution neural networks for efficient edge-computation with FPGA","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_flow_for_compressing_convolution_neural_networks_for_efficient_edge-computation_with_FPGA/#summary","title":"Summary","text":"<p> The paper discusses the quantization of CNN models to lower bit-width parameters and activations for more efficient performance on low-power and low-cost devices. The authors present an automatic flow, from trained TensorFlow models to FPGA system on chip implementation of binarized CNN, and demonstrate its effectiveness through implementation of binarized \"YOLOV2\" on a low-cost, low-power FPGA device with significant performance benefits in terms of model size and inference speed on FPGA as compared to CPU and mobile CPU platforms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_flow_for_compressing_convolution_neural_networks_for_efficient_edge-computation_with_FPGA/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_flow_for_compressing_convolution_neural_networks_for_efficient_edge-computation_with_FPGA/#content","title":"Content","text":"<p> Deep convolutional neural networks (CNN) based solutions are the current state-of-the-art for computer vision tasks. Due to the large size of these models, they are typically run on clusters of CPUs or GPUs. However, power requirements and cost budgets can be a major hindrance in adoption of CNN for IoT applications. Recent research highlights that CNN contain significant redundancy in their structure and can be quantized to lower bit-width parameters and activations, while maintaining acceptable accuracy. Low bit-width and especially single bit-width (binary) CNN are particularly suitable for mobile applications based on FPGA implementation, due to the bitwise logic operations involved in binarized CNN. Moreover, the transition to lower bit-widths opens new avenues for performance optimizations and model improvement. In this paper, we present an automatic flow from trained TensorFlow models to FPGA system on chip implementation of binarized CNN. This flow involves quantization of model parameters and activations, generation of network and model in embedded-C, followed by automatic generation of the FPGA accelerator for binary convolutions. The automated flow is demonstrated through implementation of binarized \"YOLOV2\" on the low cost, low power Cyclone-V FPGA device. Experiments on object detection using binarized YOLOV2 demonstrate significant performance benefit in terms of model size and inference speed on FPGA as compared to CPU and mobile CPU platforms. Furthermore, the entire automated flow from trained models to FPGA synthesis can be completed within one hour."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_log-scale_quantization_for_low-cost_deep_neural_networks/","title":"Automated log-scale quantization for low-cost deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_log-scale_quantization_for_low-cost_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new STLQ-aware training method for logarithmic quantization, which outperforms the previous state-of-the-art method. The proposed method has been applied to ResNet-18 weight parameters, achieving performance similar to APoT at 3-bit precision. It has also been tested on various DNNs in image enhancement and semantic segmentation and produces competitive results.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_log-scale_quantization_for_low-cost_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automated_log-scale_quantization_for_low-cost_deep_neural_networks/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_heterogeneous_quantization_of_deep_neural_networks_for_low-latency_inference_on_the_edge_for_particle_detectors/","title":"Automatic heterogeneous quantization of deep neural networks for low-latency inference on the edge for particle detectors","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_heterogeneous_quantization_of_deep_neural_networks_for_low-latency_inference_on_the_edge_for_particle_detectors/#summary","title":"Summary","text":"<p>The paper introduces a method for designing optimally heterogeneously quantized versions of deep neural network models that can achieve minimum-energy, high-accuracy, nanosecond inference, and fully automated deployment on-chip for edge devices. The automatic quantization procedure per layer and per parameter types by sampling from a wide range of quantizers minimize model energy consumption and size while maintaining high accuracy. Nanosecond inference and a resource consumption reduced by a factor of 50 when implemented on field-programmable gate array hardware are achieved. This can be useful for event selection in proton-proton collisions at CERN Large Hadron Collider where resources are limited, and O(1) s latency is required."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_heterogeneous_quantization_of_deep_neural_networks_for_low-latency_inference_on_the_edge_for_particle_detectors/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_heterogeneous_quantization_of_deep_neural_networks_for_low-latency_inference_on_the_edge_for_particle_detectors/#content","title":"Content","text":"<p>Although the quest for more accurate solutions is pushing deep learning research towards larger and more complex algorithms, edge devices demand efficient inference and therefore reduction in model size, latency and energy consumption. One technique to limit model size is quantization, which implies using fewer bits to represent weights and biases. Such an approach usually results in a decline in performance. Here, we introduce a method for designing optimally heterogeneously quantized versions of deep neural network models for minimum-energy, high-accuracy, nanosecond inference and fully automated deployment on chip. With a per-layer, per-parameter type automatic quantization procedure, sampling from a wide range of quantizers, model energy consumption and size are minimized while high accuracy is maintained. This is crucial for the event selection procedure in proton{proton collisions at the CERN Large Hadron Collider, where resources are strictly limited and a latency of O(1) s is required. Nanosecond inference and a resource consumption reduced by a factor of 50 when implemented on field-programmable gate array hardware are achieved."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_neural_network_compression_by_sparsity-quantization_joint_learning%3A_A_constrained_optimization-based_approach/","title":"Automatic neural network compression by sparsity-quantization joint learning: A constrained optimization-based approach","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_neural_network_compression_by_sparsity-quantization_joint_learning%3A_A_constrained_optimization-based_approach/#summary","title":"Summary","text":"<p> The paper proposes a framework for automatic pruning and quantization of DNNs while targeting a specific model size and maintaining model accuracy. The proposed method can compress the weights data of ResNet-50 to 836\u00d7 smaller without a loss in accuracy on CIFAR-10 and compress AlexNet to be 205\u00d7 smaller without accuracy loss on ImageNet classification. The authors discuss the need for compressing DNNs in resource-constrained devices and introduce pruning and quantization techniques as widely used methods for DNN compression. They propose their constrained optimization-based approach as an alternative to existing solutions that rely on human heuristic or black-box hyper-parameter optimization methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_neural_network_compression_by_sparsity-quantization_joint_learning%3A_A_constrained_optimization-based_approach/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_neural_network_compression_by_sparsity-quantization_joint_learning%3A_A_constrained_optimization-based_approach/#content","title":"Content","text":"<p> In this paper, the authors propose a framework to jointly prune and quantize Deep Neural Networks (DNNs) automatically without using human heuristic or black-box hyper-parameter optimization. The framework aims to compress the DNN models according to a target model size while maintaining their accuracy. The experiments show that their approach can compress the weights data of ResNet-50 to be 836\u00d7 smaller without losing accuracy on CIFAR-10 and compress AlexNet to be 205\u00d7 smaller without accuracy loss on ImageNet classification. The authors discuss the increased demand for deploying DNNs on resource-constrained devices and the critical need for compressing DNNs while maximizing their accuracy. They highlight pruning and quantization techniques as the most widely used methods for DNN compression and introduce the problem of finding the optimal compression ratio (i.e., sparsity or quantization bitwidth) for each layer in a way that meets the given resource budget. They also discuss existing solutions that tune the compression ratio based on human heuristic or black-box hyper-parameter optimization methods and propose their constrained optimization-based approach as an alternative."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_pruning_for_quantized_neural_networks/","title":"Automatic pruning for quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_pruning_for_quantized_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a combination of neural network quantization and pruning to further compress the network. The technique involved an effective pruning strategy for selecting redundant low-precision filters and the use of Bayesian optimization to determine the pruning ratio. The proposed technique achieved successful results on CIFAR-10 and ImageNet datasets with various architectures and precisions.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_pruning_for_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Automatic_pruning_for_quantized_neural_networks/#content","title":"Content","text":"<p> Neural network quantization and pruning are two techniques commonly used to reduce the computational complexity and memory footprint of these models for deployment. However, most existing pruning strategies operate on full-precision and cannot be directly applied to discrete parameter distributions after quantization. In contrast, we study a combination of these two techniques to achieve further network compression. In particular, we propose an effective pruning strategy for selecting redundant low-precision \ufb01lters. Furthermore, we leverage Bayesian optimization to ef\ufb01ciently determine the pruning ratio for each layer. We conduct extensive experiments on CIFAR-10 and ImageNet with various architectures and precisions. In particular, for ResNet-18 on ImageNet, we prune 26.12% of the model size with Binarized Neural Network quantization, achieving a top-1 classi\ufb01cation accuracy of 47.32% in a model of 2.47 MB and 59.30% with a 2-bit DoReFa-Net in 4.36 MB."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoq%3A_Automated_kernel-wise_neural_network_quantization/","title":"Autoq: Automated kernel-wise neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoq%3A_Automated_kernel-wise_neural_network_quantization/#summary","title":"Summary","text":"<p>This paper proposes a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. AutoQ reduces inference latency by 54.06% and decreases inference energy consumption by 50.69% on average, while achieving the same inference accuracy as models quantized by the state-of-the-art DRL-based schemes."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoq%3A_Automated_kernel-wise_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoq%3A_Automated_kernel-wise_neural_network_quantization/#content","title":"Content","text":"<p>Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is dif\ufb01cult for even deep reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agents to \ufb01nd a kernel-wise QBN con\ufb01guration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quan- tized by the state-of-the-art DRL-based schemes, the same models quantized by AutoQ reduce the inference latency by 54.06%, and decrease the inference energy consumption by 50.69% averagely, while achieving the same inference accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoregressive_image_generation_using_residual_quantization/","title":"Autoregressive image generation using residual quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoregressive_image_generation_using_residual_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a two-stage framework for autoregressive modeling of high-resolution images using vector quantization. The framework includes Residual-Quantized VAE and RQ-Transformer, which outperforms existing AR models in generating high-resolution images for different benchmarks of unconditional and conditional image generation.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoregressive_image_generation_using_residual_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Autoregressive_image_generation_using_residual_quantization/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/BSQ%3A_Exploring_bit-level_sparsity_for_mixed-precision_neural_network_quantization/","title":"BSQ: Exploring bit-level sparsity for mixed-precision neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/BSQ%3A_Exploring_bit-level_sparsity_for_mixed-precision_neural_network_quantization/#summary","title":"Summary","text":"<p>The paper proposes a method called bit-level sparsity quantization (BSQ) for mixed-precision quantization of deep neural networks. This method induces bit-level sparsity to enable dynamic precision reduction, leading to a mixed-precision quantization scheme of the original model. With only one hyperparameter, the full mixed-precision space can be explored with a single gradient-based optimization process. Comparing to previous methods, BSQ achieves higher accuracy as well as higher bit reduction on various model architectures on the CIFAR-10 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/BSQ%3A_Exploring_bit-level_sparsity_for_mixed-precision_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/BSQ%3A_Exploring_bit-level_sparsity_for_mixed-precision_neural_network_quantization/#content","title":"Content","text":"<p>Mixed-precision quantization can potentially achieve the optimal tradeoff between performance and compression rate of deep neural networks. However, determining the exact quantization scheme lacks a systematic method. This work proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization by inducing bit-level sparsity. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed-precision quantization scheme of the original model. The method enables the exploration of the full mixed-precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. BSQ achieves both higher accuracy and higher bit reduction on various model architectures on the CIFAR-10 and ImageNet datasets comparing to previous methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Balanced_quantization%3A_An_effective_and_efficient_approach_to_quantized_neural_networks/","title":"Balanced quantization: An effective and efficient approach to quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Balanced_quantization%3A_An_effective_and_efficient_approach_to_quantized_neural_networks/#summary","title":"Summary","text":"<p>The paper discusses the limitations of uniform quantization in Quantized Neural Networks (QNNs), and proposes a new method to ensure the uniform distribution of values. The proposed method addresses the issue of imbalanced distribution of parameters in Neural Networks, which leads to underutilization of available bitwidth."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Balanced_quantization%3A_An_effective_and_efficient_approach_to_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Balanced_quantization%3A_An_effective_and_efficient_approach_to_quantized_neural_networks/#content","title":"Content","text":"<p>Quantized Neural Networks (QNNs), which use low bitwidth numbers for representing parameters and performing computations, have been proposed to reduce the computation complexity, storage size and memory usage. In QNNs, parameters and activations are uniformly quantized, such that the multiplications and additions can be accelerated by bitwise operations. However, distributions of parameters in Neural Networks are often imbalanced, such that the uniform quantization determined from extremal values may underutilize available bitwidth. In this paper, we propose a novel quantization method that can ensure the balance of distributions of quantized values."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Batch_normalization_in_quantized_networks/","title":"Batch normalization in quantized networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Batch_normalization_in_quantized_networks/#summary","title":"Summary","text":"<p> This paper discusses the challenges with training quantized neural networks and highlights the important role of the batch normalization layer in preventing gradient explosion. The authors note that while there have been many studies on batch normalization in full-precision networks, there is a lack of research on its impact in quantized training. They show through experiments that batch normalization is crucial in avoiding gradient explosion."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Batch_normalization_in_quantized_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Batch_normalization_in_quantized_networks/#content","title":"Content","text":"<p> Implementation of quantized neural networks on computing hardware leads to considerable speed up and memory saving. However, quantized deep networks are d if\ufb01cult to train and batch normal- ization (BatchNorm) layer plays an important role in traini ng full-precision and quantized networks. Most studies on BatchNorm are focused on full-precision net works, and there is little research in understanding BatchNorm affect in quantized training whic h we address here. We show BatchNorm avoids gradient explosion which is counter-intuitive and r ecently observed in numerical experiments by other researchers."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Batchquant%3A_Quantized-for-all_architecture_search_with_robust_quantizer/","title":"Batchquant: Quantized-for-all architecture search with robust quantizer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Batchquant%3A_Quantized-for-all_architecture_search_with_robust_quantizer/#summary","title":"Summary","text":"<p>The paper discusses the challenges faced in single-shot quantized neural architecture search and proposed a solution called BatchQuant that allows robust quantizer formulation for fast and stable training of a compact, mixed-precision, weight-sharing supernet. The proposed method can adapt to various scenarios with varying resource constraints, making it suitable for model deployment on edge devices."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Batchquant%3A_Quantized-for-all_architecture_search_with_robust_quantizer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Batchquant%3A_Quantized-for-all_architecture_search_with_robust_quantizer/#content","title":"Content","text":"<p>As the applications of deep learning models on edge devices increase at an accelerating pace, fast adaptation to various scenarios with varying resource constraints has become a crucial aspect of model deployment. As a result, model optimization strategies with adaptive configuration are becoming increasingly popular. While single-shot quantized neural architecture search enjoys flexibility in both model architecture and quantization policy, the combined search space comes with many challenges, including instability when training the weight-sharing supernet and difficulty in navigating the exponentially growing search space. Existing methods tend to either limit the architecture search space to a small set of options or limit the quantization policy search space to fixed precision policies. To this end, we propose BatchQuant, a robust quantizer formulation that allows fast and stable training of a compact, single-shot, mixed-precision, weight-sharing supernet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bayesian_bits%3A_Unifying_quantization_and_pruning/","title":"Bayesian bits: Unifying quantization and pruning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bayesian_bits%3A_Unifying_quantization_and_pruning/#summary","title":"Summary","text":"<p>The paper introduces a practical method for joint mixed precision quantization and pruning through gradient-based optimization called Bayesian Bits. The method uses a sequential doubling of the bit width and quantizes the residual error between the full precision value and the previously rounded value at each stage. The method employs learnable stochastic gates to control the bit width and obtain low bit solutions by performing approximate inference. The experimental results show that the proposed method can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit-width equivalents."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bayesian_bits%3A_Unifying_quantization_and_pruning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bayesian_bits%3A_Unifying_quantization_and_pruning/#content","title":"Content","text":"<p>We introduce Bayesian Bits, a practical method for joint mixed precision quantization and pruning through gradient based optimization. Bayesian Bits employs a novel decomposition of the quantization operation, which sequentially considers doubling the bit width. At each new bit width, the residual error between the full precision value and the previously rounded value is quantized. We then decide whether or not to add this quantized residual error for a higher effective bit width and lower quantization noise. By starting with a power-of-two bit width, this decomposition will always produce hardware-friendly configurations, and through an additional 0-bit option, serves as a unified view of pruning and quantization. Bayesian Bits then introduces learnable stochastic gates, which collectively control the bit width of the given tensor. As a result, we can obtain low bit solutions by performing approximate inference over the gates, with prior distributions that encourage most of them to be switched off. We experimentally validate our proposed method on several benchmark datasets and show that we can learn pruned, mixed precision networks that provide a better trade-off between accuracy and efficiency than their static bit width equivalents."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Benchmarking_Quantized_Neural_Networks_on_FPGAs_with_FINN/","title":"Benchmarking Quantized Neural Networks on FPGAs with FINN","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Benchmarking_Quantized_Neural_Networks_on_FPGAs_with_FINN/#summary","title":"Summary","text":"<p> This article discusses the importance of quantization in neural networks deployed on FPGAs to reduce resource usage while maintaining accuracy. The impact of mixed-precision using 2 to 8 bit precisions and weights with several parallelization configurations has been evaluated using the FINN and Brevitas frameworks. The compressed network can be better parallelized, allowing the deployed network throughput to be 62 times faster."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Benchmarking_Quantized_Neural_Networks_on_FPGAs_with_FINN/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Benchmarking_Quantized_Neural_Networks_on_FPGAs_with_FINN/#content","title":"Content","text":"<p> The cost of training and inference for state-of-the-art neural networks has led to a search for ways to reduce resource usage while maintaining accuracy. Lower precision can result in only minimal loss of accuracy. Deploying a neural network on low-power and low-resource hardware architectures requires recon\ufb01gurable architectures, which have been shown to be more powerful and \ufb02exible than GPUs. This article aims to assess the impact of mixed-precision when applied to neural networks deployed on FPGAs. FINN and Brevitas frameworks are used to assess the importance of quantization and the framework quality, to evaluate the impact of quantization on neural networks using 2 to 8 bit precisions and weights with several parallelization con\ufb01gurations. Equivalent accuracy can be obtained using lower-precision representation and enough training, and the compressed network can be better parallelized, allowing the deployed network throughput to be 62 times faster."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/BiScaled-DNN%3A_Quantizing_long-tailed_datastructures_with_two_scale_factors_for_deep_neural_networks/","title":"BiScaled-DNN: Quantizing long-tailed datastructures with two scale factors for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/BiScaled-DNN%3A_Quantizing_long-tailed_datastructures_with_two_scale_factors_for_deep_neural_networks/#summary","title":"Summary","text":"<p> <p>The paper discusses the use of fixed-point implementations in Deep Neural Networks (DNNs) for energy-constrained platforms. The authors propose a new number representation called BiScaled-FXP, which takes into account that most datastructures in DNNs are long-tailed, with a majority of small values and a small fraction of much larger values. The proposed method is designed to provide better range and resolution for long-tailed datastructures.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/BiScaled-DNN%3A_Quantizing_long-tailed_datastructures_with_two_scale_factors_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/BiScaled-DNN%3A_Quantizing_long-tailed_datastructures_with_two_scale_factors_for_deep_neural_networks/#content","title":"Content","text":"<p>Fixed-point implementations (FxP) are prominently used to realize Deep Neural Networks (DNNs) efficiently on energy-constrained platforms. The choice of bit-width is often constrained by the ability of FxP to represent the entire range of numbers in the datastructure with sufficient resolution. In this work, we leverage a key insight that almost all datastructures in DNNs are long-tailed i.e., a significant majority of the elements are small in magnitude, with a small fraction being orders of magnitude larger. We propose BiScaled-FXP, a new number representation which caters to the disparate range and resolution needs of long-tailed data-structures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binarized_neural_networks/","title":"Binarized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binarized_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper introduces a method to train Binarized Neural Networks that achieved nearly state-of-the-art results over MNIST, CIFAR-10, and SVHN datasets. BNNs drastically reduce memory size and replace most arithmetic operations with bit-wise operations which is expected to substantially improve power-efficiency. The code for training and running BNNs is also available online.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binarized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binarized_neural_networks/#content","title":"Content","text":"<p>We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-ef\ufb01ciency. The code for training and running our BNNs is available online."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binary_neural_networks%3A_A_survey/","title":"Binary neural networks: A survey","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binary_neural_networks%3A_A_survey/#summary","title":"Summary","text":"<p>Summary: This paper presents a comprehensive survey of algorithms used in binary neural networks. These algorithms are categorized into native solutions and optimized ones like minimizing the quantization error, improving the network loss function, and reducing the gradient error. The paper also investigates other practical aspects of binary neural networks and evaluates their performances on different tasks such as image classification, object detection, and semantic segmentation. Finally, the challenges that may be faced in future research are predicted.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binary_neural_networks%3A_A_survey/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binary_neural_networks%3A_A_survey/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binary_quantization_analysis_of_neural_networks_weights_on_MNIST_dataset/","title":"Binary quantization analysis of neural networks weights on MNIST dataset","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binary_quantization_analysis_of_neural_networks_weights_on_MNIST_dataset/#summary","title":"Summary","text":"<p>Summary: This paper proposes a binary scalar quantizer for Laplacian source that is implemented for compressing neural network weights. Two variance range selection criteria are suggested, and the performance is evaluated for a classification task. The experimental results show good matching between theory and practice.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binary_quantization_analysis_of_neural_networks_weights_on_MNIST_dataset/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Binary_quantization_analysis_of_neural_networks_weights_on_MNIST_dataset/#content","title":"Content","text":"<p>This paper discusses the design of a binary scalar quantizer for Laplacian source and its application in compressed neural networks. Two selection criteria for the variance range are proposed, and the binary quantizers are implemented for compressing neural network weights. The performance is analyzed for a simple classification task, and good matching between theory and experiment is observed. The paper concludes with a summary of experimental results obtained by implementation in neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Biqgemm%3A_matrix_multiplication_with_lookup_table_for_binary-coding-based_quantized_dnns/","title":"Biqgemm: matrix multiplication with lookup table for binary-coding-based quantized dnns","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Biqgemm%3A_matrix_multiplication_with_lookup_table_for_binary-coding-based_quantized_dnns/#summary","title":"Summary","text":"<p> This paper proposes a new matrix multiplication method, BiQGEMM, specifically designed for quantized deep neural networks. The method allows for the simultaneous access of multiple quantized weights in a single instruction, resulting in higher performance than traditional schemes when DNNs are quantized. The paper highlights the benefit of quantization in reducing computations and memory requirements in DNNs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Biqgemm%3A_matrix_multiplication_with_lookup_table_for_binary-coding-based_quantized_dnns/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Biqgemm%3A_matrix_multiplication_with_lookup_table_for_binary-coding-based_quantized_dnns/#content","title":"Content","text":"<p>The number of parameters in deep neural networks (DNNs) is rapidly increasing to support complicated tasks and improve model accuracy. Correspondingly, the amount of computations and required memory footprint increase as well. Quantization is an efficient method to address such concerns by compressing DNNs such that computations can be simplified while required storage footprint is significantly reduced. In this paper, we propose a novel matrix multiplication method, called BiQGEMM, dedicated to quantized DNNs. BiQGEMM can access multiple quantized weights simultaneously in one instruction. Our extensive experimental results show that BiQGEMM presents higher performance than conventional schemes when DNNs are quantized."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit-flip_attack%3A_Crushing_neural_network_with_progressive_bit_search/","title":"Bit-flip attack: Crushing neural network with progressive bit search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit-flip_attack%3A_Crushing_neural_network_with_progressive_bit_search/#summary","title":"Summary","text":"<p>Summary: The paper introduces Bit-Flip Attack (BFA), a novel methodology to maliciously flip a small number of bits within the weight storage memory system of a deep neural network (DNN) and presents an algorithm to identify the most vulnerable bits of DNN weight parameters with a minimum number of bit-flips using Progressive Bit Search (PBS) method. The authors were able to successfully attack a ResNet-18 with only 13 bit-flips and raise concerns about the vulnerability of DNNs to fault injection attacks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit-flip_attack%3A_Crushing_neural_network_with_progressive_bit_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit-flip_attack%3A_Crushing_neural_network_with_progressive_bit_search/#content","title":"Content","text":"<p>In this paper, the authors propose a novel methodology to attack the parameters of a deep neural network (DNN) called Bit-Flip Attack (BFA), which maliciously flips a small number of bits within the weight storage memory system. They present an algorithm to identify the most vulnerable bits of DNN weight parameters that can maximize the accuracy degradation with a minimum number of bit-flips. The proposed BFA utilizes a Progressive Bit Search (PBS) method to identify the most vulnerable bit to be flipped. The authors successfully attacked a ResNet-18 with only 13 bit-flips out of 93 million bits, while randomly flipping 100 bits hardly degrades the accuracy by less than 1%. The security challenge of DNN\u2019s parameters is not well explored yet, and fault injection attacks raise concerns about the storage of DNN parameters. The authors highlight that the deployed DNN is vulnerable to popular fault injection techniques such as row hammer and laser beam."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit-slicing_fpga_accelerator_for_quantized_neural_networks/","title":"Bit-slicing fpga accelerator for quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit-slicing_fpga_accelerator_for_quantized_neural_networks/#summary","title":"Summary","text":"<p> This paper proposes a new Quantized Neural Network (QNN) accelerator architecture that supports arbitrary low-precision fixed-point formats for efficient hardware performance of neural networks. The architecture enables processing of the inference pass in large neural networks in a flexible pipelined scheme that minimizes the movement of data to off-chip memory. The design implemented on a modest-sized FPGA offers high computational throughput and energy efficiency."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit-slicing_fpga_accelerator_for_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit-slicing_fpga_accelerator_for_quantized_neural_networks/#content","title":"Content","text":"<p>Deep Neural Networks (DNNs) are commonly used in various applications such as speech recognition and computer vision. Despite their efficiency, they require huge computational resources and thus are not suitable for embedded applications. Extreme quantization, which involves the use of low-precision and fixed-point arithmetic, is a promising path for improving the hardware performance of neural networks. In this paper, we propose a new Quantized Neural Network (QNN) accelerator architecture that supports arbitrary low-precision fixed-point formats. Our architecture enables processing of the inference pass in large neural networks while implementing an efficient streaming data flow in a flexible pipelined scheme that minimizes the movement of data to off-chip memory. The design is implemented on a modest-sized FPGA and offers high computational throughput and energy efficiency."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit_efficient_quantization_for_deep_neural_networks/","title":"Bit efficient quantization for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit_efficient_quantization_for_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper presents a comparison of post-training quantization approaches for deep neural networks that can reduce precision up to 3 bits without significant loss of accuracy. The proposed quantization methods are data-free and maintain the DNN model's optimization with the dataset distribution. The quantization approaches are analyzed across various state-of-the-art DNNs trained with ImageNet. The paper also discusses methods to decrease bit-precision beyond the quantization limits using object class clustering.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit_efficient_quantization_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bit_efficient_quantization_for_deep_neural_networks/#content","title":"Content","text":"<p>Quantization techniques have been a key component to designing efficient deep neural networks (DNNs), which reduce the memory usage of edge devices and optimize low-power inference. In this paper, we present a model-parameter driven comparison of post-training quantization approaches that can reduce the precision to as low as 3 bits with minimal loss of accuracy. These methods are data-free, so the resulting weight values remain tightly linked to the dataset distribution that optimizes the DNN model. Our quantization approaches are analyzed across a range of state-of-the-art DNNs that have been trained with a large dataset like ImageNet. We describe how different quantization schemes can afford local sparsity of values and a broad range-per-bit for compression. We also explore methods to decrease bit-precision beyond the quantization limits with object class clustering."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bitpruning%3A_Learning_bitlengths_for_aggressive_and_accurate_quantization/","title":"Bitpruning: Learning bitlengths for aggressive and accurate quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bitpruning%3A_Learning_bitlengths_for_aggressive_and_accurate_quantization/#summary","title":"Summary","text":"<p>The paper proposes a training method for neural network quantization that minimizes the inference bit-length while maintaining accuracy. The method introduces a regularizer that penalizes large bit-length representations throughout the architecture. The average per layer bit-length on AlexNet, ResNet18, and MobileNet V2 is 4.13, 3.76, and 4.36 bits respectively, staying within 2.0%, 0.5%, and 0.5% of the base TOP-1 accuracy. The proposed method helps achieve state-of-the-art accuracy using low-bit-length integers quantization, resulting in energy and execution time benefits."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bitpruning%3A_Learning_bitlengths_for_aggressive_and_accurate_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bitpruning%3A_Learning_bitlengths_for_aggressive_and_accurate_quantization/#content","title":"Content","text":"<p>Neural networks have achieved state-of-the-art accuracy using low-bitlength integer quantization, yielding execution time and energy benefits. However, determining the minimum bitlength required for a desired accuracy remains an open question. This paper proposes a training method that minimizes inference bitlength while maintaining accuracy, by introducing a regularizer that penalizes large bitlength representations throughout the architecture. The method produces an average per layer bitlength of 4.13, 3.76 and 4.36 bits on AlexNet, ResNet18 and MobileNet V2 respectively, remaining within 2.0%, 0.5% and 0.5% of the base TOP-1 accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bits-net%3A_Bit-sparse_deep_neural_network_for_energy-efficient_rram-based_compute-in-memory/","title":"Bits-net: Bit-sparse deep neural network for energy-efficient rram-based compute-in-memory","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bits-net%3A_Bit-sparse_deep_neural_network_for_energy-efficient_rram-based_compute-in-memory/#summary","title":"Summary","text":"<p>The paper proposes a novel model compression scheme BitS-Net that enables efficient implementation using bit-level sparsity for in-memory computing macros. The authors demonstrate that BitS-Net can improve the energy efficiency by up to 5x for ResNet models on the ImageNet dataset when applied to compute-in-memory with resistive RAM (RRAM) for developing energy-efficient DNN accelerators."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bits-net%3A_Bit-sparse_deep_neural_network_for_energy-efficient_rram-based_compute-in-memory/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bits-net%3A_Bit-sparse_deep_neural_network_for_energy-efficient_rram-based_compute-in-memory/#content","title":"Content","text":"<p>We propose a novel model compression scheme that allows inference to be carried out using bit-level sparsity, which can be efficiently implemented using in-memory computing macros. In this paper, we introduce a method called BitS-Net to leverage the benefits of bit-sparsity when applied to compute-in-memory with resistive RAM (RRAM) to develop energy efficient DNN accelerators operating in the inference mode. We demonstrate that BitS-Net improves the energy efficiency by up to 5x for ResNet models on the ImageNet dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Blended_coarse_gradient_descent_for_full_quantization_of_deep_neural_networks/","title":"Blended coarse gradient descent for full quantization of deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Blended_coarse_gradient_descent_for_full_quantization_of_deep_neural_networks/#summary","title":"Summary","text":"<p> This paper proposes a training algorithm for quantized deep neural networks that involves piecewise constant activation functions and discrete weights, which involves mathematical challenges. The proposed blended coarse gradient descent (BCGD) algorithm involves coarse gradient correction of a weighted average of full precision weights and their quantization which yields sufficient descent in the objective value and accelerates training."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Blended_coarse_gradient_descent_for_full_quantization_of_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Blended_coarse_gradient_descent_for_full_quantization_of_deep_neural_networks/#content","title":"Content","text":"<p>Quantized deep neural networks (QDNNs) are attractive due to their much lower memory storage and faster inference speed than their regular full precision counterparts. To maintain the same performance level especially at low bit-widths, QDNNs must be retrained. Their training involves piecewise constant activation functions and discrete weights, hence mathematical challenges arise. We introduce the notion of coarse gradient and propose the blended coarse gradient descent (BCGD) algorithm, for training fully quantized neural networks. Coarse gradient is generally not a gradient of any function but an artificial ascent direction. The weight update of BCGD goes by coarse gradient correction of a weighted average of the full precision weights and their quantization (the so-called blending), which yields sufficient descent in the objective value and thus accelerates the training."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bmxnet%3A_An_open-source_binary_neural_network_implementation_based_on_mxnet/","title":"Bmxnet: An open-source binary neural network implementation based on mxnet","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bmxnet%3A_An_open-source_binary_neural_network_implementation_based_on_mxnet/#summary","title":"Summary","text":"<p> This paper introduces BMXNet, an open-source binary neural network library which supports both XNOR-Networks and Quantized Neural Networks. The goal is to improve efficiency and lower energy consumption on low power devices by applying bit-wise operations instead of standard arithmetic operations. BMXNet is developed by the multimedia research group at Hasso Plattner Institute and is released under Apache license. The library has been validated through extensive experiments and includes sample projects and pre-trained binary deep models available for download on GitHub."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bmxnet%3A_An_open-source_binary_neural_network_implementation_based_on_mxnet/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bmxnet%3A_An_open-source_binary_neural_network_implementation_based_on_mxnet/#content","title":"Content","text":"<p>Binary Neural Networks (BNNs) can drastically reduce memory size and accesses by applying bit-wise operations instead of standard arithmetic operations. Therefore it could significantly improve the efficiency and lower the energy consumption at runtime, which enables the application of state-of-the-art deep learning models on low power devices. BMXNet is an open-source BNN library based on MXNet, which supports both XNOR-Networks and Quantized Neural Networks. The developed BNN layers can be seamlessly applied with other standard library components and work in both GPU and CPU mode. BMXNet is maintained and developed by the multimedia research group at Hasso Plattner Institute and released under Apache license. Extensive experiments validate the efficiency and effectiveness of our implementation. The BMXNet library, several sample projects, and a collection of pre-trained binary deep models are available for download at https://github.com/hpi-xnor."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Brecq%3A_Pushing_the_limit_of_post-training_quantization_by_block_reconstruction/","title":"Brecq: Pushing the limit of post-training quantization by block reconstruction","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Brecq%3A_Pushing_the_limit_of_post-training_quantization_by_block_reconstruction/#summary","title":"Summary","text":"<p> The paper proposes a Post-training Quantization (PTQ) framework called BRECQ, which can quantize the neural network to INT2 for the first time, by reconstructing the building blocks one-by-one to balance cross-layer dependency and generalization error. The framework incorporates mixed precision technique by approximating inter-layer and intra-layer sensitivity. The experiments show that PTQ can achieve 4-bit ResNet and MobileNetV2 comparable to QAT, with 240x faster production of quantized models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Brecq%3A_Pushing_the_limit_of_post-training_quantization_by_block_reconstruction/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Brecq%3A_Pushing_the_limit_of_post-training_quantization_by_block_reconstruction/#content","title":"Content","text":"<p>We propose a novel Post-training Quantization (PTQ) framework, called BRECQ, which is capable of pushing the bitwidth in PTQ down to INT2 for the first time. BRECQ reconstructs the building blocks in neural networks one-by-one to achieve a good balance between cross-layer dependency and generalization error. We incorporate mixed precision technique in our framework by approximating the inter-layer and intra-layer sensitivity. Our experiments show that PTQ can attain 4-bit ResNet and MobileNetV2 comparable with QAT and enjoy 240x faster production of quantized models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bridging_the_accuracy_gap_for_2-bit_quantized_neural_networks_%28qnn%29/","title":"Bridging the accuracy gap for 2-bit quantized neural networks (qnn)","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bridging_the_accuracy_gap_for_2-bit_quantized_neural_networks_%28qnn%29/#summary","title":"Summary","text":"<p>Summary: The paper presents techniques for quantizing a neural network by separately quantizing weights and activations resulting in an overall quantized neural network (QNN). The activation quantization technique, PACT, optimizes an activation clipping parameter during training to find the right quantization scale. The weight quantization scheme, SAWB, finds the optimal scaling factor based on the statistical characteristics of the weight distribution without an exhaustive search. The combination of PACT and SAWB results in a 2-bit QNN that achieves state-of-the-art classification accuracy on popular models and datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bridging_the_accuracy_gap_for_2-bit_quantized_neural_networks_%28qnn%29/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Bridging_the_accuracy_gap_for_2-bit_quantized_neural_networks_%28qnn%29/#content","title":"Content","text":"<p>Deep learning algorithms are computationally expensive, but quantization schemes have been proposed to address this issue. This paper presents novel techniques for weight and activation quantizations separately resulting in an overall quantized neural network (QNN). The activation quantization technique, PArameterized Clipping acTivation (PACT), uses an activation clipping parameter optimized during training to find the right quantization scale. The weight quantization scheme, statistics-aware weight binning (SAWB), finds the optimal scaling factor that minimizes the quantization error based on the statistical characteristics of the weight distribution without the need for an exhaustive search. The combination of PACT and SAWB results in a 2-bit QNN that achieves state-of-the-art classification accuracy comparable to full precision networks across a range of popular models and datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/CLIP_Itself_is_a_Strong_Fine-tuner%3A_Achieving_85.7%25_and_88.0%25_Top-1_Accuracy_with_ViT-B_and_ViT-L_on_ImageNet/","title":"CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/CLIP_Itself_is_a_Strong_Fine-tuner%3A_Achieving_85.7%25_and_88.0%25_Top-1_Accuracy_with_ViT-B_and_ViT-L_on_ImageNet/#summary","title":"Summary","text":"<p> The paper demonstrates through a comprehensive study that fine-tuning performance of CLIP is impacted by hyper-parameter choices and improves fine-tuning CLIP for classification tasks by evaluating the impact of the hyper-parameters. Equipped with hyperparameter refinement, CLIP itself is better or competitive compared with large-scale supervised pre-training approaches or latest works that use CLIP as prediction targets in Masked Image Modeling. Additionally, the observations challenge the conventional conclusion that CLIP is not suitable for fine-tuning and motivate researchers to rethink recently proposed improvements based on CLIP."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/CLIP_Itself_is_a_Strong_Fine-tuner%3A_Achieving_85.7%25_and_88.0%25_Top-1_Accuracy_with_ViT-B_and_ViT-L_on_ImageNet/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/CLIP_Itself_is_a_Strong_Fine-tuner%3A_Achieving_85.7%25_and_88.0%25_Top-1_Accuracy_with_ViT-B_and_ViT-L_on_ImageNet/#content","title":"Content","text":"<p>Recent studies have shown that CLIP has achieved remarkable success in performing zero-shot inference while its fine-tuning performance is not satisfactory. In this paper, we identify that fine-tuning performance is significantly impacted by hyper-parameter choices. We examine various key hyper-parameters and empirically evaluate their impact in fine-tuning CLIP for classification tasks through a comprehensive study. Equipped with hyper-parameter refinement, we demonstrate CLIP itself is better or at least competitive in fine-tuning compared with large-scale supervised pre-training approaches or latest works that use CLIP as prediction targets in Masked Image Modeling. CLIP ViT-Base/16 and CLIP ViT-Large/14 can achieve 85:7%;88:0% fine-tuning Top-1 accuracy on the ImageNet-1K dataset. These observations challenge the conventional conclusion that CLIP is not suitable for fine-tuning, and motivate us to rethink recently proposed improvements based on CLIP."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Central_similarity_quantization_for_efficient_image_and_video_retrieval/","title":"Central similarity quantization for efficient image and video retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Central_similarity_quantization_for_efficient_image_and_video_retrieval/#summary","title":"Summary","text":"<p>The paper introduces a new global similarity metric called central similarity, which aims to improve the learning efficiency and retrieval accuracy of data-dependent hashing methods. They introduce the concept of hash centers and provide a method of constructing such centers efficiently. The proposed Central Similarity Quantization (CSQ) optimizes the central similarity between data points w.r.t. their hash centers, leading to an improvement in large-scale image and video retrieval tasks' retrieval performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Central_similarity_quantization_for_efficient_image_and_video_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Central_similarity_quantization_for_efficient_image_and_video_retrieval/#content","title":"Content","text":"<p>In this paper, the authors propose a new global similarity metric, called central similarity, to improve the learning efficiency and retrieval accuracy of data-dependent hashing methods. They introduce the concept of hash centers and provide an efficient method to construct well-separated hash centers. They then propose the Central Similarity Quantization (CSQ) that optimizes the central similarity between data points w.r.t. their hash centers instead of optimizing the local similarity. The proposed method is shown to achieve a noticeable boost in retrieval performance in large-scale image and video retrieval tasks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Channel-level_variable_quantization_network_for_deep_image_compression/","title":"Channel-level variable quantization network for deep image compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Channel-level_variable_quantization_network_for_deep_image_compression/#summary","title":"Summary","text":"<p> This paper proposes a channel-level variable quantization network for deep image compression that dynamically allocates more bitrates for significant channels and withdraws bitrates for negligible channels. The proposed method results in superior performance and better visual reconstructions."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Channel-level_variable_quantization_network_for_deep_image_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Channel-level_variable_quantization_network_for_deep_image_compression/#content","title":"Content","text":"<p>Deep image compression systems mainly contain four components: encoder, quantizer, entropy model, and decoder. In this paper, we propose a channel-level variable quantization network to dynamically allocate more bitrates for significant channels and withdraw bitrates for negligible channels. Our method achieves superior performance and can produce much better visual reconstructions."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Channel-wise_hessian_aware_trace-weighted_quantization_of_neural_networks/","title":"Channel-wise hessian aware trace-weighted quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Channel-wise_hessian_aware_trace-weighted_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>The paper introduces Channel-wise Hessian Aware trace-Weighted Quantization (CW-HAWQ), which uses Hessian traces to determine the relative sensitivity order of different channels of activations and weights. CW-HAWQ proposes using deep Reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG) based agent to find the optimal ratios of different quantization bits and assign bits to channels according to the Hessian trace order. CW-HAWQ shows better results compared to state-of-the-art methods for multiple networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Channel-wise_hessian_aware_trace-weighted_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Channel-wise_hessian_aware_trace-weighted_quantization_of_neural_networks/#content","title":"Content","text":"<p> Second-order information has proven to be very effective in determining the redundancy of neural network weights and activations. Recent paper proposes to use Hessian traces of weights and activations for mixed-precision quantization and achieves state-of-the-art results. Here, we introduce Channel-wise Hessian Aware trace-Weighted Quantization (CW-HAWQ). CW-HAWQ uses Hessian trace to determine the relative sensitivity order of different channels of activations and weights. What\u2019s more, CW-HAWQ proposes to use deep Reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agent to \ufb01nd the optimal ratios of different quantization bits and assign bits to channels according to the Hessian trace order. The number of states in CW-HAWQ is much smaller compared with traditional AutoML based mix-precision methods since we only need to search ratios for the quantization bits. Compare CW-HAWQ with state-of-the-art shows that we can achieve better results for multiple networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Characterizing_generalization_under_out-of-distribution_shifts_in_deep_metric_learning/","title":"Characterizing generalization under out-of-distribution shifts in deep metric learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Characterizing_generalization_under_out-of-distribution_shifts_in_deep_metric_learning/#summary","title":"Summary","text":"<p>Summary: The paper introduces the out-of-distribution deep metric learning (ooDML) benchmark to test zero-shot generalization performance for different train-test splits. The study finds that some deep metric learning methods can retain performance better as out-of-distribution shifts increase. The paper proposes few-shot deep metric learning to improve generalization for future unknown test shifts presented in ooDML.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Characterizing_generalization_under_out-of-distribution_shifts_in_deep_metric_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Characterizing_generalization_under_out-of-distribution_shifts_in_deep_metric_learning/#content","title":"Content","text":"<p>Deep Metric Learning (DML) aims to find representations suitable for zero-shot transfer to a priori unknown test distributions. However, common evaluation protocols only test a single, fixed data split in which train and test classes are assigned randomly. In this work, we systematically construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under out-of-distribution shifts in DML. Based on our new benchmark, we conduct a thorough empirical analysis of state-of-the-art DML methods. We find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, we propose few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in ooDML."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Clip-q%3A_Deep_network_compression_learning_by_in-parallel_pruning-quantization/","title":"Clip-q: Deep network compression learning by in-parallel pruning-quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Clip-q%3A_Deep_network_compression_learning_by_in-parallel_pruning-quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a combined approach of network pruning and weight quantization that perform pruning and quantization jointly using a single learning framework. The proposed CLIP-Q method compresses the deep neural networks, including AlexNet, GoogLeNet, and ResNet-50 while preserving the accuracy on ImageNet.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Clip-q%3A_Deep_network_compression_learning_by_in-parallel_pruning-quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Clip-q%3A_Deep_network_compression_learning_by_in-parallel_pruning-quantization/#content","title":"Content","text":"<p>Deep neural networks enable state-of-the-art accuracy on visual recognition tasks such as image classification and object detection. However, modern deep networks contain millions of learned weights; a more efficient utilization of computation resources would assist in a variety of deployment scenarios, from embedded platforms with resource constraints to computing clusters running ensembles of networks. In this paper, we combine network pruning and weight quantization in a single learning framework that performs pruning and quantization jointly, and in parallel with fine-tuning. This allows us to take advantage of the complementary nature of pruning and quantization and to recover from premature pruning errors, which is not possible with current two-stage approaches. Our proposed CLIP-Q method (Compression Learning by In-Parallel Pruning-Quantization) compresses AlexNet by 51-fold, GoogLeNet by 10-fold, and ResNet-50 by 15-fold, while preserving the uncompressed network accuracies on ImageNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Cluster-promoting_quantization_with_bit-drop_for_minimizing_network_quantization_loss/","title":"Cluster-promoting quantization with bit-drop for minimizing network quantization loss","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Cluster-promoting_quantization_with_bit-drop_for_minimizing_network_quantization_loss/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel quantization method, Cluster-Promoting Quantization (CPQ), to reduce the bit-length of neural network weights and activations for resource-limited devices. CPQ finds optimal quantization grids which encourage full-precision weights to gather around those quantization grids cohesively during training, reducing the quantization errors that typically result from network quantization.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Cluster-promoting_quantization_with_bit-drop_for_minimizing_network_quantization_loss/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Cluster-promoting_quantization_with_bit-drop_for_minimizing_network_quantization_loss/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ClusterQ%3A_Semantic_Feature_Distribution_Alignment_for_Data-Free_Quantization/","title":"ClusterQ: Semantic Feature Distribution Alignment for Data-Free Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ClusterQ%3A_Semantic_Feature_Distribution_Alignment_for_Data-Free_Quantization/#summary","title":"Summary","text":"<p> The paper proposes a new and effective data-free quantization method called ClusterQ, which employs feature distribution alignment for synthetic data generation to reduce performance degradation caused by low inter-class separability of semantic features. The proposed method achieves state-of-the-art performance on different deep models over the ImageNet dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ClusterQ%3A_Semantic_Feature_Distribution_Alignment_for_Data-Free_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ClusterQ%3A_Semantic_Feature_Distribution_Alignment_for_Data-Free_Quantization/#content","title":"Content","text":"<p>To obtain lower inference latency and less memory footprint of deep neural networks, model quantization has been widely employed in deep model deployment, by converting the floating points to low-precision integers. However, previous methods require original data for the fine-tuning or calibration of quantized model, which makes them inapplicable to the cases that original data are not accessed due to privacy or security. This gives birth to the data-free quantization method with synthetic data generation. While current data-free quantization methods still suffer from severe performance degradation when quantizing a model into lower bit, caused by the low inter-class separability of semantic features. To this end, we propose a new and effective data-free quantization method termed ClusterQ, which utilizes the feature distribution alignment for synthetic data generation. We also employ the exponential moving average to update the centroid of each cluster for further feature distribution improvement. Extensive experiments based on different deep models over the ImageNet dataset demonstrate that our proposed ClusterQ model obtains state-of-the-art performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/CoCo-FL%3A_Communication-and_Computation-Aware_Federated_Learning_via_Partial_NN_Freezing_and_Quantization/","title":"CoCo-FL: Communication-and Computation-Aware Federated Learning via Partial NN Freezing and Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/CoCo-FL%3A_Communication-and_Computation-Aware_Federated_Learning_via_Partial_NN_Freezing_and_Quantization/#summary","title":"Summary","text":"<p>The paper presents a Federated Learning technique called CoCoFL, that maintains the full NN structure on all devices and adapts to heterogeneous resources by freezing and quantizing selected layers, enhancing the accuracy of FL systems and providing fairness to constrained devices. CoCoFL outperforms existing techniques and can reach up to 8.9p.p. higher accuracy with independent and up to 20p.p. with non-independent, non-identical distributed data."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/CoCo-FL%3A_Communication-and_Computation-Aware_Federated_Learning_via_Partial_NN_Freezing_and_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/CoCo-FL%3A_Communication-and_Computation-Aware_Federated_Learning_via_Partial_NN_Freezing_and_Quantization/#content","title":"Content","text":"<p>Devices participating in federated learning (FL) usually have diverse resources, such as computation, communication, and memory, and in synchronous FL, all devices need to finish training by the same deadline set by the server. Existing techniques that train subsets of the neural network (NN) model on less capable devices, i.e., reducing neurons/filters, is inefficient and does not provide fairness to constrained devices, especially in cases when there is a skewed distribution of class labels across devices. In this paper, a novel Federated Learning technique, called CoCoFL, is proposed that maintains the full NN structure on all devices, adapting to heterogeneous resources by freezing and quantizing selected layers, allowing devices to reach high accuracy while utilizing available resources, increasing fairness and improving the final accuracy of the model. CoCoFL is evaluated, demonstrating that it outperforms existing techniques, reaching up to 8.9p.p. higher accuracy with independent and up to 20p.p. with non-independent, non-identical distributed data, enhancing the accuracy of FL systems."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Combined_scaling_for_zero-shot_transfer_learning/","title":"Combined scaling for zero-shot transfer learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Combined_scaling_for_zero-shot_transfer_learning/#summary","title":"Summary","text":"<p>Summary: The paper introduces BASIC, a combined scaling method for contrastive learning that achieves a top-1 accuracy of 85.7% on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet data. BASIC surpasses previously published similar models, CLIP and ALIGN, by 9.3%. The authors scale up the contrastive learning framework by increasing data size, model size, and batch size, and propose methods such as gradient checkpointing and model parallelism to overcome memory limitations. They also show that larger contrastive batch sizes lead to smaller generalization gaps for image-text models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Combined_scaling_for_zero-shot_transfer_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Combined_scaling_for_zero-shot_transfer_learning/#content","title":"Content","text":"<p> We present BASIC, a combined scaling method that achieves 85.7% top-1 accuracy on ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. BASIC surpasses best-published similar models - CLIP and ALIGN by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks like ImageNet-{A,R,V2,Sketch} and ObjectNet. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. We encountered two main challenges, the limited memory of accelerators, such as GPUs and TPUs, and the effect of a large contrastive batch size on such contrastive-trained image-text models is not well-understood. To overcome these challenges, we propose two simple methods which make use of gradient checkpointing and model parallelism, and a theoretical framework that shows larger contrastive batch sizes lead to smaller generalization gaps for image-text models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Composite_quantization/","title":"Composite quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Composite_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a composite quantization framework for approximating nearest neighbor search using a short code composed of indices of elements selected from different dictionaries. The approach called near-orthogonal composite quantization guarantees search efficiency through a near-orthogonality constraint, reducing the cost of distance computation. The proposed approach achieves state-of-the-art performance in approximating nearest neighbor search in terms of the Euclidean distance.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Composite_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Composite_quantization/#content","title":"Content","text":"<p> This paper introduces a composite quantization framework to convert vectors to compact codes, with the goal of approximating nearest neighbor search. The idea is to approximate a vector using the composition of M elements each selected from a different dictionary, and to represent this vector by a short code composed of the indices of the selected elements. The resulting approach is called near-orthogonal composite quantization. A near-orthogonality constraint is introduced to make search efficiency guaranteed, reducing the cost of distance computation from O(D) to O(M) through a distance table lookup scheme. The equivalence between near-orthogonal composite quantization and minimizing an upper bound of a function is theoretically justified. The proposed approach achieves state-of-the-art performances in approximate nearest neighbor search in terms of the Euclidean distance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Compressing_deep_convolutional_networks_using_vector_quantization/","title":"Compressing deep convolutional networks using vector quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Compressing_deep_convolutional_networks_using_vector_quantization/#summary","title":"Summary","text":"<p>The paper investigates information theoretical vector quantization methods for compressing the parameters of CNNs to overcome the storage issue for very deep CNNs with many layers and millions of parameters, making it difficult for usage in resource-limited hardware. The study found that vector quantization methods offer a clear gain over existing matrix factorization methods for compressing storage demanding dense connected layers while maintaining model size and recognition accuracy. The study achieved 16-24 times compression of the network for the 1000-category classification task in the ImageNet challenge with only a 1% loss of classification accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Compressing_deep_convolutional_networks_using_vector_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Compressing_deep_convolutional_networks_using_vector_quantization/#content","title":"Content","text":"<p>Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classi\ufb01cation and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classi\ufb01cation task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classi\ufb01cation accuracy using the state-of-the-art CNN."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Compressing_unknown_images_with_product_quantizer_for_efficient_zero-shot_classification/","title":"Compressing unknown images with product quantizer for efficient zero-shot classification","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Compressing_unknown_images_with_product_quantizer_for_efficient_zero-shot_classification/#summary","title":"Summary","text":"<p>Summary: This paper proposes a new method called Product Quantization Zero-Shot Learning (PQZSL) for compressing zero-shot classes to enable efficient Approximate NN (ANN) search. The method uses a combination of visual feature projection and individual property quantization using Product Quantization (PQ) to create a more discriminative classification for Generalized Zero-Shot Learning (GZSL). PQZSL achieves state-of-the-art performance and accelerates ANN search by 10-100 times faster than traditional NN search.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Compressing_unknown_images_with_product_quantizer_for_efficient_zero-shot_classification/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Compressing_unknown_images_with_product_quantizer_for_efficient_zero-shot_classification/#content","title":"Content","text":"<p> For Zero-Shot Learning (ZSL), a new method called Product Quantization Zero-Shot Learning (PQZSL) is proposed to compress zero-shot classes for efficient Approximate NN (ANN) search. Visual features are projected into an orthogonal semantic space, and then the Product Quantization (PQ) is utilized to quantize individual properties. As classes in orthogonal common space are more discriminative, the classification based on PQZSL achieves state-of-the-art performance in Generalized Zero-Shot Learning (GZSL) task. The speed of ANN search is 10-100 times higher than traditional NN search."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Convolutional_Neural_Networks_Quantization_with_Attention/","title":"Convolutional Neural Networks Quantization with Attention","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Convolutional_Neural_Networks_Quantization_with_Attention/#summary","title":"Summary","text":"<p>Summary: The paper discusses the benefits of quantizing deep convolutional neural networks (DCNNs) for embedded devices with limited memory and computing resources. The authors propose a double-stage Squeeze-and-Threshold (double-stage ST) activation quantization method that uses the attention mechanism to achieve improved accuracy. The method is easy to apply and achieves state-of-the-art results, surpassing the accuracy of the full-precision baseline model. The paper compares the proposed method with previous works and concludes that it is a promising approach for quantizing DCNNs. Keywords include quantization, attention, and convolutional neural networks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Convolutional_Neural_Networks_Quantization_with_Attention/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Convolutional_Neural_Networks_Quantization_with_Attention/#content","title":"Content","text":"<p>It has been proposed that deep convolutional neural networks (DCNNs) can operate with low precision during inference, thereby saving memory space and power consumption, compared to using 32-bit floating-point numbers in the training phase. However, quantizing networks is always accompanied by a decrease in accuracy. In this paper, the authors propose a method, called double-stage Squeeze-and-Threshold (double-stage ST), which uses the attention mechanism to quantize networks and achieve improved accuracy. Using this method, the 3-bit model can achieve accuracy that exceeds the accuracy of the full-precision baseline model. The proposed double-stage ST activation quantization is easy to apply by inserting it before the convolution. The paper discusses network quantization and the benefits it offers for embedded devices that run neural networks with limited memory and computing resources. The authors compare their method with previous works and explain the success of the attention mechanism on CNNs. The proposed method achieves state-of-art results and provides a promising approach for quantizing deep neural networks. The paper concludes with keywords such as quantization, attention, and convolutional neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Convolutional_neural_network_quantization_using_generalized_gamma_distribution/","title":"Convolutional neural network quantization using generalized gamma distribution","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Convolutional_neural_network_quantization_using_generalized_gamma_distribution/#summary","title":"Summary","text":"<p>This paper proposes a novel quantization algorithm for hardware accelerators in CNNs, which determines the optimal fractional bit length for weights and biases and quantization step size using asymptotical closed form solution of generalized gamma distribution (GGD). The algorithm outperforms other quantization schemes in terms of signal-to-quantization-noise ratio (SQNR)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Convolutional_neural_network_quantization_using_generalized_gamma_distribution/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Convolutional_neural_network_quantization_using_generalized_gamma_distribution/#content","title":"Content","text":"<p>In this paper, a novel quantization algorithm for energy-efficient deployment of hardware accelerators for convolutional neural networks (CNN) is proposed. The proposed algorithm determines the optimal bit length of the fractional part for weights and biases to minimize the quantization error over their distribution. For feature-map data, the sample distribution is well approximated with the generalized gamma distribution (GGD), and the optimal quantization step size can be obtained through the asymptotical closed form solution of GGD. The proposed algorithm outperforms other quantization schemes previously proposed for CNNs in terms of signal-to-quantization-noise ratio (SQNR)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DBQ%3A_A_differentiable_branch_quantizer_for_lightweight_deep_neural_networks/","title":"DBQ: A differentiable branch quantizer for lightweight deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DBQ%3A_A_differentiable_branch_quantizer_for_lightweight_deep_neural_networks/#summary","title":"Summary","text":"<p>The paper proposes a new fully differentiable non-uniform quantizer (DBQ) which can efficiently map onto ternary-based dot product engines to reduce the computational and storage complexity of deep neural networks. This quantizer successfully quantizes lightweight networks such as MobileNetV1, MobileNetV2, and ShuffleNetV2 with minimal training overhead and achieves state-of-the-art results with the best accuracy-complexity trade-off on CIFAR-10, ImageNet, and Visual Wake Words datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DBQ%3A_A_differentiable_branch_quantizer_for_lightweight_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DBQ%3A_A_differentiable_branch_quantizer_for_lightweight_deep_neural_networks/#content","title":"Content","text":"<p>Deep neural networks have high computational and storage complexity, which limits their deployment on resource-constrained devices. To reduce the cost of implementing these networks, various complexity reduction techniques have been used including lightweight architecture design and parameter quantization. However, existing quantization techniques fail to replicate their success on lightweight architectures such as MobileNet. In this paper, a novel fully differentiable non-uniform quantizer (DBQ) is proposed that can be efficiently mapped onto ternary-based dot product engines. Comprehensive experiments on CIFAR-10, ImageNet, and Visual Wake Words datasets show that the proposed quantizer successfully quantizes lightweight networks such as MobileNetV1, MobileNetV2, and ShuffleNetV2 with minimal training overhead. DBQ achieves state-of-the-art results with the best accuracy-complexity trade-off."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DNQ%3A_Dynamic_network_quantization/","title":"DNQ: Dynamic network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DNQ%3A_Dynamic_network_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Dynamic Network Quantization (DNQ) framework for network quantization, which includes a bit-width controller and a quantizer. This approach uses policy gradient to train the bit-width controller to learn the bit-width of each layer of the network, allowing for a trade-off between accuracy and compression ratio. The quantizer uses quantization distance as the criterion for weights importance during quantization. Results demonstrate the effectiveness of the proposed approach.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DNQ%3A_Dynamic_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DNQ%3A_Dynamic_network_quantization/#content","title":"Content","text":"<p> Network quantization is an effective method for the deployment of neural networks on memory and energy constrained mobile devices. In this paper, we propose a Dynamic Network Quantization (DNQ) framework which is composed of two modules: a bit-width controller and a quantizer. Unlike most existing quantization methods that use a universal quantization bit-width for the whole network, we utilize policy gradient to train an agent to learn the bit-width of each layer by the bit-width controller. This controller can make a trade-off between accuracy and compression ratio. Given the quantization bit-width sequence, the quantizer adopts the quantization distance as the criterion of the weights importance during quantization. We extensively validate the proposed approach on various main-stream neural networks and obtain impressive results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DPOQ%3A_dynamic_precision_onion_quantization/","title":"DPOQ: dynamic precision onion quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DPOQ%3A_dynamic_precision_onion_quantization/#summary","title":"Summary","text":"<p> The paper proposes a new strategy called Dynamic Precision Onion Quantization (DPOQ) that enables dynamic precision in neural network parameters. The authors train the network using scaled gradients loss and propose Precision Shift Batch Normalization (PSBN) to improve performance and make different precision networks compatible. They also propose an adaptable input-specific inference mechanism based on this architecture. Experiments on CIFAR and ImageNet dataset show DPOQ achieves higher accuracy and better flexibility than individual quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DPOQ%3A_dynamic_precision_onion_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DPOQ%3A_dynamic_precision_onion_quantization/#content","title":"Content","text":"<p>In this paper, we propose a novel network architecture reuse strategy enabling dynamic precision in parameters, named dynamic precision onion quantization (DPOQ). We train the network using the joint loss with scaled gradients and propose the precision shift batch normalization (PSBN) to further improve performance and make different precision networks compatible with each other. We also propose a scalable input-specific inference mechanism based on this architecture to make the network more adaptable. Experiments on the CIFAR and ImageNet dataset have shown that our DPOQ achieves higher accuracy and better flexibility than individual quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DQ-SGD%3A_Dynamic_quantization_in_SGD_for_communication-efficient_distributed_learning/","title":"DQ-SGD: Dynamic quantization in SGD for communication-efficient distributed learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DQ-SGD%3A_Dynamic_quantization_in_SGD_for_communication-efficient_distributed_learning/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel dynamically quantized SGD (DQ-SGD) framework that addresses the lack of a systematic approach to dynamically quantize gradients. It adapts the quantization bits by taking into account the desired model performance, the remaining number of iterations, and the norm of gradients, and minimizes communication costs under convergence error constraints, while exploring the trade-off between communication cost and convergence error. Extensive experiments on large-scale natural language processing and computer vision tasks validate the theoretical analysis, showing that the proposed scheme outperforms the baseline quantization methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DQ-SGD%3A_Dynamic_quantization_in_SGD_for_communication-efficient_distributed_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/DQ-SGD%3A_Dynamic_quantization_in_SGD_for_communication-efficient_distributed_learning/#content","title":"Content","text":"<p> Gradient quantization is an emerging technique to reduce communication costs in distributed learning. The main challenge with existing gradient quantization algorithms is the lack of a systematic approach to dynamically quantize gradients. In this paper, we propose a novel dynamically quantized SGD (DQ-SGD) framework that allows us to adjust the quantization scheme dynamically for each gradient descent step. We minimize the communication cost under the convergence error constraints while exploring the trade-off between communication cost and convergence error. We derive an upper bound, tight in some cases, for the convergence error for a restricted family of quantization schemes and loss functions. Our DQ-SGD algorithm adapts the quantization bits by taking into account the desired model performance, the remaining number of iterations, and the norm of gradients. We validate our theoretical analysis through extensive experiments on large-scale natural language processing and computer vision tasks, and our proposed scheme outperforms the baseline quantization methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Daq%3A_Channel-wise_distribution-aware_quantization_for_deep_image_super-resolution_networks/","title":"Daq: Channel-wise distribution-aware quantization for deep image super-resolution networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Daq%3A_Channel-wise_distribution-aware_quantization_for_deep_image_super-resolution_networks/#summary","title":"Summary","text":"<p> This paper presents a new ultra-low precision distribution-aware quantization approach called Distribution-Aware Quantization (DAQ) that is designed for image super-resolution (SR). The proposed quantization method reduces the computational and resource costs without significantly sacrificing SR performance compared to other quantization methods. The authors observed that in recent SR networks, each channel has different distribution characteristics and thus proposed a channel-wise distribution-aware quantization scheme. Results from experiments were provided to demonstrate the effectiveness of the approach."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Daq%3A_Channel-wise_distribution-aware_quantization_for_deep_image_super-resolution_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Daq%3A_Channel-wise_distribution-aware_quantization_for_deep_image_super-resolution_networks/#content","title":"Content","text":"<p> Since the resurgence of deep neural networks (DNNs), image super-resolution (SR) has recently seen a huge progress in improving the quality of low resolution images, however at the great cost of computations and resources. Recently, there has been several efforts to make DNNs more efficient via quantization. However, SR demands pixel-level accuracy in the system, it is more difficult to perform quantization without significantly sacrificing SR performance. To this end, we introduce a new ultra-low precision yet effective quantization approach specifically designed for SR. In particular, we observe that in recent SR networks, each channel has different distribution characteristics. Thus we propose a channel-wise distribution-aware quantization scheme. Experimental results demonstrate that our proposed quantization, dubbed Distribution-Aware Quantization (DAQ), manages to greatly reduce the computational and resource costs without significant sacrifice in SR performance, compared to other quantization methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data-Free_Quantization_with_Accurate_Activation_Clipping_and_Adaptive_Batch_Normalization/","title":"Data-Free Quantization with Accurate Activation Clipping and Adaptive Batch Normalization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data-Free_Quantization_with_Accurate_Activation_Clipping_and_Adaptive_Batch_Normalization/#summary","title":"Summary","text":"<p>Summary: The authors propose a data-free quantization method that uses accurate activation clipping and adaptive batch normalization to compress neural networks to low bit-width without accessing the original training data. The proposed method achieves high performance, including 64.33% top-1 accuracy of 4-bit ResNet18 on the ImageNet dataset and 3.7% absolute improvement over existing state-of-the-art methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data-Free_Quantization_with_Accurate_Activation_Clipping_and_Adaptive_Batch_Normalization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data-Free_Quantization_with_Accurate_Activation_Clipping_and_Adaptive_Batch_Normalization/#content","title":"Content","text":"<p>In this paper, the authors propose a data-free quantization method that compresses neural networks to low bit-width without accessing the original training data. Existing data-free quantization methods cause severe performance degradation due to inaccurate activation clipping range and quantization error, particularly for low bit-width. The proposed method uses accurate activation clipping (AAC) and adaptive batch normalization (ABN) to improve model accuracy. AAC exploits accurate activation information from the full-precision model, while ABN updates the batch normalization layer adaptively to address the quantization error from distribution changes. Experimental results show that the proposed method achieves surprisingly high performance, with 64.33% top-1 accuracy of 4-bit ResNet18 on the ImageNet dataset and 3.7% absolute improvement over existing state-of-the-art methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data-free_quantization_through_weight_equalization_and_bias_correction/","title":"Data-free quantization through weight equalization and bias correction","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data-free_quantization_through_weight_equalization_and_bias_correction/#summary","title":"Summary","text":"<p>Summary: The paper introduces a data-free quantization technique for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves high performance on common computer vision architectures and tasks through weight equalization and bias correction. State-of-the-art quantized model performance is achieved on architectures like MobileNet, and the method extends to other computer vision architectures and tasks like semantic segmentation and object detection.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data-free_quantization_through_weight_equalization_and_bias_correction/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data-free_quantization_through_weight_equalization_and_bias_correction/#content","title":"Content","text":"<p> We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition, the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance and can be applied to many common computer vision architectures with a straightforward API call. We achieve state-of-the-art quantized model performance for common architectures, such as the MobileNet family, and show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data_efficient_language-supervised_zero-shot_recognition_with_optimal_transport_distillation/","title":"Data efficient language-supervised zero-shot recognition with optimal transport distillation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data_efficient_language-supervised_zero-shot_recognition_with_optimal_transport_distillation/#summary","title":"Summary","text":"<p> The paper proposes a method called OTTER that uses online entropic optimal transport to find soft image-text match as labels for contrastive learning, achieving strong zero-shot recognition performance compared to InfoNCE loss, label smoothing and knowledge distillation on Google Open Images and multi-labeled ImageNet. Source code for OTTER is available on Github."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data_efficient_language-supervised_zero-shot_recognition_with_optimal_transport_distillation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Data_efficient_language-supervised_zero-shot_recognition_with_optimal_transport_distillation/#content","title":"Content","text":"<p> We propose OTTER (Optimal Transport Distillation for Efficient Zero-Shot Recognition), which uses online entropic optimal transport to find a soft image-text match as labels for contrastive learning. Based on pretrained image and text encoders, models trained with OTTER achieve strong performance with only 3M image text pairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation, OTTER consistently outperforms these baselines in zero-shot evaluation on Google Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032 classes) from Tencent ML-Images. Our source code is open sourced at https://github.com/facebookresearch/OTTER."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_Image_Compression_with_Latent_Optimization_and_Piece-wise_Quantization_Approximation/","title":"Deep Image Compression with Latent Optimization and Piece-wise Quantization Approximation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_Image_Compression_with_Latent_Optimization_and_Piece-wise_Quantization_Approximation/#summary","title":"Summary","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_Image_Compression_with_Latent_Optimization_and_Piece-wise_Quantization_Approximation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_Image_Compression_with_Latent_Optimization_and_Piece-wise_Quantization_Approximation/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_asymmetric_hashing_with_dual_semantic_regression_and_class_structure_quantization/","title":"Deep asymmetric hashing with dual semantic regression and class structure quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_asymmetric_hashing_with_dual_semantic_regression_and_class_structure_quantization/#summary","title":"Summary","text":"<p> This paper proposes a dual semantic asymmetric hashing (DSAH) method for generating discriminative hash codes that utilizes class prior to conduct class structure quantization, a label mechanism for characterizing inter-class separability, and a pairwise similarity preserving loss for minimizing the distances between class-related network outputs based on an affinity graph. The proposed method has demonstrated superior performance in comparison with state-of-the-art deep hashing methods in various data sets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_asymmetric_hashing_with_dual_semantic_regression_and_class_structure_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_asymmetric_hashing_with_dual_semantic_regression_and_class_structure_quantization/#content","title":"Content","text":"<p>Abstract: Recently, deep hashing methods have been widely used in image retrieval task. Most existing deep hashing approaches adopt one-to-one quantization to reduce information loss. However, such class-unrelated quantization cannot give discriminative feedback for network training. In addition, these methods only utilize single label to integrate supervision information of data for hashing function learning, which may result in inferior network generalization performance and relatively low-quality hash codes since the inter-class information of data is totally ignored. In this paper, we propose a dual semantic asymmetric hashing (DSAH) method, which generates discriminative hash codes under three-fold constraints. Firstly, DSAH utilizes class prior to conduct class structure quantization so as to transmit class information during the quantization process. Secondly, a simple yet effective label mechanism is designed to characterize both the intra-class compactness and inter-class separability of data, thereby achieving semantic-sensitive binary code learning. Finally, a meaningful pairwise similarity preserving loss is devised to minimize the distances between class-related network outputs based on an affinity graph. With these three main components, high-quality hash codes can be generated through network. Extensive experiments conducted on various data sets demonstrate the superiority of DSAH in comparison with state-of-the-art deep hashing methods. Keywords: Asymmetric hashing, dual semantic regression, class structure quantization.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_asymmetric_metric_learning_via_rich_relationship_mining/","title":"Deep asymmetric metric learning via rich relationship mining","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_asymmetric_metric_learning_via_rich_relationship_mining/#summary","title":"Summary","text":"<p>The paper proposes a new framework called Deep Asymmetric Metric Learning via Rich Relationship Mining (DAMLRRM) to learn effective distance metrics between data with a satisfying sampling size to avoid overfitting through the use of rich relationships mining. The asymmetric structure of DAMLRRM enables the interlacing of two differently structured, unequal-length data streams, facilitating informative comparisons between new data pairs over iterations. To improve generalization ability, the paper suggests relaxing constraints on intra-class relationships by building a minimum-cost spanning tree to ensure a connected region. The proposed framework is shown to effectively improve the performance of deep metric learning approaches on benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_asymmetric_metric_learning_via_rich_relationship_mining/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_asymmetric_metric_learning_via_rich_relationship_mining/#content","title":"Content","text":"<p>Learning effective distance metric between data has gained increasing popularity, for its promising performance on various tasks, such as face verification, zero-shot learning, and image retrieval. A major line of researches employs hard data mining, which makes efforts on searching a subset of significant data. However, the hard data mining based approaches only rely on a small percentage of data, which is apt to overfitting. This motivates us to propose a novel framework, named deep asymmetric metric learning via rich relationship mining (DAMLRRM), to mine rich relationship under satisfying sampling size. DAMLRRM constructs two asymmetric data streams that are differently structured and of unequal length. The asymmetric structure enables the two data streams to interlace each other, which allows for the informative comparison between new data pairs over iterations. To improve the generalization ability, we further relax the constraint on the intra-class relationship. Rather than greedily connecting all possible positive pairs, DAMLRRM builds a minimum-cost spanning tree within each category to ensure the formation of a connected region. As such there exists at least one direct or indirect path between arbitrary positive pairs to bridge intra-class relevance. Extensive experimental results on three benchmark datasets including CUB-200-2011, Cars196, and Stanford Online Products show that DAMLRRM effectively boosts the performance of existing deep metric learning approaches."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_compression%3A_Compressing_deep_neural_networks_with_pruning%2C_trained_quantization_and_huffman_coding/","title":"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_compression%3A_Compressing_deep_neural_networks_with_pruning%2C_trained_quantization_and_huffman_coding/#summary","title":"Summary","text":"<p> <p>This paper proposes a three-stage pipeline called \"deep compression\" to reduce the storage requirement of neural networks between 35% and 49% without affecting their accuracy. The method involves pruning the network by learning only important connections, quantizing the weights by enforcing weight sharing, and applying Huffman coding. The retrained network fine-tuned the remaining connections and quantized centroids without a loss of accuracy. The method was applied to the AlexNet and VGG-16 models, reducing their storage requirement effectively, enabling the model to fit into on-chip SRAM caches.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_compression%3A_Compressing_deep_neural_networks_with_pruning%2C_trained_quantization_and_huffman_coding/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_compression%3A_Compressing_deep_neural_networks_with_pruning%2C_trained_quantization_and_huffman_coding/#content","title":"Content","text":"<p>Neural networks are computationally and memory intensive, making them difficult to deploy on hardware systems with limited resources. To address this issue, we introduce a three-stage pipeline called \"deep compression\" that can reduce the storage requirement of neural networks by between 35% and 49% without affecting their accuracy. Our method involves pruning the network by learning only important connections, quantizing the weights to enforce weight sharing, and applying Huffman coding. After pruning and quantizing, we retrain the network to fine-tune the remaining connections and quantized centroids. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35%, from 240MB to 6.9MB, and reduced the size of VGG-16 by 49%, from 552MB to 11.3MB, with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_image_compression_with_iterative_non-uniform_quantization/","title":"Deep image compression with iterative non-uniform quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_image_compression_with_iterative_non-uniform_quantization/#summary","title":"Summary","text":"<p> The paper proposes a novel iterative non-uniform quantization scheme for deep image compression that optimizes the non-uniform quantizer and the encoder-decoder alternatively, resulting in improved PSNR index and more flexibility in compressing complex image structures than existing deep compressors and JPEG2000. However, the non-differentiable nature of quantizers limits their performance when using deep convolutional neural networks (CNNs) in image compression."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_image_compression_with_iterative_non-uniform_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_image_compression_with_iterative_non-uniform_quantization/#content","title":"Content","text":"<p>In this paper, we propose an iterative non-uniform quantization scheme for deep image compression. The scheme alternately optimizes the non-uniform quantizer and the encoder-decoder of the compression system. The non-uniform quantizer is optimized based on the features\u2019 distribution while the encoder-decoder is updated by fixing the quantizer. This method improves the PSNR index and allows for more flexibility in compressing complex image structures than existing deep compressors and JPEG2000. Although deep convolutional neural networks (CNNs) have led to breakthroughs in image compression, the non-differentiable nature of quantizers limits their performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-assisted_terahertz_QPSK_detection_relying_on_single-bit_quantization/","title":"Deep learning-assisted terahertz QPSK detection relying on single-bit quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-assisted_terahertz_QPSK_detection_relying_on_single-bit_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a deep learning-assisted THz receiver using single-bit quantization which compensates for imperfections in THz devices such as in-phase/quadrature-phase imbalance, phase noise and nonlinearity. The authors derive the deflection ratio of the maximum-likelihood detector and propose a twin-phase training strategy and a neural network-based demodulator to compensate for phase offset before sampling. Simulation results show that the proposed receiver achieves satisfactory bit error rate performance.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-assisted_terahertz_QPSK_detection_relying_on_single-bit_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-assisted_terahertz_QPSK_detection_relying_on_single-bit_quantization/#content","title":"Content","text":"<p> In this paper, the authors propose a deep learning-assisted THz receiver that relies on single-bit quantization. The ultra-wide bandwidth of THz communications requires high-speed, high-resolution analog-to-digital converters, which are hard to implement due to their high complexity and power consumption. The authors investigate the imperfections of THz devices, including their in-phase/quadrature-phase imbalance, phase noise and nonlinearity. They derive the deflection ratio of the maximum-likelihood detector used by their single-bit-quantization THz receiver, which reveals the effect of phase offset on the demodulation performance, guiding the architecture design of the proposed receiver. A twin-phase training strategy and a neural network-based demodulator are proposed, where the phase offset of the received signal is compensated before sampling. The simulation results demonstrate that the proposed deep learning-assisted receiver is capable of achieving a satisfactory bit error rate performance, despite the grave distortions encountered."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-based_image_compression_with_trellis_coded_quantization/","title":"Deep learning-based image compression with trellis coded quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-based_image_compression_with_trellis_coded_quantization/#summary","title":"Summary","text":"<p>The paper proposes the use of trellis coded quantizer in a deep learning based image compression framework to achieve superior performance at low bit rates. The authors present a simple image compression model consisting of three subnetworks and optimize all components in an end-to-end manner. They experiment on two high resolution image datasets and compare TCQ and scalar quantizer based on their proposed baseline model, demonstrating the advantage of TCQ."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-based_image_compression_with_trellis_coded_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-based_image_compression_with_trellis_coded_quantization/#content","title":"Content","text":"<p>In this paper, the authors propose to incorporate trellis coded quantizer (TCQ) into a deep learning based image compression framework. They develop a simple image compression model that consists of three subnetworks (encoder, decoder and entropy estimation), and optimize all of the components in an end-to-end manner. They experiment on two high resolution image datasets and both show that their model can achieve superior performance at low bit rates. They also show the comparisons between TCQ and scalar quantizer (SQ) based on their proposed baseline model and demonstrate the advantage of TCQ."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-based_video_coding%3A_A_review_and_a_case_study/","title":"Deep learning-based video coding: A review and a case study","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-based_video_coding%3A_A_review_and_a_case_study/#summary","title":"Summary","text":"<p> This paper reviews the recent development of using deep learning for image and video coding, and introduces various techniques for intra/inter-picture prediction, filtering, and encoding optimizations. The authors also present their prototype video codec called Deep Learning Video Coding (DLVC), which achieves higher compression efficiency than HEVC for video coding. The source code for DLVC is available for future research."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-based_video_coding%3A_A_review_and_a_case_study/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning-based_video_coding%3A_A_review_and_a_case_study/#content","title":"Content","text":"<p> The paper reviews the representative works about using deep learning for image/video coding, which has been an actively developing research area since the year of 2015. There have been several proposed techniques using deep learning to perform intra-picture prediction, inter-picture prediction, cross-channel prediction, probability distribution prediction, transform, post- or in-loop filtering, down- and up-sampling, as well as encoding optimizations. According to the newest reports, deep schemes have achieved comparable or even higher compression efficiency than the state-of-the-art traditional schemes for image coding, deep tools have demonstrated the compression capability beyond HEVC for video coding. In the hope of advocating the research of deep learning-based video coding, the authors present a case study of their developed prototype video codec, namely Deep Learning Video Coding (DLVC). DLVC is able to achieve on average 39.6% and 33.0% bits saving than HEVC, under random-access and low-delay configurations, respectively. The source code of DLVC has been released for future researches."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning_optimization_for_edge_devices%3A_Analysis_of_training_quantization_parameters/","title":"Deep learning optimization for edge devices: Analysis of training quantization parameters","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning_optimization_for_edge_devices%3A_Analysis_of_training_quantization_parameters/#summary","title":"Summary","text":"<p>Summary: This paper discusses the problem of convolution neural network quantization, which involves converting floating-point into integer-point numbers, and its importance in optimizing network inference while preserving its robustness. It also emphasizes the need for comparing different quantization configurations to understand the selection of hyperparameters during training. The paper performs an in-depth analysis of parameters in quantization aware training to evaluate their impact on the accuracy of the deep neural network aimed at performing efficient calculations on resource-constrained devices.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning_optimization_for_edge_devices%3A_Analysis_of_training_quantization_parameters/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_learning_optimization_for_edge_devices%3A_Analysis_of_training_quantization_parameters/#content","title":"Content","text":"<p>This paper focuses on convolution neural network quantization problem. The quantization has a distinct stage of data conversion from \ufb02oating-point into integer-point numbers. In general, the process of quantization is associated with the reduction of the matrix dimension via limited precision of the numbers. However, the training and inference stages of deep learning neural network are limited by the space of the memory and a variety of factors including programming complexity and even reliability of the system. On the whole the process of quantization becomes more and more popular due to signi\ufb01cant impact on performance and minimal accuracy loss. Various techniques for networks quantization have been already proposed, including quantization aware training and integer arithmetic-only inference. Yet, a detailed comparison of various quantization con\ufb01gurations, combining all proposed methods haven\u2019t been presented yet. This comparison is important to understand selection of quantization hyperparameters during training to optimize networks for inference while preserving their robustness. In this work, we perform in-depth analysis of parameters in the quantization aware training, the process of simulating precision loss in the forward pass by quantizing and dequantizing tensors. Speci\ufb01cally, we modify rounding modes, input preprocessing, output data signedness, bitwidth of the quantization and locations of precision loss simulation to evaluate how they affect accuracy of deep neural network aimed at performing ef\ufb01cient calculations on resource-constrained devices. Keywords\u2014Deep neural network, quantization, quantization aware training, data analysis, arti\ufb01cial intelligence, edge devices"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_metric_learning_via_lifted_structured_feature_embedding/","title":"Deep metric learning via lifted structured feature embedding","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_metric_learning_via_lifted_structured_feature_embedding/#summary","title":"Summary","text":"<p>Summary: The paper presents an algorithm for learning the metric distance between pairs of examples in neural network training. It describes a method for lifting the vector of pairwise distances within a training batch to a matrix of pairwise distances, enabling the algorithm to optimize a novel structured prediction objective. The authors also collected a dataset of 120k images of online products for metric learning and experimented with the GoogLeNet network. Results show significant improvement over existing deep feature embedding methods on all tested embedding sizes.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_metric_learning_via_lifted_structured_feature_embedding/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_metric_learning_via_lifted_structured_feature_embedding/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_compression_by_in-parallel_pruning-quantization/","title":"Deep neural network compression by in-parallel pruning-quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_compression_by_in-parallel_pruning-quantization/#summary","title":"Summary","text":"<p>This paper proposes a deep network compression algorithm called CLIP-Q(Compression Learning by In-Parallel Pruning-Quantization) which jointly performs weight pruning and quantization. The approach is capable of recovering from premature pruning errors, which is an advantage over two-stage approaches. CLIP-Q achieves state-of-the-art performance in network compression on various architectures including AlexNet, VGGNet, GoogLeNet and ResNet, according to experiments on ImageNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_compression_by_in-parallel_pruning-quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_compression_by_in-parallel_pruning-quantization/#content","title":"Content","text":"<p>Deep neural networks enable state-of-the-art accuracy on visual recognition tasks such as image classification and object detection. However, modern networks contain millions of learned connections, and the current trend is towards deeper and more densely connected architectures. In this paper, we propose a deep network compression algorithm that performs weight pruning and quantization jointly, and in parallel with fine-tuning. Our approach takes advantage of the complementary nature of pruning and quantization and recovers from premature pruning errors, which is not possible with two-stage approaches. In experiments on ImageNet, CLIP-Q (Compression Learning by In-Parallel Pruning-Quantization) improves the state-of-the-art in network compression on AlexNet, VGGNet, GoogLeNet, and ResNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_compression_with_single_and_multiple_level_quantization/","title":"Deep neural network compression with single and multiple level quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_compression_with_single_and_multiple_level_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes two new network quantization methods, SLQ and MLQ, for high-bit and extremely low-bit quantization. These approaches consider network quantization from both the width and depth level and demonstrated impressive results through experiments using state-of-the-art neural networks including AlexNet, VGG-16, GoogleNet, and ResNet-18. Network compression is crucial and is an effective solution for decreasing storage and computation costs for DNN models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_compression_with_single_and_multiple_level_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_compression_with_single_and_multiple_level_quantization/#content","title":"Content","text":"<p> In this paper, the authors propose two novel network quantization approaches, single-level network quantization (SLQ) for high-bit quantization and multi-level network quantization (MLQ) for extremely low-bit quantization (ternary). They are the first to consider the network quantization from both width and depth level. The proposed approaches are validated with extensive experiments based on the state-of-the-art neural networks including AlexNet, VGG-16, GoogleNet and ResNet-18. Both SLQ and MLQ achieve impressive results. Network compression is critical and has become an effective solution to reduce the storage and computation costs for DNN models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_quantization_via_layer-wise_optimization_using_limited_training_data/","title":"Deep neural network quantization via layer-wise optimization using limited training data","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_quantization_via_layer-wise_optimization_using_limited_training_data/#summary","title":"Summary","text":"<p>Summary: The paper presents a layer-wise quantization method for deep neural networks, using a discrete optimization problem and ADMM to provide an efficient closed-form solution. The method is effective in preserving the performance of the original network post-quantization, and is tested with 1% of CIFAR10 and ImageNet datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_quantization_via_layer-wise_optimization_using_limited_training_data/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_neural_network_quantization_via_layer-wise_optimization_using_limited_training_data/#content","title":"Content","text":"<p> The paper proposes a layer-wise quantization method for deep neural networks that only requires limited training data. The method formulates parameters quantization for each layer as a discrete optimization problem and solves it using Alternative Direction Method of Multipliers (ADMM), which gives an efficient closed-form solution. The proposed method is able to preserve the performance of the original network after quantization. Extensive experiments on benchmark deep models are conducted to demonstrate the effectiveness of our proposed method using 1% of CIFAR10 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_quantization%3A_Encoding_convolutional_activations_with_deep_generative_model/","title":"Deep quantization: Encoding convolutional activations with deep generative model","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_quantization%3A_Encoding_convolutional_activations_with_deep_generative_model/#summary","title":"Summary","text":"<p>Summary: This paper proposes Fisher Vector encoding with Variational Auto-Encoder (FV-VAE), a deep architecture that quantizes local activations of convolutional layers using a deep generative model, trained in an end-to-end manner. The proposed FV-VAE is more flexible for representing the natural properties of data and yields improved generalization. The authors conduct experiments on three public datasets and report superior results compared to state-of-the-art representations, achieving the best published accuracy of 94.2% on UCF101.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_quantization%3A_Encoding_convolutional_activations_with_deep_generative_model/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_quantization%3A_Encoding_convolutional_activations_with_deep_generative_model/#content","title":"Content","text":"<p> Deep convolutional neural networks (CNNs) are highly effective for visual recognition, but learning a universal representation from convolutional layer activations remains a fundamental problem. In this paper, the authors propose Fisher Vector encoding with Variational Auto-Encoder (FV-VAE), a deep architecture that quantizes local activations of convolutional layers using a deep generative model, trained in an end-to-end manner. The authors introduce Variational Auto-Encoder to steer a variational inference and learning in a neural network that can be easily optimized using standard stochastic gradient methods. The proposed FV-VAE is more flexible for representing the natural properties of data and yields improved generalization. The authors conduct experiments on three public datasets and report superior results compared to state-of-the-art representations, achieving to-date the best published accuracy of 94.2% on UCF101."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_recurrent_quantization_for_generating_sequential_binary_codes/","title":"Deep recurrent quantization for generating sequential binary codes","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_recurrent_quantization_for_generating_sequential_binary_codes/#summary","title":"Summary","text":"<p> The paper proposes a Deep Recurrent Quantization (DRQ) architecture that generates sequential binary codes to address the issue with existing quantization methods, which require training multiple models for different code lengths. The DRQ model achieves comparable or better performance than state-of-the-art models for image retrieval and requires significantly less number of parameters and training times."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_recurrent_quantization_for_generating_sequential_binary_codes/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_recurrent_quantization_for_generating_sequential_binary_codes/#content","title":"Content","text":"<p>Quantization has been an effective technology in ANN search, but existing methods require training several models to generate different code lengths, which incurs a considerable training time cost and reduces flexibility. To address this issue, the authors propose a Deep Recurrent Quantization ( DRQ ) architecture which can generate sequential binary codes. Experimental results show that their model achieves comparable or even better performance compared with the state-of-the-art for image retrieval and requires significantly less number of parameters and training times."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_semisupervised_zero-shot_learning_with_maximum_mean_discrepancy/","title":"Deep semisupervised zero-shot learning with maximum mean discrepancy","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_semisupervised_zero-shot_learning_with_maximum_mean_discrepancy/#summary","title":"Summary","text":"<p>The paper proposes a novel deep semisupervised learning method for zero-shot learning by replacing original labels with textual descriptions to capture category semantics better. The method also minimizes the distribution difference by considering the heterogeneity gap between different modalities and the correlation among unimodal instances, achieving significant improvements over previous methods on benchmark datasets CU200-Birds and Oxford Flowers-102."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_semisupervised_zero-shot_learning_with_maximum_mean_discrepancy/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_semisupervised_zero-shot_learning_with_maximum_mean_discrepancy/#content","title":"Content","text":"<p>Due to the difficulty of collecting labeled images for hundreds of thou- sandsofvisualcategories,zero-shotlearning,whereunseencategoriesdo not have any labeled images in training stage, has attracted more atten- tion. In the past, many studies focused on transferring knowledge from seentounseencategoriesbyprojectingallcategorylabelsintoasemantic space. However, the label embeddings could not adequately express the semanticsofcategories.Furthermore,thecommonsemanticsofseenand unseen instances cannot be captured accurately because the distribution of these instances may be quite different. For these issues, we propose a novel deep semisupervised method by jointly considering the het- erogeneity gap between different modalities and the correlation among unimodal instances. This method replaces the original labels with the correspondingtextual descriptions to better capture the category seman- tics. This method also overcomes the problem of distribution difference by minimizing the maximum mean discrepancy between seen and un- seeninstancedistributions.Extensiveexperimentalresultsontwobench- mark data sets, CU200-Birds and Oxford Flowers-102, indicate that our methodachievessignificantimprovementsoverpreviousmethods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_spherical_quantization_for_image_search/","title":"Deep spherical quantization for image search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_spherical_quantization_for_image_search/#summary","title":"Summary","text":"<p>Summary: The authors propose a method called Deep Spherical Quantization (DSQ) to generate compact binary codes for efficient image search by learning a mapping to transform high-dimensional images into a low-dimensional discriminative space and quantizing the transformed data points using multi-codebook quantization. The technique includes a supervised quantization technique for vectors on the unit hypersphere and an extension for enforcing sparsity on the codebooks. The experiments show that DSQ outperforms state-of-the-art image retrieval methods on three benchmarks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_spherical_quantization_for_image_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_spherical_quantization_for_image_search/#content","title":"Content","text":"<p> In this paper, the authors propose a novel method called Deep Spherical Quantization (DSQ) to generate supervised and compact binary codes for efficient image search. The approach simultaneously learns a mapping to transform high-dimensional images into a low-dimensional discriminative space and quantizes the transformed data points using multi-codebook quantization. The network is forced to L2normalize the features to eliminate the negative effect of norm variance on codebook learning. The resulting vectors are then quantized using a new supervised quantization technique specifically designed for points lying on a unit hypersphere. The authors also introduce an easy-to-implement extension of the technique that enforces sparsity on the codebooks. The experiments demonstrate that DSQ and its sparse variant can generate semantically separable compact binary codes outperforming many state-of-the-art image retrieval methods on three benchmarks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_transferring_quantization/","title":"Deep transferring quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_transferring_quantization/#summary","title":"Summary","text":"<p> The paper proposes a method called deep transferring quantization (DTQ) which introduces transfer learning to network quantization to obtain an accurate low-precision model. They introduce a learnable attentive transfer module and the Kullback-Leibler (KL) divergence to train a low-precision model. Experimental results on image classification and face recognition demonstrate the effectiveness of DTQ."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_transferring_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_transferring_quantization/#content","title":"Content","text":"<p>Network quantization is an effective method for network compression. Existing methods train a low-precision network by fine-tuning from a pre-trained model. However, training a low-precision network often requires large-scale labeled data to achieve superior performance. In many real-world scenarios, only limited labeled data are available due to expensive labeling costs or privacy protection. With limited training data, fine-tuning methods may suffer from the overfitting issue and substantial accuracy loss. To alleviate these issues, we introduce transfer learning into network quantization to obtain an accurate low-precision model. Specifically, we propose a method named deep transferring quantization (DTQ) to effectively exploit the knowledge in a pre-trained full-precision model. To this end, we propose a learnable attentive transfer module to identify the informative channels for alignment. In addition, we introduce the Kullback-Leibler (KL) divergence to further help train a low-precision model. Extensive experiments on both image classification and face recognition demonstrate the effectiveness of DTQ. Keywords: Quantization Deep Transfer Knowledge Distillation"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_triplet_quantization/","title":"Deep triplet quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_triplet_quantization/#summary","title":"Summary","text":"<p> This paper proposes a novel deep learning approach called Deep Triplet Quantization (DTQ) for image retrieval by introducing a triplet training strategy to deep quantization. They demonstrate the effectiveness of the proposed method over other hashing solutions for similarity retrieval. They also design a new triplet selection approach and apply triplet quantization with weak orthogonality during training to generate compact binary codes. Experiments show that DTQ produces high-quality and compact binary codes, outperforming other methods on NUS-WIDE, CIFAR-10, and MS-COCO datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_triplet_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_triplet_quantization/#content","title":"Content","text":"<p>Deep hashing methods have achieved efficient and effective image retrieval by end-to-end learning of deep representations and hash codes from similarity data. We propose Deep Triplet Quantization (DTQ) for efficient and effective image retrieval, which introduces a novel triplet training strategy to deep quantization. The proposed solution focuses on deep learning to quantization approach that has shown superior performance over hashing solutions for similarity retrieval. We design a new triplet selection approach, Group Hard, that randomly selects hard triplets in each image group. To generate compact binary codes, we further apply a triplet quantization with weak orthogonality during triplet training. Extensive experiments demonstrate that DTQ can generate high-quality and compact binary codes, which yields state-of-the-art image retrieval performance on three benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_visual-semantic_quantization_for_efficient_image_retrieval/","title":"Deep visual-semantic quantization for efficient image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_visual-semantic_quantization_for_efficient_image_retrieval/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method for efficient image retrieval through deep visual-semantic quantization (DVSQ). DVSQ is capable of learning deep quantization models from labeled image data and semantic information underlying general text domains. It uses hybrid networks and well-specified loss functions to jointly learn deep visual-semantic embeddings and visual-semantic quantizers. DVSQ is able to generate compact binary codes and achieve state-of-the-art similarity retrieval performance on standard benchmarks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_visual-semantic_quantization_for_efficient_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deep_visual-semantic_quantization_for_efficient_image_retrieval/#content","title":"Content","text":"<p> This paper presents a solution for efficient image retrieval by end-to-end representation learning and compact encoding using deep visual-semantic quantization (DVSQ). DVSQ is the first approach to learning deep quantization models from labeled image data and semantic information underlying general text domains. It jointly learns deep visual-semantic embeddings and visual-semantic quantizers using hybrid networks and well-specified loss functions, enabling efficient and effective image retrieval by supporting maximum inner-product search. Comprehensive empirical evidence shows that DVSQ can generate compact binary codes and yield state-of-the-art similarity retrieval performance on standard benchmarks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deeply_tensor_compressed_transformers_for_end-to-end_object_detection/","title":"Deeply tensor compressed transformers for end-to-end object detection","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deeply_tensor_compressed_transformers_for_end-to-end_object_detection/#summary","title":"Summary","text":"<p>Summary: This paper proposes a method for compressing DETR models used in object detection pipelines by using low-rank tensor decomposition. Through experiments on the COCO dataset, the authors demonstrate that their method can achieve more than 3x compression with minimal loss in accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deeply_tensor_compressed_transformers_for_end-to-end_object_detection/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deeply_tensor_compressed_transformers_for_end-to-end_object_detection/#content","title":"Content","text":"<p> DEtection TRansformer (DETR) is a recently proposed method that streamlines the detection pipeline without complex anchor generation and post-processing procedures, making the detection pipeline more intuitive. However, the numerous redundant parameters in transformers make the DETR models computation and storage intensive, which hinders them from being deployed on resources-constrained devices. To obtain a compact end-to-end detection framework, a proposed deeply compressed transformer with low-rank tensor decomposition can achieve significant parameter and model size reduction while maintaining high detection performance. The proposed method is validated on the COCO dataset through extensive experiments. The experimental results show that 3.7\u00d7 full model compression can be attained with 482\u00d7 feed forward network (FFN) parameter reduction and only 0.6 points accuracy drop."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defend_deep_neural_networks_against_adversarial_examples_via_fixed_and_dynamic_quantized_activation_functions/","title":"Defend deep neural networks against adversarial examples via fixed and dynamic quantized activation functions","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defend_deep_neural_networks_against_adversarial_examples_via_fixed_and_dynamic_quantized_activation_functions/#summary","title":"Summary","text":"<p>This paper proposes a method for defending against adversarial attacks by using quantization of activation functions. The authors also introduce a technique called Dynamic Quantized Activation (DQA) for training robust neural networks. Experiments using the MNIST and CIFAR-10 datasets show that the proposed method significantly improves the robustness of DNNs against various attack methods. This is the first work to apply quantization of activation functions to defend against adversarial examples."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defend_deep_neural_networks_against_adversarial_examples_via_fixed_and_dynamic_quantized_activation_functions/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defend_deep_neural_networks_against_adversarial_examples_via_fixed_and_dynamic_quantized_activation_functions/#content","title":"Content","text":"<p>Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks. In this work, we propose to defend DNNs against adversarial examples using quantization of activation functions. We also propose a Dynamic Quantized Activation (DQA) technique for training robust neural networks. Our experiments with the MNIST and CIFAR-10 datasets under different white-box and black-box attack methods including FGSM, PGD, and C&amp;W attacks demonstrate that the robustness of DNNs can be greatly improved using the proposed DQA. This is the first work to use quantization of activation functions to defend against adversarial examples."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defending_and_harnessing_the_bit-flip_based_adversarial_weight_attack/","title":"Defending and harnessing the bit-flip based adversarial weight attack","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defending_and_harnessing_the_bit-flip_based_adversarial_weight_attack/#summary","title":"Summary","text":"<p>Summary: The paper discusses a new type of adversarial attack on the quantized neural network weights called Bit-Flip Attack (BFA). The authors propose that binarization-aware training and its relaxation, piece-wise clustering, can be effective countermeasures against BFA. The experiments conducted in the study indicate that defending against BFA requires significantly more effective malicious bit-flips on ResNet-20 and VGG-11 networks compared to defend-free networks to achieve the same level of prediction accuracy degradation on CIFAR-10 dataset.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defending_and_harnessing_the_bit-flip_based_adversarial_weight_attack/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defending_and_harnessing_the_bit-flip_based_adversarial_weight_attack/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defensive_quantization%3A_When_efficiency_meets_robustness/","title":"Defensive quantization: When efficiency meets robustness","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defensive_quantization%3A_When_efficiency_meets_robustness/#summary","title":"Summary","text":"<p>Summary: This paper discusses a new approach called Defensive Quantization that optimizes both the efficiency and robustness of deep learning models. This method controls the Lipschitz constant of the network during quantization to defend against adversarial attacks by making sure that the adversarial noise's magnitude remains non-expansive during inference. This results in superior robustness compared to full-precision models while maintaining the same level of hardware efficiency as vanilla quantization approaches.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defensive_quantization%3A_When_efficiency_meets_robustness/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Defensive_quantization%3A_When_efficiency_meets_robustness/#content","title":"Content","text":"<p>Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms. However, conventional quantization approaches are vulnerable to adversarial attacks, and the inferior robustness comes from the error amplification effect. This paper proposes a novel Defensive Quantization method to jointly optimize the efficiency and robustness of deep learning models. The method controls the Lipschitz constant of the network during quantization such that the adversarial noise's magnitude remains non-expansive during inference. The new quantization method effectively defends neural networks against adversarial examples and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Demian%3A_Deep_modality_invariant_adversarial_network/","title":"Demian: Deep modality invariant adversarial network","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Demian%3A_Deep_modality_invariant_adversarial_network/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method for obtaining common representations from different modalities, called modality-invariant representations, using a novel algorithm called Deep Modality Invariant Adversarial Network (DeMIAN). DeMIAN utilizes the idea of Domain Adaptation (DA) and achieved better classification accuracy than the state-of-the-art methods, particularly for benchmark datasets of zero-shot learning. The authors suggest that this method is more straightforward than addressing labeled multi-modal data.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Demian%3A_Deep_modality_invariant_adversarial_network/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Demian%3A_Deep_modality_invariant_adversarial_network/#content","title":"Content","text":"<p> Obtaining common representations from different modalities is important in that they are interchangeable with each other in a classi\ufb01cation problem. For example, we can train a classi\ufb01er on image features in the common representationsandapplyittothetestingofthetextfeatu res in the representations. Existing multi-modalrepresentat ion learning methods mainly aim to extract rich information from paired samples and train a classi\ufb01er by the cor- responding labels; however, collecting paired samples and their labels simultaneously involves high labor costs. Addressing paired modal samples without their labels and single modal data with their labels independently is much easierthanaddressinglabeledmulti-modaldata. Toobtain the common representations under such a situation, we propose to make the distributions over different modalitie s similar in the learned representations, namely modality- invariantrepresentations. Inparticular,weproposeanov el algorithm for modality-invariant representation learnin g, named Deep Modality Invariant Adversarial Network (DeMIAN), which utilizes the idea of Domain Adaptation (DA).Usingthemodality-invariantrepresentationslearn ed by DeMIAN, we achieved better classi\ufb01cation accuracy than with the state-of-the-art methods, especially for som e benchmarkdatasetsofzero-shotlearning."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deploying_image_deblurring_across_mobile_devices%3A_A_perspective_of_quality_and_latency/","title":"Deploying image deblurring across mobile devices: A perspective of quality and latency","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deploying_image_deblurring_across_mobile_devices%3A_A_perspective_of_quality_and_latency/#summary","title":"Summary","text":"<p> The paper focuses on the challenge of deploying image deblurring on mobile devices with acceptable latency. The authors searched for portable network architectures for better quality-latency trade-off and used quantization and pruning for image deblurring. They conducted comprehensive experiments and comparisons and successfully deployed image deblurring application on mobile devices with the acceleration of deep learning accelerators. The paper provides practical deployment guidelines and has been adopted by the championship-winning team in NTIRE 2020 Image Deblurring Challenge on Smartphone Track."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deploying_image_deblurring_across_mobile_devices%3A_A_perspective_of_quality_and_latency/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deploying_image_deblurring_across_mobile_devices%3A_A_perspective_of_quality_and_latency/#content","title":"Content","text":"<p> In this paper, we focus on the challenge of deploying image deblurring on mobile devices with acceptable latency. We searched for portable network architectures that provide a better quality-latency trade-off across mobile devices. We also demonstrate the effectiveness of network optimizations such as quantization and pruning for image deblurring tasks. We conducted comprehensive experiments and comparisons to analyze latency and image quality, and successfully deployed image deblurring application on mobile devices with the acceleration of deep learning accelerators. This paper provides practical deployment guidelines and has been adopted by the championship-winning team in NTIRE 2020 Image Deblurring Challenge on Smartphone Track."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deployment_of_deep_neural_networks_for_object_detection_on_edge_AI_devices_with_runtime_optimization/","title":"Deployment of deep neural networks for object detection on edge AI devices with runtime optimization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deployment_of_deep_neural_networks_for_object_detection_on_edge_AI_devices_with_runtime_optimization/#summary","title":"Summary","text":"<p>Summary: The paper presents a case study on the deployment of object detection networks on an edge AI platform with emphasis on experiences and needs for deployment in embedded environments. The study describes modifications necessary to convert the algorithms from a PyTorch training environment to the deployment environment and evaluates the runtime of the deployed DNN using two different libraries, TensorRT and TorchScript. The study observes slight advantages of TensorRT for convolutional layers and TorchScript for fully connected layers, and concludes that quantization significantly reduces the runtime while having only little impact on the detection performance.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deployment_of_deep_neural_networks_for_object_detection_on_edge_AI_devices_with_runtime_optimization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Deployment_of_deep_neural_networks_for_object_detection_on_edge_AI_devices_with_runtime_optimization/#content","title":"Content","text":"<p> Deep neural networks have proven increasingly important for automotive scene understanding with new algorithms offering constant improvements of the detection performance. However, there is little emphasis on experiences and needs for deployment in embedded environments. We therefore perform a case study of the deployment of two representative object detection networks on an edge AI platform. In particular, we consider RetinaNet for image-based 2D object detection and PointPillars for LiDAR-based 3D object detection. We describe the modifications necessary to convert the algorithms from a PyTorch training environment to the deployment environment taking into account the available tools. We evaluate the runtime of the deployed DNN using two different libraries, TensorRT and TorchScript. In our experiments, we observe slight advantages of TensorRT for convolutional layers and TorchScript for fully connected layers. We also study the trade-off between runtime and performance, when selecting an optimized setup for deployment, and observe that quantization significantly reduces the runtime while having only little impact on the detection performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Design_automation_for_efficient_deep_learning_computing/","title":"Design automation for efficient deep learning computing","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Design_automation_for_efficient_deep_learning_computing/#summary","title":"Summary","text":"<p> The paper proposes design automation techniques for efficient neural networks including auto design specialized models, auto channel pruning, and auto mixed-precision quantization. The learning-based automated design achieves better performance and efficiency than rule-based human design and reduces the design cycle by 200%."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Design_automation_for_efficient_deep_learning_computing/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Design_automation_for_efficient_deep_learning_computing/#content","title":"Content","text":"<p> Ef\ufb01cient deep learning computing requires algorithm and hardware co-design to enable specialization. We propose design automation techniques for ef\ufb01cient neural networks, including auto design specialized models, auto channel pruning, and auto mixed-precision quantization. Our learning-based, automated design achieves superior performance and ef\ufb01ciency than rule-based human design. We also demonstrate a 200% shorter design cycle than previous work, making it affordable to design specialized neural network models for different hardware platforms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dfqf%3A_Data_free_quantization-aware_fine-tuning/","title":"Dfqf: Data free quantization-aware fine-tuning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dfqf%3A_Data_free_quantization-aware_fine-tuning/#summary","title":"Summary","text":"<p>Summary: The paper proposes a data free quantization-aware fine-tuning (DFQF) technique for neural network quantization. The method fine-tunes the quantized network with generated images instead of real training data through knowledge distillation. The proposed method outperforms existing post-train quantization methods and achieves W4A4 quantization of ResNet20 on the CIFAR10 dataset with less than 1% accuracy drop.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dfqf%3A_Data_free_quantization-aware_fine-tuning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dfqf%3A_Data_free_quantization-aware_fine-tuning/#content","title":"Content","text":"<p>Data free deep neural network quantization is a practical challenge, since the original training data is often unavailable due to some privacy, proprietary or transmission issues. The existing methods implicitly equate data-free with training-free and quantize model manually through analyzing the weights' distribution. It leads to a significant accuracy drop in lower than 6-bit quantization. In this work, we propose the data free quantization-aware fine-tuning (DFQF), wherein no real training data is required, and the quantized network is fine-tuned with generated images. Specifically, we start with training a generator from the pre-trained full-precision network with inception score loss, batch-normalization statistics loss and adversarial loss to synthesize a fake image set. Then we fine-tune the quantized student network with the full-precision teacher network and the generated images by utilizing knowledge distillation (KD). The proposed DFQF outperforms state-of-the-art post-train quantization methods, and achieve W4A4 quantization of ResNet20 on the CIFAR10 dataset within 1% accuracy drop.  Keywords: Data-free, Quantization, Adversarial"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_dynamic_quantization_with_mixed_precision_and_adaptive_resolution/","title":"Differentiable dynamic quantization with mixed precision and adaptive resolution","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_dynamic_quantization_with_mixed_precision_and_adaptive_resolution/#summary","title":"Summary","text":"<p>Summary: The paper introduces a Differentiable Dynamic Quantization (DDQ) approach in neural network quantization. It is a hardware-friendly and fully differentiable approach which learns all of the tedious hyper-parameters like precision, dynamic range and stepsize. DDQ is capable of quantizing lightweight architectures like MobileNets and reduces training runtime by 25%. Extensive experiments show that DDQ outperforms prior arts on many networks and benchmarks, especially on efficient and compact models where it achieves lossless 4-bit quantization for MobileNetV2 on ImageNet.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_dynamic_quantization_with_mixed_precision_and_adaptive_resolution/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_dynamic_quantization_with_mixed_precision_and_adaptive_resolution/#content","title":"Content","text":"<p> Model quantization is challenging due to many tedious hyper-parameters such as precision (bitwidth), dynamic range (minimum and maximum discrete values) and stepsize (interval between discrete values). Unlike prior arts that carefully tune these values, we present a fully differentiable approach to learn all of them, named Differentiable Dynamic Quantization (DDQ), which has several benefits. DDQ is able to quantize challenging lightweight architectures like MobileNets, where different layers prefer different quantization parameters. DDQ is hardware-friendly and can be easily implemented using low-precision matrix-vector multiplication, making it capable in many hardware such as ARM. DDQ reduces training runtime by 25% compared to state-of-the-arts. Extensive experiments show that DDQ outperforms prior arts on many networks and benchmarks, especially when models are already efficient and compact. e.g. DDQ is the first approach that achieves lossless 4-bit quantization for MobileNetV2 on ImageNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_fine-grained_quantization_for_deep_neural_network_compression/","title":"Differentiable fine-grained quantization for deep neural network compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_fine-grained_quantization_for_deep_neural_network_compression/#summary","title":"Summary","text":"<p> This paper proposes a fine-grained quantization approach for deep neural network compression by optimizing a mixed-precision quantization scheme. The proposed approach optimizes the search space of quantization bitwidth from discrete to a continuous domain. The approach can potentially produce more efficient models compared to traditional quantization methods by striking a better balance between accuracy and compression rate for different layers/structures. The mixed-precision quantization scheme generated by the proposed approach outperforms the accuracy of traditional quantization methods under the same compression rate."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_fine-grained_quantization_for_deep_neural_network_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_fine-grained_quantization_for_deep_neural_network_compression/#content","title":"Content","text":"<p>Neural networks have shown great performance in cognitive tasks. When deploying network models on mobile devices with limited resources, weight quantization has been widely adopted. Binary quantization obtains the highest compression but usually results in big accuracy drop. In practice, 8-bit or 16-bit quantization is often used aiming at maintaining the same accuracy as the original 32-bit precision. We observe different quantization schemes have different accuracy impact on different layers. Thus judiciously selecting different precision for different layers/structures can potentially produce more ef\ufb01cient models compared to traditional quantization methods by striking a better balance between accuracy and compression rate. In this work, we propose a \ufb01ne-grained quantization approach for deep neural network compression by relaxing the search space of quantization bitwidth from discrete to a continuous domain. The proposed approach applies gradient descend based optimization to generate a mixed-precision quantization scheme that outperforms the accuracy of traditional quantization methods under the same compression rate."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_model_compression_via_pseudo_quantization_noise/","title":"Differentiable model compression via pseudo quantization noise","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_model_compression_via_pseudo_quantization_noise/#summary","title":"Summary","text":"<p>The paper proposes DiffQ, a differentiable method for model compression for quantizing model parameters. The method involves adding independent pseudo quantization noise to model parameters during training to approximate the effect of a quantization operator. DiffQ optimizes the number of bits used per individual weight or groups of weights, given a single hyper-parameter balancing between the quantized model size and accuracy in end-to-end training. The authors demonstrate that their method is competitive with STE based quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_model_compression_via_pseudo_quantization_noise/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_model_compression_via_pseudo_quantization_noise/#content","title":"Content","text":"<p>We propose DiffQa di\ufb00erentiable method for model compression for quantizing model parameters without gradient approximations. The method adds independent pseudo quantization noise to model parameters during training to approximate the e\ufb00ect of a quantization operator. DiffQis di\ufb00erentiable both with respect to the unquantized weights and the number of bits used. Given a single hyper-parameter balancingbetweenthequantizedmodelsizeandaccuracy, DiffQoptimizesthenumberofbits used per individual weight or groups of weights, in end-to-end training. The authors experimentally verify that their method is competitive with STE based quantization techniques on several benchmarks and architectures for image classification, language modeling, and audio source separation. For instance, on the ImageNet dataset, DiffQcompresses a 12 layers transformer-based model by more than a factor of 8, with a loss of 0.3% in model accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_soft_quantization%3A_Bridging_full-precision_and_low-bit_neural_networks/","title":"Differentiable soft quantization: Bridging full-precision and low-bit neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_soft_quantization%3A_Bridging_full-precision_and_low-bit_neural_networks/#summary","title":"Summary","text":"<p>Summary: The authors propose a method called Differentiable Soft Quantization (DSQ) to train low-bit neural networks. DSQ approximates standard quantization, maintains accurate gradients in backward propagation and reduces quantization loss. They show through experiments that DSQ outperforms state-of-the-art quantization methods and present an efficient implementation for deploying 2 to 4-bit DSQ on devices with ARM architecture, achieving up to 1.7\u00d7 speedup compared to the open-source 8-bit high-performance inference framework NCNN.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_soft_quantization%3A_Bridging_full-precision_and_low-bit_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Differentiable_soft_quantization%3A_Bridging_full-precision_and_low-bit_neural_networks/#content","title":"Content","text":"<p>In this paper, the authors propose a Differentiable Soft Quantization (DSQ) method to bridge the gap between full-precision and low-bit neural networks. DSQ can evolve during training to approximate standard quantization and can help pursue accurate gradients in backward propagation while reducing quantization loss. The authors demonstrate through extensive experiments over several popular network structures that training low-bit neural networks with DSQ outperforms state-of-the-art quantization methods. The authors also present an efficient implementation for deploying 2 to 4-bit DSQ on devices with ARM architecture, achieving up to 1.7\u00d7 speedup compared to the open-source 8-bit high-performance inference framework NCNN [31]."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Direct_quantization_for_training_highly_accurate_low_bit-width_deep_neural_networks/","title":"Direct quantization for training highly accurate low bit-width deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Direct_quantization_for_training_highly_accurate_low_bit-width_deep_neural_networks/#summary","title":"Summary","text":"<p> This paper proposes two novel techniques for the training of deep convolutional neural networks with low bit-width weights and activations. The first technique enables direct updating of quantized weights to minimize the cost function using gradient descent. The second technique considers the quantization errors of individual channels to learn activation quantizers that minimize errors in most channels. These techniques achieve state-of-the-art performance on image classification tasks using AlexNet, ResNet, and MobileNetV2 architectures on CIFAR-100 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Direct_quantization_for_training_highly_accurate_low_bit-width_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Direct_quantization_for_training_highly_accurate_low_bit-width_deep_neural_networks/#content","title":"Content","text":"<p>This paper proposes two novel techniques to train deep convolutional neural networks with low bit-width weights and activations. The first technique proposes a novel method that enables direct updating of quantized weights with learnable quantization levels to minimize the cost function using gradient descent, to address the mismatch caused by updating only full-precision weights. The second technique proposes a method to take into account the quantization errors of individual channels to learn activation quantizers that minimize the quantization errors in the majority of channels. The proposed method achieves state-of-the-art performance on the image classification task, using AlexNet, ResNet, and MobileNetV2 architectures on CIFAR-100 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Discrimination-aware_channel_pruning_for_deep_neural_networks/","title":"Discrimination-aware channel pruning for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Discrimination-aware_channel_pruning_for_deep_neural_networks/#summary","title":"Summary","text":"<p>The paper introduces a new approach for deep model compression called discrimination-aware channel pruning (DCP). This approach uses discrimination-aware losses to increase the discriminative power of intermediate layers and select the most discriminative channels for each layer. A greedy algorithm is proposed for channel selection and parameter optimization. Experiments show that DCP is effective, achieving a pruned ResNet-50 with a 0.39% improvement in top-1 accuracy on ILSVRC-12 with a 30% reduction of channels."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Discrimination-aware_channel_pruning_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Discrimination-aware_channel_pruning_for_deep_neural_networks/#content","title":"Content","text":"<p>Channel pruning is an effective approach for deep model compression, but existing methods have limitations such as computational expense and ignoring the discriminative power of channels. In this paper, we propose discrimination-aware channel pruning (DCP) which introduces discrimination-aware losses into the network to increase the discriminative power of intermediate layers and select the most discriminative channels for each layer by considering the additional loss and reconstruction error. A greedy algorithm is proposed for channel selection and parameter optimization. Experiments demonstrate the effectiveness of DCP, with a pruned ResNet-50 achieving a 0.39% improvement in top-1 accuracy on ILSVRC-12 with a 30% reduction of channels."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance-aware_quantization/","title":"Distance-aware quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance-aware_quantization/#summary","title":"Summary","text":"<p> This paper proposes a novel quantization method called Distance-Aware Quantizer (DAQ) to reduce bit-widths of weights and/or activations in neural networks. The method consists of a distance-aware soft rounding (DASR) and a temperature controller. DASR approximates discrete rounding with the kernel soft argmax to overcome the gradient mismatch issue, and the controller adjusts the temperature parameter in DASR adaptively according to the input to address the quantizer gap problem. The experimental results demonstrate that DAQ outperforms the state-of-the-art for various bit-widths."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance-aware_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance-aware_quantization/#content","title":"Content","text":"<p>We address the problem of network quantization in this paper, which is reducing bit-widths of weights and/or activations to lighten network architectures. We introduce a novel quantizer, called distance-aware quantizer (DAQ), which consists of a distance-aware soft rounding (DASR) and a temperature controller. DASR approximates the discrete rounding with the kernel soft argmax to alleviate the gradient mismatch problem. The controller adjusts the temperature parameter in DASR adaptively according to the input to address the quantizer gap problem. Experimental results show that DAQ outperforms the state of the art significantly for various bit-widths."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance-based_image_classification%3A_Generalizing_to_new_classes_at_near-zero_cost/","title":"Distance-based image classification: Generalizing to new classes at near-zero cost","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance-based_image_classification%3A_Generalizing_to_new_classes_at_near-zero_cost/#summary","title":"Summary","text":"<p>Summary: The paper introduces a new distance-based image classification method called the rank distance. This technique represents images as ranked sets of distances to a set of prototypes, making it computationally cheap and easy to implement. The method outperforms more complex state-of-the-art methods on several datasets and is suitable for automatic image tagging and interactive search as it can generalize well to new classes at near-zero cost.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance-based_image_classification%3A_Generalizing_to_new_classes_at_near-zero_cost/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance-based_image_classification%3A_Generalizing_to_new_classes_at_near-zero_cost/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance_encoded_product_quantization_for_approximate_k-nearest_neighbor_search_in_high-dimensional_space/","title":"Distance encoded product quantization for approximate k-nearest neighbor search in high-dimensional space","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance_encoded_product_quantization_for_approximate_k-nearest_neighbor_search_in_high-dimensional_space/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new compact code representation for approximate K-nearest neighbor search that encodes cluster index and quantized distance between a point and its cluster center in each subspace using bit-budget distribution. It also proposes two distance estimators tailored to the representation, which is evaluated on benchmarks and shown to improve search accuracy over other tested techniques.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance_encoded_product_quantization_for_approximate_k-nearest_neighbor_search_in_high-dimensional_space/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance_encoded_product_quantization_for_approximate_k-nearest_neighbor_search_in_high-dimensional_space/#content","title":"Content","text":"<p> Approximate K-nearest neighbor search is a fundamental problem in computer science, for high-dimensional and large-scale data. Recently, product quantization has been proposed as a technique for encoding high-dimensional data to compact codes. In this paper, a new compact code representation that encodes both the cluster index and quantized distance between a point and its cluster center in each subspace by distributing the bit-budget is proposed. Two distance estimators tailored to the representation are also proposed. This method is evaluated on benchmarks consisting of GIST, VLAD, and CNN features and is shown to significantly improve search accuracy over other tested techniques."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance_quantization_method_for_fast_nearest_neighbor_search_computations_with_applications_to_motion_estimation/","title":"Distance quantization method for fast nearest neighbor search computations with applications to motion estimation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance_quantization_method_for_fast_nearest_neighbor_search_computations_with_applications_to_motion_estimation/#summary","title":"Summary","text":"<p>Summary: This paper proposes an approach for reducing the complexity of finding the nearest neighbor of a query vector in a high-dimensional space. The approach involves applying non-uniform quantization within the metric computation process, which reduces the search metric computation resolution while preserving the minimum distance ranking. The proposed approach can significantly reduce the number and complexity of required arithmetic operations, does not increase complexity with the order of l p-norm, input bit size, or dimensionality, and has a small penalty in performance. The proposed approach is demonstrated through analytical and experimental studies using motion estimation and compensation for video coding as an example application.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance_quantization_method_for_fast_nearest_neighbor_search_computations_with_applications_to_motion_estimation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distance_quantization_method_for_fast_nearest_neighbor_search_computations_with_applications_to_motion_estimation/#content","title":"Content","text":"<p> The problem of finding the nearest neighbor of a query vector in a high dimensional space poses computational challenges due to the size of the database, dimensionality of the search space, and the metric complexity. While many existing algorithms reduce complexity by altering the dataset or computing the chosen distance metric to full precision, our proposed approach reduces complexity further by applying non-uniform quantization within the metric computation process. This reduces the search metric computation resolution while preserving the minimum distance ranking. Our approach can significantly reduce the number and complexity of required arithmetic operations, does not increase complexity with the order of l p-norm or input bit size, increases only slowly with dimensionality, and has a small penalty in performance if designed optimally. We present analytical and experimental studies of our proposed approach using motion estimation and compensation for video coding as an example application."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distilled_low_rank_neural_radiance_field_with_quantization_for_light_field_compression/","title":"Distilled low rank neural radiance field with quantization for light field compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distilled_low_rank_neural_radiance_field_with_quantization_for_light_field_compression/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new light field compression technique called QDLR-NeRF, which uses a Neural Radiance Field (NeRF) to learn an implicit scene representation and enable view synthesis. The method reduces model size by learning under a Low Rank (LR) constraint using a Tensor Train (TT) decomposition in an Alternating Direction Method of Multipliers (ADMM) optimization framework and by quantizing the components of the tensor train decomposition. To overcome the optimization challenge, a network distillation operation that separates the LR approximation and the weight quantization in the network training is introduced. The experimental results show that the proposed technique achieves better compression efficiency compared to state-of-the-art methods and allows for high-quality light field view synthesis.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distilled_low_rank_neural_radiance_field_with_quantization_for_light_field_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distilled_low_rank_neural_radiance_field_with_quantization_for_light_field_compression/#content","title":"Content","text":"<p> In this paper, a novel light field compression method called Quantized Distilled Low Rank Neural Radiance Field (QDLR-NeRF) representation is proposed. Unlike other compression methods that encode the set of light field sub-aperture images, this method learns an implicit scene representation in the form of a Neural Radiance Field (NeRF), which also enables view synthesis. The size of the model is reduced by first learning it under a Low Rank (LR) constraint using a Tensor Train (TT) decomposition in an Alternating Direction Method of Multipliers (ADMM) optimization framework. To further reduce the model size, the components of the tensor train decomposition need to be quantized. The optimization of the NeRF model by simultaneously taking the low rank constraint and the rate-constrained weight quantization into consideration is challenging. To deal with this difficulty, a network distillation operation that separates the low rank approximation and the weight quantization in the network training is introduced. The information from the initial LR-constrained NeRF (LR-NeRF) is distilled to a model of a much smaller dimension (DLR-NeRF) based on the TT decomposition of the LR-NeRF. An optimized global codebook is then learned to quantize all TT components, producing the final QDLR-NeRF. Experimental results show that this proposed method yields better compression efficiency compared to state-of-the-art methods and it also has the advantage of allowing the synthesis of any light field view with a high quality."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distributed_adaptive_binary_quantization_for_fast_nearest_neighbor_search/","title":"Distributed adaptive binary quantization for fast nearest neighbor search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distributed_adaptive_binary_quantization_for_fast_nearest_neighbor_search/#summary","title":"Summary","text":"<p>Summary: The paper proposes an adaptive binary quantization method for generating discriminative hash codes using prototypes associated with small binary codes, making it computationally efficient and scalable for long hash codes. They also developed a distributed framework for large-scale learning and demonstrated that the proposed method outperforms state-of-the-art hashing methods in experiments.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distributed_adaptive_binary_quantization_for_fast_nearest_neighbor_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distributed_adaptive_binary_quantization_for_fast_nearest_neighbor_search/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distribution-aware_adaptive_multi-bit_quantization/","title":"Distribution-aware adaptive multi-bit quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distribution-aware_adaptive_multi-bit_quantization/#summary","title":"Summary","text":"<p>Summary: The paper presents a distribution-aware multi-bit quantization method (DMBQ) and loss-guided bit-width allocation (LBA) to compress deep neural networks by quantizing weights and activations into multi-bit binary networks (MBNs). The proposed method outperforms state-of-the-art quantized networks in terms of accuracy and is more efficient even for extremely low bit-width quantization cases.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distribution-aware_adaptive_multi-bit_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distribution-aware_adaptive_multi-bit_quantization/#content","title":"Content","text":"<p>In this paper, the authors propose a distribution-aware multi-bit quantization method (DMBQ) for compressing deep neural networks by quantizing the weights and activations into multi-bit binary networks (MBNs). They also propose loss-guided bit-width allocation (LBA) to adaptively prune and optimize neural networks. The experimental results show that their method outperforms state-of-the-art quantized networks in terms of accuracy and is more efficient in terms of training time, even for the extremely low bit-width quantization cases."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distribution_adaptive_int8_quantization_for_training_cnns/","title":"Distribution adaptive int8 quantization for training cnns","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distribution_adaptive_int8_quantization_for_training_cnns/#summary","title":"Summary","text":"<p> The paper presents a training framework for convolutional neural networks based on INT8 quantization with Gradient Vectorized Quantization and Magnitude-aware Clipping Strategy. The proposed method achieves almost lossless training accuracy for various computer vision tasks and is superior to state-of-the-art techniques. Additionally, an INT8 kernel is implemented which accelerates the training iteration by over 200% under the latest Turing architecture, demonstrating the method's excellence in training accuracy and speed."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distribution_adaptive_int8_quantization_for_training_cnns/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Distribution_adaptive_int8_quantization_for_training_cnns/#content","title":"Content","text":"<p>In this paper, we propose a novel INT8 quantization training framework for convolutional neural networks that addresses the variability and uncertainty of gradient distribution. We adopt Gradient Vectorized Quantization to quantize the gradient and introduce Magnitude-aware Clipping Strategy to take the magnitudes of gradients into consideration when minimizing the quantization error. Experimental results on various computer vision tasks demonstrate that the proposed method has achieved almost lossless training accuracy for different backbones, which is superior to state-of-the-art techniques. Furthermore, we have implemented an INT8 kernel that can accelerate the training iteration by more than 200% under the latest Turing architecture, thus exhibiting our method's excellence on both training accuracy and speed."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Diversifying_sample_generation_for_accurate_data-free_quantization/","title":"Diversifying sample generation for accurate data-free quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Diversifying_sample_generation_for_accurate_data-free_quantization/#summary","title":"Summary","text":"<p>Summary: The Diverse Sample Generation (DSG) scheme is proposed to mitigate the performance drop in quantized neural network models due to homogenized synthetic data created for calibration. It slackens the alignment of feature statistics in BN layer to relax the constraint at the distribution level, and designs a layerwise enhancement to reinforce specific layers for different data samples. It consistently improves various network architectures and quantization methods, especially when quantized to lower bits. The synthetic data calibrated models perform close to those with real data and even outperform them on W4A4.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Diversifying_sample_generation_for_accurate_data-free_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Diversifying_sample_generation_for_accurate_data-free_quantization/#content","title":"Content","text":"<p> Quantization is a prevalent approach for compressing and accelerating neural networks, and data-free quantization has been studied as a promising solution. However, the synthetic data created for calibration suffers from homogenization at both distribution and sample levels, leading to a performance drop in quantized models. To mitigate this issue, the Diverse Sample Generation (DSG) scheme is proposed, which slackens the alignment of feature statistics in the BN layer to relax the constraint at the distribution level and designs a layerwise enhancement to reinforce specific layers for different data samples. The DSG scheme consistently improves various network architectures and quantization methods for large-scale image classification tasks, especially when quantized to lower bits. Moreover, models calibrated with synthetic data perform close to those calibrated with real data and even outperform them on W4A4."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Divide_and_conquer%3A_Leveraging_intermediate_feature_representations_for_quantized_training_of_neural_networks/","title":"Divide and conquer: Leveraging intermediate feature representations for quantized training of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Divide_and_conquer%3A_Leveraging_intermediate_feature_representations_for_quantized_training_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper presents a new knowledge distillation approach called Divide and Conquer Quantization (DCQ) for quantized training of neural networks. The algorithm divides a pretrained full-precision DNN into multiple sections, and trains each section independently in the quantized domain. DCQ improves the performance of DoReFa-Net by 21.6% and 9.3% for binary and ternary quantization. The divide and conquer strategy offers additional speedup through enabling parallelization, and all sections are later stitched together to form the fully quantized network.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Divide_and_conquer%3A_Leveraging_intermediate_feature_representations_for_quantized_training_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Divide_and_conquer%3A_Leveraging_intermediate_feature_representations_for_quantized_training_of_neural_networks/#content","title":"Content","text":"<p> The paper presents a new approach towards knowledge distillation through teacher-student paradigm, called Divide and Conquer Quantization (DCQ), for quantized training of neural networks. The algorithm harvests rich intermediate representations from the deep layers of a DNN for quantization. It divides a pretrained full-precision DNN to multiple sections, each of which exposes intermediate features to train a team of students independently in the quantized domain. Experiments on various DNNs show that DCQ improves the performance of a state-of-the-art quantized training technique, called DoReFa-Net, by 21.6% and 9.3% for binary and ternary quantization, respectively. The paper proposes a divide and conquer strategy, which makes the training of each student section possible in isolation, offering additional speedup through enabling parallelization, while all these independently trained sections are later stitched together to form the equivalent fully quantized network."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Do_All_MobileNets_Quantize_Poorly%3F_Gaining_Insights_into_the_Effect_of_Quantization_on_Depthwise_Separable_Convolutional_Networks_Through_the_Eyes_of_Multi-scale%C2%A0%E2%80%A6/","title":"Do All MobileNets Quantize Poorly? Gaining Insights into the Effect of Quantization on Depthwise Separable Convolutional Networks Through the Eyes of Multi-scale\u00a0\u2026","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Do_All_MobileNets_Quantize_Poorly%3F_Gaining_Insights_into_the_Effect_of_Quantization_on_Depthwise_Separable_Convolutional_Networks_Through_the_Eyes_of_Multi-scale%C2%A0%E2%80%A6/#summary","title":"Summary","text":"<p>Summary: The paper focuses on the quantization issue in MobileNets, the family of deep convolutional neural networks suitable for mobiles. While existing studies have introduced quantization-aware training and other methods, there is still limited understanding of why MobileNets quantize poorly compared to other CNN architectures. To gain deeper insights into this phenomenon, the authors investigate the impact of quantization on the weight and activation distributional dynamics of MobileNet-V1, reveal significant dynamic range fluctuations and a \"distributional mismatch\" between channel-wise and layer-wise distributions in DWSCNNs, and suggest innovative strategies for reducing such changes and improving post-training quantization for mobile.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Do_All_MobileNets_Quantize_Poorly%3F_Gaining_Insights_into_the_Effect_of_Quantization_on_Depthwise_Separable_Convolutional_Networks_Through_the_Eyes_of_Multi-scale%C2%A0%E2%80%A6/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Do_All_MobileNets_Quantize_Poorly%3F_Gaining_Insights_into_the_Effect_of_Quantization_on_Depthwise_Separable_Convolutional_Networks_Through_the_Eyes_of_Multi-scale%C2%A0%E2%80%A6/#content","title":"Content","text":"<p>As the \u201cMobile AI\u201d revolution continues to grow, so does the need to understand the behaviour of edge-deployed deep neural networks. In particular, MobileNets [ 9,22] are the go-to family of deep convolutional neural networks (CNN) for mobile. However, they often have significant accuracy degradation under post-training quantization. While studies have introduced quantization-aware training and other methods to tackle this challenge, there is limited understanding into why MobileNets (and potentially depthwise-separable CNNs (DWSCNN) in general) quantize so poorly compared to other CNN architectures. Motivated to gain deeper insights into this phenomenon, we take a different strategy and study the multi-scale distributional dynamics of MobileNet-V1, a set of smaller DWSCNNs, and regular CNNs. Speci\ufb01cally, we investigate the impact of quantization on the weight and activation distributional dynamics as information propagates from layer to layer, as well as overall changes in distributional dynamics at the network level. This \ufb01ne-grained analysis revealed significant dynamic range \ufb02uctuations and a \u201cdistributional mismatch\u201d between channelwise and layerwise distributions in DWSCNNs that lead to increasing quantized degradation and distributional shift during information propagation. Furthermore, analysis of the activation quantization errors show that there is greater quantization error accumulation in DWSCNN compared to regular CNNs. The hope is that such insights can lead to innovative strategies for reducing such distributional dynamics changes and improve post-training quantization for mobile."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Domain-smoothing_network_for_zero-shot_sketch-based_image_retrieval/","title":"Domain-smoothing network for zero-shot sketch-based image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Domain-smoothing_network_for_zero-shot_sketch-based_image_retrieval/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Domain-Smoothing Network (DSN) to address the challenges in cross-modal retrieval task of Zero-Shot Sketch-Based Image Retrieval. The approach involves using a cross-modal contrastive method and a category-specific memory bank with sketch features. The method demonstrated state-of-the-art results on Sketchy and TU-Berlin datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Domain-smoothing_network_for_zero-shot_sketch-based_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Domain-smoothing_network_for_zero-shot_sketch-based_image_retrieval/#content","title":"Content","text":"<p> Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) is a challenging cross-modal retrieval task. In this paper, we propose a Domain-Smoothing Network (DSN) to tackle the problem of the domain gap between sketches and natural images and the large intra-class diversity in sketches. Our approach consists of a cross-modal contrastive method and a category-specific memory bank with sketch features. The method achieved state-of-the-art results on both Sketchy and TU-Berlin datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dorefa-net%3A_Training_low_bitwidth_convolutional_neural_networks_with_low_bitwidth_gradients/","title":"Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dorefa-net%3A_Training_low_bitwidth_convolutional_neural_networks_with_low_bitwidth_gradients/#summary","title":"Summary","text":"<p>Summary: The paper presents DoReFa-Net, a method to train convolutional neural networks using low bitwidth weights and activations. The proposed method quantizes parameter gradients to low bitwidth numbers during the backward pass and uses bit convolution kernels to accelerate training and inference. The application of bit convolutions enables the efficient implementation of DoReFa-Net in various hardware platforms. The experimental results demonstrate that the proposed method can achieve comparable prediction accuracy with 32-bit counterparts. Notably, a 1-bit weights, 2-bit activations DoReFa-Net derived from AlexNet, using 6-bit gradients, achieved a top-1 accuracy of 46.1% on ImageNet validation set. The code for the DoReFa-Net AlexNet model is available openly.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dorefa-net%3A_Training_low_bitwidth_convolutional_neural_networks_with_low_bitwidth_gradients/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dorefa-net%3A_Training_low_bitwidth_convolutional_neural_networks_with_low_bitwidth_gradients/#content","title":"Content","text":"<p> We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. Our method quantizes the parameter gradients to low bitwidth numbers during the backward pass before propagating them to convolutional layers. As convolutions can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Additionally, bit convolutions can be implemented efficiently on CPU, FPGA, ASIC, and GPU, allowing DoReFa-Net to be used for training low bitwidth neural networks on these hardware platforms. Experimental results on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. A 1-bit weights, 2-bit activations DoReFa-Net derived from AlexNet can be trained from scratch using 6-bit gradients to achieve a top-1 accuracy of 46.1% on ImageNet validation set. The DoReFa-Net AlexNet model is publicly available."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_network_quantization_for_efficient_video_inference/","title":"Dynamic network quantization for efficient video inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_network_quantization_for_efficient_video_inference/#summary","title":"Summary","text":"<p>Summary: This paper presents a dynamic network quantization framework that selects optimal precision for each frame of a video clip conditioned on the input for efficient video recognition. The proposed approach trains two networks in parallel, a recognition network and a lightweight network, to achieve both competitive performance and resource efficiency. The experiments on benchmark datasets showed significant savings in computation and memory usage while outperforming existing state-of-the-art methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_network_quantization_for_efficient_video_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_network_quantization_for_efficient_video_inference/#content","title":"Content","text":"<p>Deep convolutional networks have recently achieved great success in video recognition, yet their practical realization remains a challenge due to the large amount of computational resources required to achieve robust recognition. Motivated by the effectiveness of quantization for boosting efficiency, in this paper, we propose a dynamic network quantization framework, that selects optimal precision for each frame conditioned on the input for efficient video recognition. Specifically, given a video clip, we train a very lightweight network in parallel with the recognition network, to produce a dynamic policy indicating which numerical precision to be used per frame in recognizing videos. We train both networks effectively using standard backpropagation with a loss to achieve both competitive performance and resource efficiency required for video recognition. Extensive experiments on four challenging diverse benchmark datasets demonstrate that our proposed approach provides significant savings in computation and memory usage while outperforming the existing state-of-the-art methods. Project page: https://cs-people.bu.edu/sunxm/VideoIQ/project.html."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_precision_analog_computing_for_neural_networks/","title":"Dynamic precision analog computing for neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_precision_analog_computing_for_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes extending analog computing architectures to support varying levels of precision to decrease the impact of noise, and a method for learning the precision of each layer of a pre-trained model without retraining network weights. The authors evaluate this method on analog architectures subject to various noise sources and find that employing dynamic precision reduces energy consumption significantly for computer vision and natural language processing models with &lt;2% accuracy degradation. They apply dynamic precision to a shot-noise limited homodyne optical neural network and simulate inference at an optical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERT.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_precision_analog_computing_for_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_precision_analog_computing_for_neural_networks/#content","title":"Content","text":"<p>Abstract: Analog electronic and optical computing have been recognized as a promising approach for accelerating matrix multiplications, a computationally expensive operation in deep learning. However, analog computing exhibits tremendous advantages over digital computing only when operations are executed at low precision. In this work, the authors propose extending analog computing architectures to support varying levels of precision by repeating operations and averaging the result, decreasing the impact of noise. They also propose a method for learning the precision of each layer of a pre-trained model without retraining network weights. The authors evaluate this method on analog architectures subject to a variety of noise sources such as shot noise, thermal noise, and weight noise and find that employing dynamic precision reduces energy consumption by up to 89% for computer vision models such as Resnet50 and by 24% for natural language processing models such as BERT. In one example, they apply dynamic precision to a shot-noise limited homodyne optical neural network and simulate inference at an optical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERT with &lt;2% accuracy degradation.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_programming_assisted_quantization_approaches_for_compressing_Normal_and_robust_DNN_models/","title":"Dynamic programming assisted quantization approaches for compressing Normal and robust DNN models","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_programming_assisted_quantization_approaches_for_compressing_Normal_and_robust_DNN_models/#summary","title":"Summary","text":"<p>Summary: The authors propose two weight quantization approaches, DPR and DPQ, for compressing DNNs using dynamic programming based algorithms. These approaches produce models with higher inference accuracy than recently proposed counterparts while achieving same or larger compression. They are also extended for compressing robust DNNs, achieving significant compression with less than 3% accuracy drop on both natural and adversarial examples.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_programming_assisted_quantization_approaches_for_compressing_Normal_and_robust_DNN_models/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Dynamic_programming_assisted_quantization_approaches_for_compressing_Normal_and_robust_DNN_models/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ECQ%3A_Explainability-Driven_Quantization_for_Low-Bit_and_Sparse_DNNs/","title":"ECQ: Explainability-Driven Quantization for Low-Bit and Sparse DNNs","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ECQ%3A_Explainability-Driven_Quantization_for_Low-Bit_and_Sparse_DNNs/#summary","title":"Summary","text":"<p>The paper proposes ECQx, a novel quantization method that uses Explainable AI (XAI) and information theory to generate ultra low-precision and sparse neural networks while maintaining or improving model performance. It assigns weight values based on weight relevances obtained from Layer-wise Relevance Propagation (LRP) and the information content of the clusters (entropy optimization) rather than just their distances to the quantization clusters. The method allows for highly compressible file sizes and is evaluated on different models and datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ECQ%3A_Explainability-Driven_Quantization_for_Low-Bit_and_Sparse_DNNs/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ECQ%3A_Explainability-Driven_Quantization_for_Low-Bit_and_Sparse_DNNs/#content","title":"Content","text":"<p>The paper presents a novel quantization paradigm for DNNs called ECQx, which leverages concepts of Explainable AI (XAI) and information theory to generate ultra low-precision (2-5 bit) and sparse neural networks while maintaining or improving model performance. The method assigns weight values based on weight relevances obtained from Layer-wise Relevance Propagation (LRP) and the information content of the clusters (entropy optimization) rather than just their distances to the quantization clusters. The paper claims that the rendered networks are highly compressible in terms of file size, up to 103 compared to the full-precision unquantized DNN model. The approach was evaluated on different types of models and datasets and compared with previous work."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/EasyQuant%3A_Post-training_quantization_via_scale_optimization/","title":"EasyQuant: Post-training quantization via scale optimization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/EasyQuant%3A_Post-training_quantization_via_scale_optimization/#summary","title":"Summary","text":"<p> The paper presents a post-training quantization method called EasyQuant which optimizes scales of weights and activations for all layers to achieve high quantization precision, and then reduces the bit width for both weights and activations to INT7. INT16 intermediate storage and integer Winograd convolution implementation are also used to accelerate inference. Experimental results show that EQ outperforms TensorRT and can achieve near INT8 accuracy in 7 bits width post-training. The method can be used for deployment on computation-constrained devices without suffering from the cumbersome training process or accuracy drop."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/EasyQuant%3A_Post-training_quantization_via_scale_optimization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/EasyQuant%3A_Post-training_quantization_via_scale_optimization/#content","title":"Content","text":"<p>In this paper, the authors present a post-training quantization method called EasyQuant (EQ) that optimizes scales of weights and activations for all layers, targets convolutional outputs to obtain high quantization precision, and then lowers down bit width to INT7 for both weights and activations. They also adopt INT16 intermediate storage and integer Winograd convolution implementation to accelerate inference. The experimental results show that EQ outperforms the TensorRT method and can achieve near INT8 accuracy in 7 bits width post-training. The authors argue that this method can be used for deployment on computation-constrained devices without suffering from the cumbersome training process of training-based methods or accuracy drop in post-training quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ebsp%3A_evolving_bit_sparsity_patterns_for_hardware-friendly_inference_of_quantized_deep_neural_networks/","title":"Ebsp: evolving bit sparsity patterns for hardware-friendly inference of quantized deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ebsp%3A_evolving_bit_sparsity_patterns_for_hardware-friendly_inference_of_quantized_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper introduces EBSP, a novel technique for compressing DNN models for hardware-friendly inference on edge-computing platforms. EBSP integrates aggressive joint-way compression with hardware design by introducing bit sparsity patterns that construct both expressive and regular bit distribution in the quantized network. It overall aims to optimize the precision of operands in quantization, resulting in energy reduction and performance gain. The paper outlines the limitations of existing compression techniques and highlights how the proposal overcomes them.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ebsp%3A_evolving_bit_sparsity_patterns_for_hardware-friendly_inference_of_quantized_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ebsp%3A_evolving_bit_sparsity_patterns_for_hardware-friendly_inference_of_quantized_deep_neural_networks/#content","title":"Content","text":"<p> This paper proposes a novel technique, EBSP, for the compression of DNN models, specifically for hardware-friendly inference on edge-computing platforms. It aims to integrate aggressive joint-way compression with hardware design by introducing bit sparsity patterns that construct both highly expressive and inherently regular bit distribution in the quantized network. The paper further incorporates sparsity constraint in training to evolve inherently bit distributions to the sparsity pattern. Using EBSP, a quantized network constrained by bit sparsity pattern can be processed using LUTs with the fewest entries instead of multipliers in minimally modified computational hardware, resulting in energy reduction and performance gain. The paper outlines the challenges with existing compression techniques and proposes a new angle of bit-level sparsity to optimize precision of operands in quantization. The contributions of the paper are outlined in detail."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Edge_inference_with_fully_differentiable_quantized_mixed_precision_neural_networks/","title":"Edge inference with fully differentiable quantized mixed precision neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Edge_inference_with_fully_differentiable_quantized_mixed_precision_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper proposes a new approach for mixed precision CNNs' quantization targeting edge-computing, employing hardware-aware heterogeneous differentiable quantization, targeted gradient modification, and a multi-phase learning schedule. The proposed method establishes a new pareto frontier in model accuracy and memory footprint, delivering best-in-class accuracy below 4.3 MB of weights and activations, demonstrating the effectiveness of these techniques on the ImageNet dataset across a range of models, including EfficientNet-Lite0 and MobileNetV2 models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Edge_inference_with_fully_differentiable_quantized_mixed_precision_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Edge_inference_with_fully_differentiable_quantized_mixed_precision_neural_networks/#content","title":"Content","text":"<p>The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques encompassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new pareto frontier in model accuracy and memory footprint demonstrating a range of quantized models, delivering best-in-class accuracy below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions are: (i) hardware-aware heterogeneous differentiable quantization with tensor-sliced learned precision, (ii) targeted gradient modification for wgts. and acts. to mitigate quantization errors, and (iii) a multi-phase learning schedule to address instability in learning arising from updates to the learned quantizer and model parameters. We demonstrate the effectiveness of our techniques on the ImageNet dataset across a range of models including EfficientNet-Lite0 (e.g., 4.14MB of wgts. and acts. at 67.66% accuracy) and MobileNetV2 (e.g., 3.51MB wgts. and acts. at 65.39% accuracy)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Effective_training_of_convolutional_neural_networks_with_low-bitwidth_weights_and_activations/","title":"Effective training of convolutional neural networks with low-bitwidth weights and activations","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Effective_training_of_convolutional_neural_networks_with_low-bitwidth_weights_and_activations/#summary","title":"Summary","text":"<p>This paper introduces three methods for training a convolutional neural network with low-bitwidth weights and activations. These methods include progressive quantization, stochastic precision, and joint knowledge distillation, which are effective in various experiments including CIFAR-100 and ImageNet datasets. The progressive quantization method gradually decreases the bit-width from high to low precision during training, while stochastic precision randomly quantizes sub-networks. Finally, joint knowledge distillation trains a full-precision model alongside the low-precision model for better training guidance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Effective_training_of_convolutional_neural_networks_with_low-bitwidth_weights_and_activations/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Effective_training_of_convolutional_neural_networks_with_low-bitwidth_weights_and_activations/#content","title":"Content","text":"<p> This paper proposes three approaches for training a deep convolutional neural network with low-bitwidth weights and activations: progressive quantization, stochastic precision, and joint knowledge distillation. The progressive quantization method involves two schemes for finding good local minima by optimizing a network with quantized weights first and then quantizing activations, and gradually decreasing the bit-width from high-precision to low-precision during training. To alleviate the excessive training burden, stochastic precision randomly samples and quantizes sub-networks while keeping other parts in full-precision. And, joint knowledge distillation trains a full-precision model alongside the low-precision one to provide hints for the low-precision model training. The proposed methods show effectiveness in various experiments on datasets such as CIFAR-100 and ImageNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_Activation_Quantization_via_Adaptive_Rounding_Border_for_Post-Training_Quantization/","title":"Efficient Activation Quantization via Adaptive Rounding Border for Post-Training Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_Activation_Quantization_via_Adaptive_Rounding_Border_for_Post-Training_Quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes AQuant, a framework that replaces the constant border in post-training quantization (PTQ) with a border function to minimize error in multiplying two numbers and eliminate bias in expected value, resulting in improved model accuracy. AQuant can automatically learn the border function and optimize errors, achieving better results than previous works on ResNet-18 under 2-bit weight and activation PTQ.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_Activation_Quantization_via_Adaptive_Rounding_Border_for_Post-Training_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_Activation_Quantization_via_Adaptive_Rounding_Border_for_Post-Training_Quantization/#content","title":"Content","text":"<p>Post-training quantization (PTQ) is a popular technique for deploying deep neural networks (DNNs) on resource-limited devices. Rounding is the primary source of quantization error in PTQ, for which the traditional rounding-to-nearest scheme is used. However, this work shows that optimizing rounding schemes can improve the model accuracy. To deal with this, the authors propose replacing the constant border with a simple border function that can obtain the minimal error for multiplying two numbers and eliminate the bias of its expected value, further benefiting model accuracy. Based on this, they approximate the border function to make the incurred overhead negligible and optimize propagated errors and global errors. They propose their AQuant framework, which can learn the border function automatically, achieving noticeable improvements compared to state-of-the-art works and pushing the accuracy of ResNet-18 up to 60.31% under the 2-bit weight and activation post-training quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_CNN-LSTM_based_image_captioning_using_neural_network_compression/","title":"Efficient CNN-LSTM based image captioning using neural network compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_CNN-LSTM_based_image_captioning_using_neural_network_compression/#summary","title":"Summary","text":"<p> The paper presents an efficient CNN-LSTM based Image Captioning model compression pipeline that reduces the model size by 73.1%, inference time by 71.3%, and improves BLEU score by 7.7% for a dataset with VGG16 or ResNet50 as an encoder and an LSTM decoder on the flickr8k dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_CNN-LSTM_based_image_captioning_using_neural_network_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_CNN-LSTM_based_image_captioning_using_neural_network_compression/#content","title":"Content","text":"<p>In this research paper, the authors present an efficient CNN-LSTM based Image Captioning model compression pipeline using neural network compression techniques. The pipeline is tested on a dataset with VGG16 or ResNet50 as an encoder and an LSTM decoder on the flickr8k dataset. The compression architecture reduces the model size by 73.1%, the inference time by 71.3%, and improves the BLEU score by 7.7% as compared to its uncompressed counterpart."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_and_accurate_quantized_image_super-resolution_on_mobile_NPUs%2C_mobile_AI_%26_AIM_2022_challenge%3A_report/","title":"Efficient and accurate quantized image super-resolution on mobile NPUs, mobile AI &amp; AIM 2022 challenge: report","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_and_accurate_quantized_image_super-resolution_on_mobile_NPUs%2C_mobile_AI_%26_AIM_2022_challenge%3A_report/#summary","title":"Summary","text":"<p>Summary: The paper discusses the Mobile AI challenge that focuses on designing an efficient quantized image super-resolution solution compatible with low-power mobile NPUs. The participants trained INT8 models on the DIV2K dataset and evaluated their models on the Synaptics VS680 Smart Home board with a dedicated edge NPU capable of accelerating quantized neural networks. All proposed solutions in the challenge are fully compatible with the NPU and can reconstruct Full HD resolution images at up to 60 FPS rate. A detailed description of all models is provided in the paper.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_and_accurate_quantized_image_super-resolution_on_mobile_NPUs%2C_mobile_AI_%26_AIM_2022_challenge%3A_report/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_and_accurate_quantized_image_super-resolution_on_mobile_NPUs%2C_mobile_AI_%26_AIM_2022_challenge%3A_report/#content","title":"Content","text":"<p> Image super-resolution is a common task on mobile and IoT devices, where one often needs to upscale and enhance low-resolution images and video frames. While numerous solutions have been proposed for this problem in the past, they are usually not compatible with low-power mobile NPUs having many computational and memory constraints. In this Mobile AI challenge, we address this problem and propose the participants to design an efficient quantized image super-resolution solution that can demonstrate a real-time performance on mobile NPUs. The participants were provided with the DIV2K dataset and trained INT8 models to do a high-quality 3X image upscaling. The runtime of all models was evaluated on the Synaptics VS680 Smart Home board with a dedicated edge NPU capable of accelerating quantized neural networks. All proposed solutions are fully compatible with the above NPU, demonstrating an up to 60 FPS rate when reconstructing Full HD resolution images. A detailed description of all models developed in the challenge is provided in this paper."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_convolutional_neural_network_with_binary_quantization_layer/","title":"Efficient convolutional neural network with binary quantization layer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_convolutional_neural_network_with_binary_quantization_layer/#summary","title":"Summary","text":"<p>This paper proposes a novel method for segmentation that leverages CNN features to generate visually and semantically coherent image segments. Binary encoding of CNN features is used to overcome the difficulty of clustering on high-dimensional feature spaces. The binary encodings can be embedded into the CNN as an extra layer for real-time segmentation. The proposed method is the first attempt at general semantic image segmentation using CNN and outperforms state-of-the-art non-semantic methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_convolutional_neural_network_with_binary_quantization_layer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_convolutional_neural_network_with_binary_quantization_layer/#content","title":"Content","text":"<p>In this paper, the authors introduce a novel method for segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). They propose visually and semantically coherent image segments, using binary encoding of CNN features to overcome the difficulty of clustering on the high-dimensional CNN feature space. These binary encodings can be embedded into the CNN as an extra layer at the end of the network, resulting in real-time segmentation. The authors claim that their method is the first attempt on general semantic image segmentation using CNN as opposed to being limited to a few number of categories of images. Experiments show that their segmentation algorithm outperforms the state-of-the-art non-semantic segmentation methods by large margin."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_execution_of_quantized_deep_learning_models%3A_A_compiler_approach/","title":"Efficient execution of quantized deep learning models: A compiler approach","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_execution_of_quantized_deep_learning_models%3A_A_compiler_approach/#summary","title":"Summary","text":"<p> This paper proposes a compiler approach to execute pre-quantized models from deep learning frameworks effectively on different hardware platforms by introducing a Quantized Neural Network dialect to the compiler representation. By using QNN with the compilation of pre-quantized models, the proposed compiler approach achieves significant speedups on various hardware platforms, making it possible to achieve model execution performance comparable to state-of-the-art framework-specific solutions but on a broader range of hardware platforms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_execution_of_quantized_deep_learning_models%3A_A_compiler_approach/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_execution_of_quantized_deep_learning_models%3A_A_compiler_approach/#content","title":"Content","text":"<p>Abstract: A compiler approach is proposed in this paper to tackle the challenges of executing pre-quantized models from deep learning frameworks on a variety of hardware platforms. We introduce a Quantized Neural Network (QNN) dialect to the compiler representation to optimize pre-quantized INT8 models. The QNN-augmented deep learning compiler achieves speedups of up to 2.35x, 2.15x, 1.35x, and 1.40x on various hardware platforms including Intel Xeon Cascade Lake CPUs, Nvidia Tesla T4 GPUs, ARM Cortex-A CPUs on Raspberry Pi3 and Pi4, respectively. Using QNN with the compilation of pre-quantized models enables developers to achieve model execution performance comparable to state-of-the-art framework-specific solutions but on a wider range of hardware platforms.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_quantization_for_neural_networks_with_binary_weights_and_low_bitwidth_activations/","title":"Efficient quantization for neural networks with binary weights and low bitwidth activations","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_quantization_for_neural_networks_with_binary_weights_and_low_bitwidth_activations/#summary","title":"Summary","text":"<p>Summary: This paper proposes a novel quantization strategy for deep neural networks by applying distinct quantization methods to weights and activations. They introduce CReLU, an activation function, and develop a quantization strategy that approximates weights with binary values and quantizes activations using linear or logarithmic quantizer. The proposed quantized model with binary weights and ultra-low bitwidth activations surpasses previous models and achieves significant theoretical speedup with ResNet-18. CReLU is demonstrated to be effective and robust through experiments and theoretical analysis.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_quantization_for_neural_networks_with_binary_weights_and_low_bitwidth_activations/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Efficient_quantization_for_neural_networks_with_binary_weights_and_low_bitwidth_activations/#content","title":"Content","text":"<p> In this paper, the authors propose a new quantization strategy for deep neural networks which involves applying different quantization methods to weights and activations, respectively. They introduce a new activation function called CReLU and develop a specific quantization strategy which involves formulating the forward and backward approximation of weights with binary values and quantizing activations to low bitwdth using linear or logarithmic quantizer. The authors show, for the first time, that their final quantized model with binary weights and ultra-low bitwidth activations outperforms the previous best models by large margins on ImageNet, as well as achieving nearly a 10:85 theoretical speedup with ResNet-18. The effectiveness and robustness of CReLU are demonstrated through ablation experiments and theoretical analysis in comparison with other activation functions."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/End-to-end_supervised_product_quantization_for_image_search_and_retrieval/","title":"End-to-end supervised product quantization for image search and retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/End-to-end_supervised_product_quantization_for_image_search_and_retrieval/#summary","title":"Summary","text":"<p>This paper presents Deep Product Quantization (DPQ), which is a supervised dictionary-free hashing method used for retrieval and classification tasks. DPQ achieves higher accuracy retrieval and classification with similar computational complexity and memory footprint as the Product Quantization method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/End-to-end_supervised_product_quantization_for_image_search_and_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/End-to-end_supervised_product_quantization_for_image_search_and_retrieval/#content","title":"Content","text":"<p>Product Quantization is a leading hashing technique used in the absence of labels, which constructs look-up tables approximating the features. However, the research community has shifted towards supervised dictionary-free methods that compute Hamming distances on binary representations. This paper presents Deep Product Quantization (DPQ), a technique that learns a dictionary-based supervised representation inspired by PQ, which achieves higher accuracy retrieval and classification than the latest state-of-the-art methods. DPQ learns both soft and hard representations, and obtains state-of-the-art results on various retrieval and classification experiments while maintaining similar computational complexity and memory footprint as the PQ method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Eva%3A_Exploring_the_limits_of_masked_visual_representation_learning_at_scale/","title":"Eva: Exploring the limits of masked visual representation learning at scale","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Eva%3A_Exploring_the_limits_of_masked_visual_representation_learning_at_scale/#summary","title":"Summary","text":"<p>Summary: The paper introduces EVA, a ViT-based model pre-trained to reconstruct masked out image-text aligned vision features to scale up to one billion parameters. It is capable of achieving state-of-the-art performance on various vision-related downstream tasks without extensive supervised training. The authors also demonstrate that changes in scaling EVA can result in qualitative changes in transfer learning performance. The paper includes the release of the code and its billion-scale models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Eva%3A_Exploring_the_limits_of_masked_visual_representation_learning_at_scale/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Eva%3A_Exploring_the_limits_of_masked_visual_representation_learning_at_scale/#content","title":"Content","text":"<p> We launch EVA, a vanilla ViT-based model that explores the limits of visual representation by using only publicly accessible data. EVA is pre-trained to reconstruct the masked out image-text aligned vision features, conditioned on visible image patches. Via this pretext task, we can efficiently scale up EVA to one billion parameters and set new records on a broad range of representative vision downstream tasks such as image recognition, video action recognition, object detection, instance segmentation, and semantic segmentation, without heavy supervised training. EVA can also serve as a vision-centric, multi-modal pivot to connect images and text. We observe quantitative changes in scaling EVA result in qualitative changes in transfer learning performance that are not present in other models. To facilitate future research, we release all the code and billion-scale models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploiting_bi-directional_channel_reciprocity_in_deep_learning_for_low_rate_massive_MIMO_CSI_feedback/","title":"Exploiting bi-directional channel reciprocity in deep learning for low rate massive MIMO CSI feedback","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploiting_bi-directional_channel_reciprocity_in_deep_learning_for_low_rate_massive_MIMO_CSI_feedback/#summary","title":"Summary","text":"<p> The paper proposes a learning-based CSI feedback framework for MIMO wireless systems that uses limited feedback and bi-directional reciprocal channel characteristics. The authors propose two deep learning architectures, called DualNet-MAG and DualNet-ABS, which significantly reduce the CSI feedback payload. The results of experiments demonstrate that these architectures demonstrate improved performance compared to the downlink-based architecture."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploiting_bi-directional_channel_reciprocity_in_deep_learning_for_low_rate_massive_MIMO_CSI_feedback/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploiting_bi-directional_channel_reciprocity_in_deep_learning_for_low_rate_massive_MIMO_CSI_feedback/#content","title":"Content","text":"<p>Channel state information (CSI) feedback is important for multiple-input multiple-output (MIMO) wireless systems to achieve their capacity gain in frequency division duplex (FDD) mode. This work proposes a learning-based CSI feedback framework based on limited feedback and bi-directional reciprocal channel characteristics. We propose two deep learning architectures, DualNet-MAG and DualNet-ABS, to significantly reduce the CSI feedback payload based on the multipath reciprocity. Experiment results demonstrate that our architectures bring an obvious improvement compared with the downlink-based architecture."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploring_accumulated_gradient-based_quantization_and_compression_for_deep_neural_networks/","title":"Exploring accumulated gradient-based quantization and compression for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploring_accumulated_gradient-based_quantization_and_compression_for_deep_neural_networks/#summary","title":"Summary","text":"<p> This paper highlights the issues faced by deep neural networks (DNNs) in terms of memory and storage requirements. To address these issues, researchers proposed quantization and pruning techniques. This paper explores the combination of two techniques, accumulated gradient-based quantization, and compression. It shows that combining these techniques can lead to reduced model size and computational requirements without compromising accuracy. The approach was evaluated on two benchmark datasets, indicating improved results compared to state-of-the-art methods, including up to 23x reduction in model size and up to 16x improvements in inference time speedups."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploring_accumulated_gradient-based_quantization_and_compression_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploring_accumulated_gradient-based_quantization_and_compression_for_deep_neural_networks/#content","title":"Content","text":"<p>Deep Neural Networks (DNNs) have shown significant advancements in various sectors, including computer vision, natural language processing, speech recognition, and many others. However, these networks are computationally intensive, requiring significant memory and storage resources. To address these challenges, researchers have proposed various quantization and pruning techniques to reduce the compute and storage requirements of DNNs. This thesis explores the combination of two such techniques: accumulated gradient-based quantization and compression. We show that these two techniques can be combined to achieve significant reductions in model size and computational requirements with little to no loss in accuracy. We evaluate this approach on two benchmark datasets and show that it achieves results comparable to state-of-the-art methods while reducing model size up to 23x and achieving inference time speedups up to 16x."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploring_neural_networks_quantization_via_layer-wise_quantization_analysis/","title":"Exploring neural networks quantization via layer-wise quantization analysis","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploring_neural_networks_quantization_via_layer-wise_quantization_analysis/#summary","title":"Summary","text":"<p>Summary: This paper presents an analytic framework for quantization which breaks down overall degradation to its per-layer contributions, revealing the intrinsic and extrinsic factors that determine a layer's contribution to degradation. Layer-wise analysis allows for a more nuanced examination of how quantization affects the network, and identifies local fail-cases not reflected when inspecting overall performance. The authors demonstrate how the approach can be used to design better performing quantization schemes, using an example of state-of-the-art post-training quantization methods performing poorly on a single layer of ResNext26.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploring_neural_networks_quantization_via_layer-wise_quantization_analysis/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Exploring_neural_networks_quantization_via_layer-wise_quantization_analysis/#content","title":"Content","text":"<p>Quantization is a crucial step in deploying deep learning models, but fails cases resulting in excessive degradation are not addressed in current literature. In this paper, the authors present an analytic framework that breaks down overall degradation to its per-layer contributions. They observe that a layer's contribution is determined by intrinsic (local) factors, such as the distribution of the layer's weights and activations, and extrinsic (global) factors, such as interaction with the rest of the layers. Layer-wise analysis of existing quantization schemes reveals local fail-cases not reflected when inspecting their overall performance. The authors also present an example of ResNext26 on which state-of-the-art post-training quantization methods perform poorly, and show that almost all of the degradation stems from a single layer. Layer-wise analysis allows for a more nuanced examination of how quantization affects the network, enabling the design of better performing schemes."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Extremely_lightweight_quantization_robust_real-time_single-image_super_resolution_for_mobile_devices/","title":"Extremely lightweight quantization robust real-time single-image super resolution for mobile devices","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Extremely_lightweight_quantization_robust_real-time_single-image_super_resolution_for_mobile_devices/#summary","title":"Summary","text":"<p>Summary: This paper proposes a lightweight quantization robust real-time super resolution network (XLSR) that is hardware limitation aware and achieves state-of-the-art results in Single-Image Super Resolution (SISR) using deep learning methodologies. The network overcomes the limitations of previous state-of-the-art methods that have millions of parameters and layers, making it more practical for real-world applications.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Extremely_lightweight_quantization_robust_real-time_single-image_super_resolution_for_mobile_devices/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Extremely_lightweight_quantization_robust_real-time_single-image_super_resolution_for_mobile_devices/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Extremely_low-bit_convolution_optimization_for_quantized_neural_network_on_modern_computer_architectures/","title":"Extremely low-bit convolution optimization for quantized neural network on modern computer architectures","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Extremely_low-bit_convolution_optimization_for_quantized_neural_network_on_modern_computer_architectures/#summary","title":"Summary","text":"<p>Summary: The paper discusses the effectiveness of quantization in reducing the size of deep neural networks without significant loss in accuracy. The authors explore optimization methods for performing extremely low-bit convolutions on diverse architectures such as ARM CPU (2-8 bits) and NVIDIA GPU (4-8 bits). They propose instruction schemes, data padding and packing optimizations, winograd algorithm, data partition mechanism and quantization fusion to achieve higher performance. The results of their implementations perform better than other state-of-the-art frameworks and libraries. This is the first work that efficiently implements extremely low-bit convolutions on ARM CPU and NVIDIA GPU.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Extremely_low-bit_convolution_optimization_for_quantized_neural_network_on_modern_computer_architectures/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Extremely_low-bit_convolution_optimization_for_quantized_neural_network_on_modern_computer_architectures/#content","title":"Content","text":"<p>With the continuous demand for higher accuracy of deep neural networks, the model size has increased significantly. Quantization is one of the most widely used model compression methods, which can effectively reduce the model size without severe accuracy loss. Modern processors such as ARM CPU and NVIDIA GPU have already provided the support of low-bit arithmetic instructions. However, there lack efficient and practical optimizations for convolution computation towards extremely low-bit on ARM CPU (e.g., 2 \u223c8-bit) and NVIDIA GPU (e.g., 4-bit and 8-bit). This paper explores the performance optimization methods of extremely low-bit convolution on diverse architectures. On ARM CPU, we propose two instruction schemes for 2\u223c3-bit and 4\u223c8-bit convolution with corresponding register allocation methods. In addition, we re-design the GEMM computation with data padding and packing optimizations. We also implement winograd algorithm for convolution with some specific bit width (e.g., 4\u223c6-bit) to achieve higher performance. On NVIDIA GPU, we propose a data partition mechanism and multi-level memory access optimizations, to better adapt the computation to GPU thread and memory hierarchy. We also propose quantization fusion to eliminate unnecessary data access. The experiment results demonstrate our implementations achieve better performance of extremely low-bit convolution compared to the state-of-the-art frameworks and libraries such as ncnn and cuDNN. To the best of our knowledge, this is the first work that provides efficient implementations of extremely low-bit convolutions covering 2 \u223c8-bit on ARM CPU and 4-bit/8-bit on NVIDIA GPU."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/F8net%3A_Fixed-point_8-bit_only_multiplication_for_network_quantization/","title":"F8net: Fixed-point 8-bit only multiplication for network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/F8net%3A_Fixed-point_8-bit_only_multiplication_for_network_quantization/#summary","title":"Summary","text":"<p> F8Net is a novel quantization framework that uses only fixed-point 8-bit multiplication to reduce the performance gap between quantized and full-precision models. The framework applies different fixed-point formats for weights and activations of different layers with a novel algorithm to automatically determine the right format for each layer during training. The approach achieves state-of-the-art performance and is verified on ImageNet for MobileNet V1/V2 and ResNet18/50 models compared to existing quantization techniques with INT32 multiplication or floating-point arithmetic or full-precision counterparts."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/F8net%3A_Fixed-point_8-bit_only_multiplication_for_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/F8net%3A_Fixed-point_8-bit_only_multiplication_for_network_quantization/#content","title":"Content","text":"<p>Neural network quantization is a promising compression technique to reduce memory footprint and save energy consumption, potentially leading to real-time inference. However, there is a performance gap between quantized and full-precision models. To reduce it, existing quantization approaches require high-precision INT32 or full-precision multiplication during inference for scaling or dequantization. This introduces a noticeable cost in terms of memory, speed, and required energy. To tackle these issues, we present F8Net, a novel quantization framework consisting of only \ufb01xed-point 8-bit multiplication. To derive our method, we \ufb01rst discuss the advantages of \ufb01xed-point multiplication with different formats of \ufb01xed-point numbers and study the statistical behavior of the associated \ufb01xed-point numbers. Second, based on the statistical and algorithmic analysis, we apply different \ufb01xed-point formats for weights and activations of different layers. We introduce a novel algorithm to automatically determine the right format for each layer during training. Third, we analyze a previous quantization algorithm\u2014parameterized clipping activation (PACT)\u2014and reformulate it using \ufb01xed-point arithmetic. Finally, we unify the recently proposed method for quantization \ufb01ne-tuning and our \ufb01xed-point approach to show the potential of our method. We verify F8Net on ImageNet for MobileNet V1/V2 and ResNet18/50. Our approach achieves comparable and better performance, when compared not only to existing quantization techniques with INT32 multiplication or \ufb02oating-point arithmetic, but also to the full-precision counterparts, achieving state-of-the-art performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FAID_diversity_via_neural_networks/","title":"FAID diversity via neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FAID_diversity_via_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new method to design the decoder diversity of finite alphabet iterative decoders (FAIDs) for Low-Density Parity Check (LDPC) codes over the binary symmetric channel (BSC) using recurrent quantized neural networks (RQNNs) to learn/design FAIDs. The RQNN-aided decoder diversity helps to increase the error correction capability of LDPC codes and lowers the error floor. The training sets are constructed by sampling from the set of most problematic error patterns-trapping sets. The paper uses a frame-error-rate (FER) based loss function to train the RQNN with the objective of correcting specific error patterns rather than reducing the bit error rate (BER). The proposed method is proven to surpass the performance of a man-made decoder of the same level of complexity.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FAID_diversity_via_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FAID_diversity_via_neural_networks/#content","title":"Content","text":"<p>Abstract: \u2014Decoder diversity is a powerful error correction framework in which a collection of decoders collaboratively correct a set of error patterns otherwise uncorrectable by any individual decoder. In this paper, we propose a new approach to design the decoder diversity of \ufb01nite alphabet iterative decoders (FAIDs) for Low-Density Parity Check (LDPC) codes over the binary symmetric channel (BSC), for the purpose of lowering the error \ufb02oor while guaranteeing the waterfall performance. The proposed decoder diversity is achieved by training a recurrent quantized neural network (RQNN) to learn/design FAIDs. We demonstrated for the \ufb01rst time that a machine-learned decoder can surpass in performance a man-made decoder of the same complexity. As RQNNs can model a broad class of FAIDs, they are capable of learning an arbitrary FAID. To provide suf\ufb01cient knowledge of the error \ufb02oor to the RQNN, the training sets are constructed by sampling from the set of most problematic error patterns - trapping sets. In contrast to the existing methods that use the cross-entropy function as the loss function, we introduce a frame-error-rate (FER) based loss function to train the RQNN with the objective of correcting speci\ufb01c error patterns rather than reducing the bit error rate (BER). The examples and simulation results show that the RQNN-aided decoder diversity increases the error correction capability of LDPC codes and lowers the error \ufb02oor.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FAT%3A_Training_neural_networks_for_reliable_inference_under_hardware_faults/","title":"FAT: Training neural networks for reliable inference under hardware faults","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FAT%3A_Training_neural_networks_for_reliable_inference_under_hardware_faults/#summary","title":"Summary","text":"<p> The paper proposes a methodology called fault-aware training (FAT) that incorporates error modelling into neural network training to make quantized neural networks (QNNs) resilient to specific fault models on devices. The methodology is effective in improving the error tolerance of QNNs, making them more accurate and resilient, and enabling redundant systems that achieve higher worst-case accuracy at lower hardware costs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FAT%3A_Training_neural_networks_for_reliable_inference_under_hardware_faults/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FAT%3A_Training_neural_networks_for_reliable_inference_under_hardware_faults/#content","title":"Content","text":"<p>Deep neural networks are being increasingly deployed on embedded systems because of their high accuracy, despite the large compute and memory requirements. To reduce the hardware cost of using these networks in safety-critical environments, researchers have been exploring domain-specific solutions. This work proposes a novel methodology, called fault-aware training (FAT), which includes error modeling during neural network training to make quantized neural networks (QNNs) resilient to specific fault models on the device. The experiments show that highly accurate convolutional neural networks (CNNs) can be trained with FAT, which exhibit much better error tolerance compared to the original. Additionally, redundant systems that are built from QNNs trained with FAT achieve higher worse-case accuracy at lower hardware costs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FILM-QNN%3A_Efficient_FPGA_acceleration_of_deep_neural_networks_with_intra-layer%2C_mixed-precision_quantization/","title":"FILM-QNN: Efficient FPGA acceleration of deep neural networks with intra-layer, mixed-precision quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FILM-QNN%3A_Efficient_FPGA_acceleration_of_deep_neural_networks_with_intra-layer%2C_mixed-precision_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a framework called FILM-QNN for quantization and acceleration of multiple DNN models across different embedded FPGA devices. It includes a novel intra-layer mixed-precision quantization algorithm that assigns different precisions to filters of each layer to improve overall throughput and hardware parallelism. Optimization techniques and a resource model for the FPGA accelerator architecture are also applied. Results show that mixed-precision implementations can achieve comparable accuracy as 8-bit versions and comparable throughput as 4-bit designs.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FILM-QNN%3A_Efficient_FPGA_acceleration_of_deep_neural_networks_with_intra-layer%2C_mixed-precision_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FILM-QNN%3A_Efficient_FPGA_acceleration_of_deep_neural_networks_with_intra-layer%2C_mixed-precision_quantization/#content","title":"Content","text":"<p> With the trend of deploying DNN models on edge devices with limited resources, quantization techniques have been widely used to reduce on-chip storage and improve computation throughput. However, existing quantization work below 8-bit may lead to accuracy loss or a big gap between theoretical improvement of computation throughput and the practical inference speedup. In this work, a general framework called FILM-QNN is proposed to quantize and accelerate multiple DNN models across different embedded FPGA devices. Specifically, a novel intra-layer, mixed-precision quantization algorithm is proposed which assigns different precisions onto the filters of each layer while preserving accuracy and improving hardware parallelism. To enhance overall throughput, multiple optimization techniques for the FPGA accelerator architecture are applied, and a comprehensive resource model is developed. Experimental results of ResNet-18, ResNet-50, and MobileNet-V2 demonstrate that implementations with intra-layer, mixed-precision can achieve comparable accuracy as 8-bit versions and comparable throughput as 4-bit designs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FINN-L%3A_Library_extensions_and_design_trade-off_analysis_for_variable_precision_LSTM_networks_on_FPGAs/","title":"FINN-L: Library extensions and design trade-off analysis for variable precision LSTM networks on FPGAs","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FINN-L%3A_Library_extensions_and_design_trade-off_analysis_for_variable_precision_LSTM_networks_on_FPGAs/#summary","title":"Summary","text":"<p>Please provide the full text of the paper or identifiable sections of the paper containing the abstract so that I can better assist you in extracting the abstract section.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FINN-L%3A_Library_extensions_and_design_trade-off_analysis_for_variable_precision_LSTM_networks_on_FPGAs/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FINN-L%3A_Library_extensions_and_design_trade-off_analysis_for_variable_precision_LSTM_networks_on_FPGAs/#content","title":"Content","text":"<p>Unfortunately, the provided text does not contain the abstract of the research paper. It only includes the IEEE copyright notice and publication details. Please provide the full text of the paper or identifiable sections of the paper containing the abstract so that I can better assist you in extracting the abstract section."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FINN-R_An_End-to-End_Deep-Learning_Framework_for_Fast_Exploration_of_Quantized_Neural_Networks/","title":"FINN-R An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FINN-R_An_End-to-End_Deep-Learning_Framework_for_Fast_Exploration_of_Quantized_Neural_Networks/#summary","title":"Summary","text":"<p> This paper introduces the second generation of the FINN framework that is designed to automatically create customized inference engines for FPGAs. With reduced-precision representations, this framework optimizes neural networks for specific design targets and platforms to achieve numerical accuracy for various applications, resulting in unprecedented throughput."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FINN-R_An_End-to-End_Deep-Learning_Framework_for_Fast_Exploration_of_Quantized_Neural_Networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FINN-R_An_End-to-End_Deep-Learning_Framework_for_Fast_Exploration_of_Quantized_Neural_Networks/#content","title":"Content","text":"<p>Convolutional Neural Networks are highly successful in enabling machine vision and intelligent decision-making. However, the computational and memory requirements of these networks are challenging, and reduced-precision representations have been proposed as a promising solution. In this paper, we describe the second generation of the FINN framework, an end-to-end tool designed to automatically create customized inference engines for FPGAs. Using the tool, we demonstrate the optimization of reduced-precision neural networks on a range of platforms, including embedded devices, and achieve unprecedented throughput. The tool optimizes for specific design targets, platforms, and precision to achieve the required numerical accuracy for a given application."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FP-BNN%3A_Binarized_neural_network_on_FPGA/","title":"FP-BNN: Binarized neural network on FPGA","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FP-BNN%3A_Binarized_neural_network_on_FPGA/#summary","title":"Summary","text":"<p>The paper presents an optimized binarized neural network (BNN) for FPGAs called FP-BNN. The authors use a Resource-Aware Model Analysis (RAMA) method to improve hardware consumption and maintain accuracy. They remove bottlenecks by using bit-level XNOR and shifting operations, as well as data quantization and on-chip storage optimization. The FP-BNN designs showed a Tera operations per second inference performance with acceptable accuracy for MNIST MLP, Cifar-10 ConvNet, and AlexNet on a Stratix-V FPGA system."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FP-BNN%3A_Binarized_neural_network_on_FPGA/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FP-BNN%3A_Binarized_neural_network_on_FPGA/#content","title":"Content","text":"<p> This paper presents FP-BNN, a binarized neural network (BNN) for FPGAs, which significantly reduces hardware consumption while maintaining acceptable accuracy. The authors introduce a Resource-Aware Model Analysis (RAMA) method and remove bottlenecks involving multipliers by bit-level XNOR and shifting operations, as well as the bottleneck of parameter access by data quantization and optimized on-chip storage. They evaluate the FP-BNN accelerator designs for MNIST multi-layer perceptrons (MLP), Cifar-10 ConvNet, and AlexNet on a Stratix-V FPGA system. An inference performance of Tera operations per second with acceptable accuracy loss is obtained."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Face_behavior_a_la_carte%3A_Expressions%2C_affect_and_action_units_in_a_single_network/","title":"Face behavior a la carte: Expressions, affect and action units in a single network","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Face_behavior_a_la_carte%3A_Expressions%2C_affect_and_action_units_in_a_single_network/#summary","title":"Summary","text":"<p>The paper focuses on the study of automatic facial behavior analysis using deep neural networks. The authors present a single holistic framework called FaceBehaviorNet for simultaneously learning three iconic tasks in facial behavior analysis. These tasks include automatic recognition of basic expressions, estimation of continuous affect and detection of facial action units. The authors claim to have conducted the first and largest study of all facial behavior tasks learned jointly using a single task model."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Face_behavior_a_la_carte%3A_Expressions%2C_affect_and_action_units_in_a_single_network/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Face_behavior_a_la_carte%3A_Expressions%2C_affect_and_action_units_in_a_single_network/#content","title":"Content","text":"<p>Automatic facial behavior analysis has a long history of studies in the intersection of computer vision, physiology and psychology. However it is only recently, with the collection of large-scale datasets and powerful machine learning methods such as deep neural networks, that automatic facial behavior analysis started to thrive. Three of its iconic tasks are automatic recognition of basic expressions (e.g. happiness, sadness, surprise), estimation of continuous affect (e.g., valence and arousal), and detection of facial action units (activations of e.g. upper/inner eyebrows, nose wrinkles). Up until now these tasks have been studied independently by collecting a dedicated dataset and training a single-task model. We present the first and the largest study of all facial behaviour tasks learned jointly in a single holistic framework, which we call FaceBehaviorNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fat%3A_Learning_low-bitwidth_parametric_representation_via_frequency-aware_transformation/","title":"Fat: Learning low-bitwidth parametric representation via frequency-aware transformation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fat%3A_Learning_low-bitwidth_parametric_representation_via_frequency-aware_transformation/#summary","title":"Summary","text":"<p>The paper presents a new quantization pipeline called Frequency-Aware Transformation (FAT), which transforms network weights in the frequency domain before quantization, improving the performance of low bitwidth CNNs, without the need for hyper-parameter tuning. FAT is shown to improve both uniform and non-uniform quantizers, can be easily implemented in various CNN architectures, and outperforms recent state-of-the-art models by reducing computations by 54.9% and 45.7% against full-precision models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fat%3A_Learning_low-bitwidth_parametric_representation_via_frequency-aware_transformation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fat%3A_Learning_low-bitwidth_parametric_representation_via_frequency-aware_transformation/#content","title":"Content","text":"<p>Learning convolutional neural networks (CNNs) with low bitwidth is challenging because performance may drop significantly after quantization. In this work, a novel quantization pipeline called Frequency-Aware Transformation (FAT) is presented, which learns to transform network weights in the frequency domain before quantization, making them more amenable to training in low bitwidth. CNNs can be easily trained in low precision using simple standard quantizers without tedious hyper-parameter tuning. Theoretical analysis shows that FAT improves both uniform and non-uniform quantizers. FAT can be easily plugged into many CNN architectures, and it outperforms recent state-of-the-art by reducing 54.9% and 45.7% computations against full-precision models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Feature_map_transform_coding_for_energy-efficient_cnn_inference/","title":"Feature map transform coding for energy-efficient cnn inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Feature_map_transform_coding_for_energy-efficient_cnn_inference/#summary","title":"Summary","text":"<p> The paper proposes a lossy transform coding approach to reduce the memory bandwidth requirements of Convolutional neural networks (CNN), which often dominate the energy footprint on modern hardware. This approach compresses feature maps, which are highly correlated, with variable length coding without requiring fine-tuning the network weights. The proposed method outperforms previous approaches in terms of the number of bits per value with minor accuracy degradation on ResNet-34 and MobileNetV2. The reduction of 40% in the memory energy footprint is achieved in FPGA implementation of ResNet-18 with negligible impact on accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Feature_map_transform_coding_for_energy-efficient_cnn_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Feature_map_transform_coding_for_energy-efficient_cnn_inference/#content","title":"Content","text":"<p>Convolutional neural networks (CNNs) achieve state-of-the-art accuracy in a variety of tasks in computer vision and beyond. One of the major obstacles hindering the ubiquitous use of CNNs for inference on low-power edge devices is their high computational complexity and memory bandwidth requirements. The latter often dominates the energy footprint on modern hardware. In this paper, we introduce a lossy transform coding approach, inspired by image and video compression, designed to reduce the memory bandwidth due to the storage of intermediate activation calculation results. Our method does not require fine-tuning the network weights and halves the data transfer volumes to the main memory by compressing feature maps, which are highly correlated, with variable length coding. Our method outperforms previous approaches in the term of the number of bits per value with minor accuracy degradation on ResNet-34 and MobileNetV2. We analyze the performance of our approach on a variety of CNN architectures and demonstrate that FPGA implementation of ResNet-18 with our approach results in a reduction of around 40% in the memory energy footprint, compared to quantized network, with negligible impact on accuracy. When allowing accuracy degradation of up to 2%, the reduction of 60% is achieved. A reference implementation accompanies the paper."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Few-Shot_Zero-Shot_Learning%3A_Knowledge_Transfer_with_Less_Supervision/","title":"Few-Shot Zero-Shot Learning: Knowledge Transfer with Less Supervision","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Few-Shot_Zero-Shot_Learning%3A_Knowledge_Transfer_with_Less_Supervision/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new setting for zero-shot learning called Few-Shot Zero-Shot Learning (FSZSL), where only a few annotated images are available from each seen class. They introduce a model that uses sparse attribute propagation (SAP) to propagate attribute annotations using sparse coding, followed by learning bidirectional projections between features and attributes for ZSL. The proposed model achieves state-of-the-art results.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Few-Shot_Zero-Shot_Learning%3A_Knowledge_Transfer_with_Less_Supervision/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Few-Shot_Zero-Shot_Learning%3A_Knowledge_Transfer_with_Less_Supervision/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Filter-Wise_Quantization_of_Deep_Neural_Networks_for_IoT_Devices/","title":"Filter-Wise Quantization of Deep Neural Networks for IoT Devices","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Filter-Wise_Quantization_of_Deep_Neural_Networks_for_IoT_Devices/#summary","title":"Summary","text":"<p> The paper proposes a filter-wise quantization technique for on-device machine learning using a two-level network structure and a novel candidate generation algorithm based on differentiable neural architecture search (DNAS). They validate their technique with MobileNetV2 on ImageNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Filter-Wise_Quantization_of_Deep_Neural_Networks_for_IoT_Devices/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Filter-Wise_Quantization_of_Deep_Neural_Networks_for_IoT_Devices/#content","title":"Content","text":"<p> <p>In this paper, the authors propose a filter-wise quantization technique for on-device machine learning at consumer devices, based on the differentiable neural architecture search (DNAS). They use a two-level network structure and a novel candidate generation algorithm to compress the network by allocating different bitwidths to different convolution filters. They validate the effectiveness of their technique with MobileNetV2 on ImageNet.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Finding_the_task-optimal_low-bit_sub-distribution_in_deep_neural_networks/","title":"Finding the task-optimal low-bit sub-distribution in deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Finding_the_task-optimal_low-bit_sub-distribution_in_deep_neural_networks/#summary","title":"Summary","text":"<p>The paper discusses an adaptive-mapping quantization method that uses a concrete Gaussian Mixture to learn an optimal sub-distribution inherent within models. The method helps reduce the memory footprint and computation complexity while improving performance in tasks like image classification and object detection."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Finding_the_task-optimal_low-bit_sub-distribution_in_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Finding_the_task-optimal_low-bit_sub-distribution_in_deep_neural_networks/#content","title":"Content","text":"<p>Quantized neural networks require smaller memory footprints and lower computation complexity, making them efficient for deployment. However, quantization leads to a distribution divergence from the original network, which can degrade performance. This paper presents an adaptive-mapping quantization method to learn an optimal latent sub-distribution inherent within models and smoothly approximated with a concrete Gaussian Mixture. The network weights are projected in compliance with the GM-approximated sub-distribution, which evolves along with the weight update in a co-tuning schema guided by the direct task-objective optimization. The proposed method is demonstrated effective in image classification and object detection over various modern architectures and shows transferability."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fine-grained_data_distribution_alignment_for_post-training_quantization/","title":"Fine-grained data distribution alignment for post-training quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fine-grained_data_distribution_alignment_for_post-training_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method called fine-grained data distribution alignment (FDDA) to enhance the performance of post-training quantization using synthetic data introduced by zero-shot quantization with calibration dataset. The method uses batch normalization statistics (BNS) to align the distribution of synthetic data with the original data by considering inter-class separation and intra-class incohesion of BNS. The proposed method significantly improves the performance of post-training quantization, especially when both the first and last layers are quantized to low-bit. The code is available at https://github.com/zysxmu/FDDA.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fine-grained_data_distribution_alignment_for_post-training_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fine-grained_data_distribution_alignment_for_post-training_quantization/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fitclip%3A_Refining_large-scale_pretrained_image-text_models_for_zero-shot_video_understanding_tasks/","title":"Fitclip: Refining large-scale pretrained image-text models for zero-shot video understanding tasks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fitclip%3A_Refining_large-scale_pretrained_image-text_models_for_zero-shot_video_understanding_tasks/#summary","title":"Summary","text":"<p>Summary: The paper presents a fine-tuning strategy to adapt large-scale pretrained image-text models for zero-shot video understanding tasks, which has shown considerable improvements on two zero-shot Action Recognition tasks and three zero-shot Text-to-video Retrieval tasks. The code is available at https://github.com/bryant1410/fitclip.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fitclip%3A_Refining_large-scale_pretrained_image-text_models_for_zero-shot_video_understanding_tasks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fitclip%3A_Refining_large-scale_pretrained_image-text_models_for_zero-shot_video_understanding_tasks/#content","title":"Content","text":"<p>Large-scale pretrained image-text models have shown incredible zero-shot performance in a handful of tasks, including video ones such as action recognition and text-to-video retrieval. However, these models have not been adapted to video, mainly because they do not account for the time dimension but also because video frames are different from the typical images ( e.g., containing motion blur, less sharpness). In this paper, we present a fine-tuning strategy to refine these large-scale pretrained image-text models for zero-shot video understanding tasks. We show that by carefully adapting these models we obtain considerable improvements on two zero-shot Action Recognition tasks and three zero-shot Text-to-video Retrieval tasks. The code is available at https://github.com/bryant1410/fitclip."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fixed-point_quantization_of_convolutional_neural_networks_for_quantized_inference_on_embedded_platforms/","title":"Fixed-point quantization of convolutional neural networks for quantized inference on embedded platforms","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fixed-point_quantization_of_convolutional_neural_networks_for_quantized_inference_on_embedded_platforms/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method to optimize the quantization of weights, biases, and activations of each layer of a pretrained CNN to allow for deployment on embedded platforms. The method optimizes low bitwidth fixed-point representations for each parameter while controlling the loss in inference accuracy, resulting in a low precision CNN with accuracy losses of less than 1%. The paper finds that layer-wise quantization of parameters significantly helps in the process of quantization.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fixed-point_quantization_of_convolutional_neural_networks_for_quantized_inference_on_embedded_platforms/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fixed-point_quantization_of_convolutional_neural_networks_for_quantized_inference_on_embedded_platforms/#content","title":"Content","text":"<p> Convolutional Neural Networks (CNNs) are known to be an effective method for image classification tasks but their high computational complexity and memory usage make them impractical for deployment on embedded platforms. To address this issue, we propose a method to optimize the quantization of weights, biases, and activations of each layer of a pretrained CNN without retraining it. Our method optimizes low bitwidth fixed-point representations for each parameter while controlling the loss in inference accuracy, resulting in a low precision CNN with accuracy losses of less than 1%. Compared to a method used by commercial tools that quantize parameters to 8-bits, our approach offers lower memory consumption and cost of executing multiplications. We find that layer-wise quantization of parameters significantly helps in the process of quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fixed_point_quantization_of_deep_convolutional_networks/","title":"Fixed point quantization of deep convolutional networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fixed_point_quantization_of_deep_convolutional_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a fixed point implementation method for deep convolution networks that reduces computation and model storage resources. The authors suggest a quantizer design and optimize bit-width allocation across DCN layers. In experiments, the fixed point DCNs with optimized bit width allocation show over 20% reduction in model size without any loss in accuracy on CIFAR-10 benchmark. The paper also reports a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark and discusses implementation challenges and solutions.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fixed_point_quantization_of_deep_convolutional_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fixed_point_quantization_of_deep_convolutional_networks/#content","title":"Content","text":"<p>20% reduction in model size without any loss in accuracy on CIFAR-10 benchmark. They also report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark. The authors discuss the challenges in implementing DCNs in fixed point and suggest possible solutions.&gt;"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Flexor%3A_Trainable_fractional_quantization/","title":"Flexor: Trainable fractional quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Flexor%3A_Trainable_fractional_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes an encryption algorithm to compress quantized weights by achieving fractional numbers of bits per weight. This method is applied to neural network models using digital XOR-gate networks described using tanh(x) for backward propagation. The proposed method yields higher model accuracy compared to binary neural networks on MNIST, CIFAR-10, and ImageNet datasets with high accuracy even for fractional sub 1-bit weights.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Flexor%3A_Trainable_fractional_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Flexor%3A_Trainable_fractional_quantization/#content","title":"Content","text":"<p> Quantization based on binary codes is becoming popular for efficient computations in deep neural networks. However, only allowing for integer numbers of quantization bits limits search space for compression ratio and accuracy. In this paper, we propose an encryption algorithm to compress quantized weights to achieve fractional numbers of bits per weight. Decryption during inference uses digital XOR-gate networks added to the neural network model. XOR gates are described using tanh(x) for backward propagation to enable gradient calculations. The proposed method yields smaller size and higher model accuracy compared to binary neural networks. Experiments using MNIST, CIFAR-10, and ImageNet datasets demonstrate high accuracy even for fractional sub 1-bit weights."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Flightnns%3A_Lightweight_quantized_deep_neural_networks_for_fast_and_accurate_inference/","title":"Flightnns: Lightweight quantized deep neural networks for fast and accurate inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Flightnns%3A_Lightweight_quantized_deep_neural_networks_for_fast_and_accurate_inference/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method for selecting the \"k\" value of lightweight neural networks, which constrains the weights of DNNs to be a limited combination of powers of 2 and can replace the multiply-accumulate operation with a single shift operation or two shifts and an add operation. The method is differentiable and allows for per-filter optimization of the \"k\" value. Results from experiments show that this method, called FLightNNs, provides 2x speedup and higher computational energy efficiency compared to other methods while only causing 0.1% accuracy degradation.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Flightnns%3A_Lightweight_quantized_deep_neural_networks_for_fast_and_accurate_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Flightnns%3A_Lightweight_quantized_deep_neural_networks_for_fast_and_accurate_inference/#content","title":"Content","text":"<p>To improve the throughput and energy efficiency of Deep Neural Networks (DNNs) on customized hardware, lightweight neural networks constrain the weights of DNNs to be a limited combination (denoted as k\u2208{1,2}) of powers of 2. In such networks, the multiply-accumulate operation can be replaced with a single shift operation, or two shifts and an add operation. To provide even more design flexibility, the k for each convolutional filter can be optimally chosen instead of being fixed for every filter. In this paper, we formulate the selection of k to be differentiable, and describe model training for determining k-based weights on a per-filter basis. Over 46 FPGA-design experiments involving eight configurations and four data sets reveal that lightweight neural networks with a flexible k value (dubbed FLightNNs) fully utilize the hardware resources on Field Programmable Gate Arrays (FPGAs), our experimental results show that FLightNNs can achieve 2 \u00d7 speedup when compared to lightweight NNs with k= 2, with only 0.1% accuracy degradation. Compared to a 4-bit fixed-point quantization, FLightNNs achieve higher accuracy and up to 2 \u00d7 inference speedup, due to their lightweight shift operations. In addition, our experiments also demonstrate that FLightNNs can achieve higher computational energy efficiency for ASIC implementation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Forward_and_backward_information_retention_for_accurate_binary_neural_networks/","title":"Forward and backward information retention for accurate binary neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Forward_and_backward_information_retention_for_accurate_binary_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes an approach called Information Retention Network (IR-Net) to address the information loss in both forward and backward propagation during neural network quantization. The approach mainly relies on two technical contributions, Libra Parameter Binarization (Libra-PB) and Error Decay Estimator (EDE), for balanced weights in forward propagation and approximating the sign function in backward propagation, respectively. Empirical results show that IR-Net consistently outperforms state-of-the-art quantization methods on CIFAR-10 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Forward_and_backward_information_retention_for_accurate_binary_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Forward_and_backward_information_retention_for_accurate_binary_neural_networks/#content","title":"Content","text":"<p>Weight and activation binarization is an effective approach to deep neural network compression and can accelerate the inference by leveraging bitwise operations. Although many binarization methods have improved the accuracy of the model by minimizing the quantization error in forward propagation, there remains a noticeable performance gap between the binarized model and the full-precision one. Our empirical study indicates that the quantization brings information loss in both forward and backward propagation, which is the bottleneck of training accurate binary neural networks. To address these issues, we propose an Information Retention Network (IR-Net) to retain the information that consists in the forward activations and backward gradients. IR-Net mainly relies on two technical contributions: (1) Libra Parameter Binarization (Libra-PB): simultaneously minimizing both quantization error and information loss of parameters by balanced and standardized weights in forward propagation; (2) Error Decay Estimator (EDE): minimizing the information loss of gradients by gradually approximating the sign function in backward propagation, jointly considering the updating ability and accurate gradients. We are the first to investigate both forward and backward processes of binary networks from the unified information perspective, which provides new insight into the mechanism of network binarization. Comprehensive experiments with various network structures on CIFAR-10 and ImageNet datasets manifest that the proposed IR-Net can consistently outperform state-of-the-art quantization methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fp8_formats_for_deep_learning/","title":"Fp8 formats for deep learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fp8_formats_for_deep_learning/#summary","title":"Summary","text":"<p>The paper proposes a new 8-bit floating-point binary interchange format called FP8 that aims to accelerate deep learning training inference beyond the currently used 16-bit formats found in modern processors. The proposal includes two encodings - E4M3 and E5M2. The study demonstrates the efficacy of the format in various image and language tasks, including CNNs, RNNs, and Transformer-based models without changing any of the hyperparameters from the 16-bit baseline training sessions. The paper also explores the post-training quantization of language models trained using 16-bit formats that resisted fixed-point int8 quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fp8_formats_for_deep_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fp8_formats_for_deep_learning/#content","title":"Content","text":"<p> FP8 is a proposed 8-bit floating-point binary interchange format that aims to accelerate deep learning training inference beyond the commonly used 16-bit formats found in modern processors. The proposal includes two encodings: E4M3 and E5M2. E4M3 extends dynamic range by not representing infinities and having only one mantissa bit-pattern for NaNs, while E5M2 follows IEEE 754 conventions for representing special values. The efficacy of the format is demonstrated on various image and language tasks, performing as well as 16-bit training sessions. The authors cover CNNs, RNNs, and Transformer-based models without changing any of the hyperparameters from the 16-bit baseline training sessions. The study also includes large language models with up to 175B parameters. Additionally, the authors examine the post-training quantization of language models trained using 16-bit formats that resisted fixed-point int8 quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fq-vit%3A_Fully_quantized_vision_transformer_without_retraining/","title":"Fq-vit: Fully quantized vision transformer without retraining","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fq-vit%3A_Fully_quantized_vision_transformer_without_retraining/#summary","title":"Summary","text":"<p>This paper presents a method called Power-of-Two Factor (PTF) to reduce inter-channel variation in LayerNorm inputs in order to improve the performance degradation of fully quantized vision transformers. They also propose a simplified method called Log-Int-Softmax (LIS) using 4-bit quantization and the BitShift operator. Their Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while using lower bit-width on attention maps and achieves no loss in accuracy degradation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fq-vit%3A_Fully_quantized_vision_transformer_without_retraining/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fq-vit%3A_Fully_quantized_vision_transformer_without_retraining/#content","title":"Content","text":"<p>Network quantization has been widely used in real-world deployments to reduce model inference complexity. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs) and suffer when applied to fully quantized vision transformers. In this work, the authors present a systematic method called Power-of-Two Factor (PTF) to reduce the performance degradation and inference complexity of fully quantized vision transformers by reducing inter-channel variation in LayerNorm inputs. They also propose a method called Log-Int-Softmax (LIS) to simplify inference by using 4-bit quantization and the BitShift operator. They demonstrate that their Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while using lower bit-width on attention maps. They also achieve lossless accuracy degradation (1%) on fully quantized vision transformers."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fracbits%3A_Mixed_precision_quantization_via_fractional_bit-widths/","title":"Fracbits: Mixed precision quantization via fractional bit-widths","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fracbits%3A_Mixed_precision_quantization_via_fractional_bit-widths/#summary","title":"Summary","text":"<p>Summary: The paper introduces FracBits, a learning-based algorithm for mixed precision quantization of deep neural networks. The algorithm optimizes the bit-width of each layer/kernel in the model to be a fractional status of two consecutive bit-widths, with a differentiable regularization term to meet resource constraints during quantization-aware training. Results show that FracBits achieves comparable or better performance than previous quantization methods with mixed precision on MobilenetV1/V2, ResNet18 under different resource constraints on the ImageNet dataset.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fracbits%3A_Mixed_precision_quantization_via_fractional_bit-widths/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fracbits%3A_Mixed_precision_quantization_via_fractional_bit-widths/#content","title":"Content","text":"<p> Model quantization is a popular technique used to reduce the size and latency of deep neural networks. Mixed precision quantization is a more flexible approach that can achieve a better balance between computation cost and model accuracy. We propose a novel learning-based algorithm called FracBits, which derives mixed precision models end-to-end under target computation constraints and model sizes. FracBits optimizes the bit-width of each layer/kernel in the model to be a fractional status of two consecutive bit-widths, with a differentiable regularization term to meet resource constraints during quantization-aware training. Our final models demonstrate comparable or better performance than previous quantization methods with mixed precision on MobilenetV1/V2, ResNet18 under different resource constraints on the ImageNet dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Free%3A_Feature_refinement_for_generalized_zero-shot_learning/","title":"Free: Feature refinement for generalized zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Free%3A_Feature_refinement_for_generalized_zero-shot_learning/#summary","title":"Summary","text":"<p>Summary: The paper proposes a GZSL method called Feature Refinement for Generalized Zero-Shot Learning (FREE), which uses a feature refinement module to overcome visual-semantic domain gap and seen-unseen bias in GZSL. The proposed method employs a self-adaptive margin center loss (SAMC-loss) and a semantic cycle-consistency loss to learn class- and semantically-relevant representations for seen and unseen class samples. Experimental results indicate that the proposed method achieves significant performance gains over existing GZSL methods across five benchmark datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Free%3A_Feature_refinement_for_generalized_zero-shot_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Free%3A_Feature_refinement_for_generalized_zero-shot_learning/#content","title":"Content","text":"<p> In this research paper \"FREE: Feature Refinement for Generalized Zero-Shot Learning\" by Shiming Chen et al., the authors propose a simple yet effective GZSL method, termed feature refinement for generalized zero-shot learning (FREE), to tackle the problems of visual-semantic domain gap and seen-unseen bias in generalized zero-shot learning (GZSL). The proposed method employs a feature refinement (FR) module that incorporates semantic-visual mapping into a unified generative model to refine the visual features of seen and unseen class samples. They also propose a self-adaptive margin center loss (SAMC-loss) that cooperates with a semantic cycle-consistency loss to guide FR to learn class- and semantically-relevant representations. The fully refined features are extracted by concatenating the features in FR. The proposed method achieved significant performance gains over its baseline and current state-of-the-art GZSL methods in five benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/From_Quantized_DNNs_to_Quantizable_DNNs/","title":"From Quantized DNNs to Quantizable DNNs","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/From_Quantized_DNNs_to_Quantizable_DNNs/#summary","title":"Summary","text":"<p> This paper introduces the concept of Quantizable DNNs which can quantize its bit-width during execution without re-training. The proposed consistency-based loss optimizes all bit modes and Bit-Specific Batch Normalization reduces conflicts among different bit modes. Experimental results on CIFAR100 and ImageNet show that Quantizable DNNs are more flexible and achieve higher classification accuracy. Additionally, the consistency-based loss improves the model's generalization performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/From_Quantized_DNNs_to_Quantizable_DNNs/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/From_Quantized_DNNs_to_Quantizable_DNNs/#content","title":"Content","text":"<p>This paper proposes Quantizable DNNs, a special type of DNNs that can flexibly quantize its bit-width during execution without further re-training. To optimize for all bit modes, a combination loss of all bit modes is proposed which enforces consistent predictions ranging from low-bit mode to 32-bit mode. Because outputs of matrix multiplication in different bit modes have different distributions, Bit-Specific Batch Normalization is introduced to reduce conflicts among different bit modes. Experimental results on CIFAR100 and ImageNet have shown that compared to quantized DNNs, Quantizable DNNs have much better flexibility and achieve even higher classification accuracy. The regularization through the consistency-based loss indeed improves the model's generalization performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FrostNet%3A_Towards_quantization-aware_network_architecture_search/","title":"FrostNet: Towards quantization-aware network architecture search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FrostNet%3A_Towards_quantization-aware_network_architecture_search/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new network architecture search (NAS) method to find a network that guarantees both full-precision and INT8 performances. The method uses a critical optimization method called Frost bottleneck, which enables quantization-aware training by integrating gradient-based NAS with StatAssist and GradBoost. Using Frost bottleneck as a building block for hardware-aware NAS, the authors discovered FrostNets that show improved quantization performance compared to other mobile-target networks while maintaining FLOAT32 performance.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FrostNet%3A_Towards_quantization-aware_network_architecture_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/FrostNet%3A_Towards_quantization-aware_network_architecture_search/#content","title":"Content","text":"<p> INT8 quantization has become a standard technique for deploying convolutional neural networks on edge devices to reduce memory and computational resource usage. This paper presents a new network architecture search (NAS) procedure to find a network that guarantees both full-precision (FLOAT32) and quantized (INT8) performances. The paper proposes a critical optimization method, Frost bottleneck, which enables quantization-aware training (QAT): floating-point statistic assisting (StatAssist) and stochastic gradient boosting (GradBoost). By integrating gradient-based NAS with StatAssist and GradBoost, the authors discovered a quantization-efficient network building block, Frost bottleneck. Furthermore, FrostNets, which show improved quantization performances compared to other mobile-target networks while maintaining comparable FLOAT32 performance was obtained using Frost bottleneck as the building block for hardware-aware NAS."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fully_quantized_image_super-resolution_networks/","title":"Fully quantized image super-resolution networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fully_quantized_image_super-resolution_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Fully Quantized image Super-Resolution framework (FQSR) for improving the inference efficiency of image super-resolution using model quantization. The proposed framework achieves comparable performance with full-precision counterparts while significantly reducing the computational cost and memory consumption. The experimental results on five benchmark datasets show the effectiveness of the proposed method.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fully_quantized_image_super-resolution_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fully_quantized_image_super-resolution_networks/#content","title":"Content","text":"<p> With the rising popularity of intelligent mobile devices, it is of great practical signi\ufb01cance to develop accurate, real-time and energy-ef\ufb01cient image Super-Resolution (SR) inference methods. A prevailing method for improving the inference ef\ufb01ciency is model quantization, which allows for replacing the expensive \ufb02oating-point operations with ef\ufb01cient bitwise arithmetic. Here, we propose a Fully Quantized image Super-Resolution framework (FQSR) to jointly optimize ef\ufb01ciency and accuracy. Experimental results show that our FQSR with low-bits quantization is able to achieve on par performance compared with the full-precision counterparts on \ufb01ve benchmark datasets and surpass the state-of-the-art quantized SR methods with significantly reduced computational cost and memory consumption."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fully_quantized_network_for_object_detection/","title":"Fully quantized network for object detection","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fully_quantized_network_for_object_detection/#summary","title":"Summary","text":"<p>Summary: This paper addresses the challenge of maintaining network accuracy while using low bitwidth arithmetic for efficient neural network inference. The proposed techniques successfully overcome the instability during the fine-tuning stage of the quantization process and produce fully quantized 4-bit detectors based on RetinaNet and Faster R-CNN. The results show that the proposed methods achieve state-of-the-art performance for quantized detectors with more than 3.8\u00d7 less loss compared to existing methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fully_quantized_network_for_object_detection/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fully_quantized_network_for_object_detection/#content","title":"Content","text":"<p> Efficient neural network inference is important in many practical domains, such as deployment in mobile settings. A method for increasing inference efficiency is to use low bitwidth arithmetic which can be accelerated using dedicated hardware. However, effective quantization schemes while maintaining network accuracy is still challenging. This paper addresses the instability during the fine-tuning stage of the quantization process and proposes techniques to overcome these instabilities. The proposed techniques are applied to produce fully quantized 4-bit detectors based on RetinaNet and Faster R-CNN, and they show that these achieve state-of-the-art performance for quantized detectors. The mAP loss due to quantization using these methods is more than 3.8\u00d7 less than the loss from existing methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fxpnet%3A_Training_a_deep_convolutional_neural_network_in_fixed-point_representation/","title":"Fxpnet: Training a deep convolutional neural network in fixed-point representation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fxpnet%3A_Training_a_deep_convolutional_neural_network_in_fixed-point_representation/#summary","title":"Summary","text":"<p>Summary: FxpNet is a framework that trains deep convolutional neural networks with low bit-width arithmetics in both forward and backward passes. It adapts the fixed-point formats of stored parameters during training, thus reducing their bit-width. FxpNet includes Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) to further minimize the floating-point operations. Overall, it achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks on the CIFAR-10 dataset with 12-bit primal parameters and 12-bit gradients.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fxpnet%3A_Training_a_deep_convolutional_neural_network_in_fixed-point_representation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Fxpnet%3A_Training_a_deep_convolutional_neural_network_in_fixed-point_representation/#content","title":"Content","text":"<p> We introduce FxpNet, a framework for training deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. FxpNet reduces the bit-width of stored parameters by adaptively updating their fixed-point formats during training. In FxpNet, during forward pass fixed-point primal weights and activations will first be binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations. Evaluation on the CIFAR-10 dataset indicates that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-art binarized and quantized neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/GPTQ%3A_Accurate_Post-Training_Quantization_for_Generative_Pre-trained_Transformers/","title":"GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/GPTQ%3A_Accurate_Post-Training_Quantization_for_Generative_Pre-trained_Transformers/#summary","title":"Summary","text":"<p> The paper presents GPTQ - a new one-shot weight quantization method based on approximate second-order information for Generative Pre-trained Transformer models (GPT or OPT) with 175 billion parameters. The proposed method can reduce the bitwidth down to 3 or 4 bits per weight in just 4 GPU hours with minimal accuracy degradation compared to the uncompressed baseline. GPTQ helps to improve the compression gains in comparison to previous quantization methods and enables the execution of a 175 billion-parameter model inside a single GPU for generative inference."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/GPTQ%3A_Accurate_Post-Training_Quantization_for_Generative_Pre-trained_Transformers/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/GPTQ%3A_Accurate_Post-Training_Quantization_for_Generative_Pre-trained_Transformers/#content","title":"Content","text":"<p>Generative Pre-trained Transformer models, known as GPT or OPT, have high computational and storage costs due to their massive size, limiting their usability. In this paper, we propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute a 175 billion-parameter model inside a single GPU for generative inference."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generalized_product_quantization_network_for_semi-supervised_image_retrieval/","title":"Generalized product quantization network for semi-supervised image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generalized_product_quantization_network_for_semi-supervised_image_retrieval/#summary","title":"Summary","text":"<p>The paper presents the Generalized Product Quantization (GPQ) network, which is the first quantization-based semi-supervised image retrieval scheme. The authors propose a novel metric learning strategy to maintain semantic similarity between the labeled data and use an entropy regularization term for the unlabeled data. The proposed method achieves state-of-the-art performance on large-scale real image benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generalized_product_quantization_network_for_semi-supervised_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generalized_product_quantization_network_for_semi-supervised_image_retrieval/#content","title":"Content","text":"<p> Image retrieval methods have achieved great success with hashing or vector quantization, but expensive label information is necessary for good results. To solve this, the authors propose the first quantization-based semi-supervised image retrieval scheme: Generalized Product Quantization (GPQ) network. The authors design a novel metric learning strategy to preserve semantic similarity between labeled data and employ an entropy regularization term for unlabeled data. This increases the generalization capacity of the quantization network, leading to state-of-the-art performance on large-scale real image benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generalized_zero-shot_learning_for_action_recognition_with_web-scale_video_data/","title":"Generalized zero-shot learning for action recognition with web-scale video data","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generalized_zero-shot_learning_for_action_recognition_with_web-scale_video_data/#summary","title":"Summary","text":"<p>The paper discusses the challenges in efficient action recognition in surveillance videos due to the large number of action classes and limitations in collecting real-world footage. It explores the potential of zero-shot learning in addressing these issues and proposes a method for action recognition using generalized zero-shot learning to transfer knowledge from web videos for detecting anomalous actions in surveillance videos. The paper also evaluates existing zero-shot learning approaches under the generalized zero-shot setting and highlights their limitations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generalized_zero-shot_learning_for_action_recognition_with_web-scale_video_data/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generalized_zero-shot_learning_for_action_recognition_with_web-scale_video_data/#content","title":"Content","text":"<p>Action recognition in surveillance video makes our life safer by detecting the criminal events or predicting violent emergencies. However, efficient action recognition is not free of difficulty. First, there are so many action classes in daily life that we cannot pre-define all possible action classes beforehand. Moreover, it is very hard to collect real-word videos for certain particular actions such as steal and street fight due to legal restrictions and privacy protection. These challenges make existing data-driven recognition methods insufficient to attain desired performance. Zero-shot learning is potential to be applied to solve these issues since it can perform classification without positive example. Nevertheless, current zero-shot learning algorithms have been studied under the unreasonable setting where seen classes are absent during the testing phase. Motivated by this, we study the task of action recognition in surveillance video under a more realistic generalized zero-shot setting, where testing data contains both seen and unseen classes. To our best knowledge, this is the first work to study video action recognition under the generalized zero-shot setting. We firstly perform extensive empirical studies on several existing zero-shot leaning approaches under this new setting on a web-scale video data. Our experimental results demonstrate that, under the generalize setting, typical zero-shot learning methods are no longer effective for the dataset we applied. Then, we propose a method for action recognition by deploying generalized zero-shot learning, which transfers the knowledge of web video to detect the anomalous actions in surveillance videos."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generative_low-bitwidth_data_free_quantization/","title":"Generative low-bitwidth data free quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generative_low-bitwidth_data_free_quantization/#summary","title":"Summary","text":"<p>The paper proposes a method called Generative Low-bitwidth Data Free Quantization (GDFQ) which aims to remove the dependency on original data for quantizing deep models. It uses a knowledge matching generator to produce fake data based on distribution information and classification boundary knowledge from a pre-trained model. The paper's experiments show that the GDFQ method effectively compresses models without the need for original data for calibration."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generative_low-bitwidth_data_free_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generative_low-bitwidth_data_free_quantization/#content","title":"Content","text":"<p>Neural network quantization is an effective way to compress deep models and improve their execution latency and energy efficiency, so that they can be deployed on mobile or embedded devices. Existing quantization methods require original data for calibration or fine-tuning to get better performance. In this paper, the authors propose a simple-yet-effective method called Generative Low-bitwidth Data Free Quantization (GDFQ) to remove the data dependence burden. The authors investigate a knowledge matching generator to produce meaningful fake data by exploiting classification boundary knowledge and distribution information in the pre-trained model. With the help of generated data, the authors can quantize a model by learning knowledge from the pre-trained model. Extensive experiments on three data sets demonstrate the effectiveness of the method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generative_zero-shot_network_quantization/","title":"Generative zero-shot network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generative_zero-shot_network_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method for zero-shot quantization of convolutional neural networks using intrinsic Batch Normalization statistics. The approach generates a calibration set for corresponding zero-shot network quantization by using VAE/GAN methods to match the distribution of BN statistics. The proposed method outperforms existing data-free quantization methods as demonstrated by experiments on benchmark datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generative_zero-shot_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Generative_zero-shot_network_quantization/#content","title":"Content","text":"<p> Convolutional neural networks can generate realistic images with the help of ample training samples. However, in the absence of original training data, zero-shot quantization of such models becomes indispensable. To address this, we leverage intrinsic Batch Normalization (BN) statistics to reconstruct \u201crealistic\u201d images of each category without any training data. Following VAE/GAN methods, we regard the zero-shot optimization of synthetic images as generative modeling to match the distribution of BN statistics, thus generating a calibration set for the corresponding zero-shot network quantizations. Our experiments on benchmark datasets show that our approach consistently outperforms existing data-free quantization methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Glide%3A_Towards_photorealistic_image_generation_and_editing_with_text-guided_diffusion_models/","title":"Glide: Towards photorealistic image generation and editing with text-guided diffusion models","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Glide%3A_Towards_photorealistic_image_generation_and_editing_with_text-guided_diffusion_models/#summary","title":"Summary","text":"<p>Summary: The paper explores text-conditional image synthesis using diffusion models and compares two guidance strategies - CLIP guidance and classifier-free guidance. Human evaluators prefer the latter strategy for generating photorealistic and caption-similar samples. The models can also perform image inpainting, enabling powerful text-driven image editing. The code and weights for the smaller model have been released on Github.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Glide%3A_Towards_photorealistic_image_generation_and_editing_with_text-guided_diffusion_models/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Glide%3A_Towards_photorealistic_image_generation_and_editing_with_text-guided_diffusion_models/#content","title":"Content","text":"<p> Diffusion models can generate high-quality synthetic images when guided by a text-based technique to balance diversity and fidelity. In this paper, we explore text-conditional image synthesis using diffusion models and compare two guidance strategies: CLIP guidance and classifier-free guidance. Our human evaluators prefer the latter strategy for generating photorealistic and caption-similar samples. Furthermore, we demonstrate that our models can perform image inpainting, enabling powerful text-driven image editing. We release the code and weights for our smaller model at https://github.com/openai/glide-text2im."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Gradient__Regularization_for_Quantization_Robustness/","title":"Gradient  Regularization for Quantization Robustness","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Gradient__Regularization_for_Quantization_Robustness/#summary","title":"Summary","text":"<p>The paper discusses the effect of quantizing weights and activations in neural networks on loss and presents a regularization method to improve robustness against post-training quantization. The proposed approach allows storing a single set of weights that can be quantized to different bit-widths. The regularization scheme is validated experimentally on various architectures on CIFAR-10 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Gradient__Regularization_for_Quantization_Robustness/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Gradient__Regularization_for_Quantization_Robustness/#content","title":"Content","text":"<p> We analyze the effect of quantizing weights and activations of neural networks on their loss and derive a simple regularization scheme that improves robustness against post-training quantization. Our approach enables storing a single set of weights that can be quantized on-demand to different bit-widths. By modeling quantization as a <code>1-bounded perturbation, the first-order term in the loss expansion can be regularized using the</code>1-norm of gradients. We experimentally validate the effectiveness of our regularization scheme on different architectures on CIFAR-10 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Granularity-Aware_Adaptation_for_Image_Retrieval_Over_Multiple_Tasks/","title":"Granularity-Aware Adaptation for Image Retrieval Over Multiple Tasks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Granularity-Aware_Adaptation_for_Image_Retrieval_Over_Multiple_Tasks/#summary","title":"Summary","text":"<p> The paper proposes an approach called Grappa that adapts a pre-trained model to solve multiple image retrieval tasks concurrently, using only unlabeled images from different task domains. The model is extended with multiple sets of adaptors that use pseudo-label sets of different sizes, which are reconciled into a unified model using fusion layers. The authors show that Grappa improves the zero-shot performance of a state-of-the-art self-supervised learning model and can match or outperform a task label-aware oracle."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Granularity-Aware_Adaptation_for_Image_Retrieval_Over_Multiple_Tasks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Granularity-Aware_Adaptation_for_Image_Retrieval_Over_Multiple_Tasks/#content","title":"Content","text":"<p>Strong image search models can be learned for a specific domain if labeled images of that domain are available. However, a practical visual search model should be versatile enough to solve multiple retrieval tasks simultaneously, even if those cover very different specialized domains. We propose Grappa, an approach that adapts a strong pretrained model to tackle multiple retrieval tasks concurrently, using only unlabeled images from the different task domains. We extend the pretrained model with multiple independently trained sets of adaptors that use pseudo-label sets of different sizes, effectively mimicking different pseudo-granularities. We reconcile all adaptor sets into a single unified model suited for all retrieval tasks by learning fusion layers that we guide by propagating pseudo-granularity attentions across neighbors in the feature space. Results on a benchmark composed of six heterogeneous retrieval tasks show that the unsupervised Grappa model improves the zero-shot performance of a state-of-the-art self-supervised learning model, and in some places reaches or improves over a task label-aware oracle that selects the most fitting pseudo-granularity per task."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/HLHLp%3A_Quantized_neural_networks_training_for_reaching_flat_minima_in_loss_surface/","title":"HLHLp: Quantized neural networks training for reaching flat minima in loss surface","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/HLHLp%3A_Quantized_neural_networks_training_for_reaching_flat_minima_in_loss_surface/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel training scheme for quantized deep neural networks, which uses alternating high-low-high-low precision coupled with an abrupt change in learning rate to reach flat minima in the loss surface. The proposed technique demonstrates superior performance for convolutional neural networks and achieves state-of-the-art results for recurrent neural network-based language modeling with 2-bit weight and activation compared to previous fine-tuning-based quantization schemes.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/HLHLp%3A_Quantized_neural_networks_training_for_reaching_flat_minima_in_loss_surface/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/HLHLp%3A_Quantized_neural_networks_training_for_reaching_flat_minima_in_loss_surface/#content","title":"Content","text":"<p> Quantized deep neural networks (QDNNs) are important for efficient implementations. To this end, we propose a novel training scheme that employs high-low-high-low precision in an alternating manner for network training, coupled with an abrupt change in the learning rate at each stage for coarse- or fine-tuning, to reach flat minima in the loss surface with the aid of quantization noise. Using this proposed training technique, we demonstrate significant performance improvements for convolutional neural networks when compared to previous fine-tuning-based quantization schemes, and we achieve state-of-the-art results for recurrent neural network-based language modeling with 2-bit weight and activation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hardware-Aware_Quantization_for_Multiplierless_Neural_Network_Controllers/","title":"Hardware-Aware Quantization for Multiplierless Neural Network Controllers","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hardware-Aware_Quantization_for_Multiplierless_Neural_Network_Controllers/#summary","title":"Summary","text":"<p>Summary: The paper discusses a hardware-aware quantization technique for multiplierless neural network controllers that considers both hardware constraints and neural network accuracy requirements during the quantization process. The method determines the optimal quantization bit-width for different network layers by minimizing quantization error under hardware constraints. Experimental results show that the proposed method achieves significant reduction in memory footprint and computational complexity with minimal accuracy degradation compared to existing approaches, making it suitable for deployment on resource-constrained hardware devices such as microcontrollers and FPGAs.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hardware-Aware_Quantization_for_Multiplierless_Neural_Network_Controllers/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hardware-Aware_Quantization_for_Multiplierless_Neural_Network_Controllers/#content","title":"Content","text":"<p>Hardware-aware quantization techniques for multiplierless neural network controllers are described in this paper. The proposed method considers both the hardware constraints and the neural network accuracy requirements during the quantization process. The optimal quantization bit-width for different network layers is determined by minimizing the quantization error subject to hardware constraints. Experimental results show that the proposed method achieves significant reduction in memory footprint and computational complexity with minimal accuracy degradation compared to existing approaches. This makes the method suitable for deployment on resource-constrained hardware devices such as microcontrollers and FPGAs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Harmonious_coexistence_of_structured_weight_pruning_and_ternarization_for_deep_neural_networks/","title":"Harmonious coexistence of structured weight pruning and ternarization for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Harmonious_coexistence_of_structured_weight_pruning_and_ternarization_for_deep_neural_networks/#summary","title":"Summary","text":"<p> This paper proposes a PE-wise structured pruning approach and optimized weight ternarization to compress deep neural networks. The proposed techniques can achieve up to 21 times compression rate with minimal accuracy degradation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Harmonious_coexistence_of_structured_weight_pruning_and_ternarization_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Harmonious_coexistence_of_structured_weight_pruning_and_ternarization_for_deep_neural_networks/#content","title":"Content","text":"<p>Deep convolutional neural networks have exhibited tremendous success in computer vision, but their large model sizes and high computing complexity hinder their deployment on resource-limited embedded systems. Weight pruning and quantization have been used to compress DNN models, but combining the two techniques exhibits disharmony, especially when more aggressive compression schemes are applied. This work introduces a PE-wise structured pruning scheme and integrates it with an optimized weight ternarization approach to quantize weights into ternary values. The proposed techniques achieve up to 21 times PE-wise structured compression rate with only a 1.74%/0.94% accuracy degradation of ResNet-18 on the ImageNet dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Harnessing_object_and_scene_semantics_for_large-scale_video_understanding/","title":"Harnessing object and scene semantics for large-scale video understanding","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Harnessing_object_and_scene_semantics_for_large-scale_video_understanding/#summary","title":"Summary","text":"<p>This paper proposes a semantic fusion network to combine frame-based low-level CNN features with object features and scene features to improve supervised activity and video categorization on two large-scale datasets - ActivityNet and FCVID. The fusion network has the ability to discover semantic relationships between video classes and objects/scenes by examining and back propagating information through the network. Zero-shot action/video classification and clustering experiments show the effectiveness of this semantic representation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Harnessing_object_and_scene_semantics_for_large-scale_video_understanding/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Harnessing_object_and_scene_semantics_for_large-scale_video_understanding/#content","title":"Content","text":"<p>Large-scale action recognition and video categorization are important computer vision problems. In this paper, the authors propose a semantic fusion network that combines three streams of information: (i) frame-based low-level CNN features, (ii) object features from a state-of-the-art large-scale CNN object-detector trained to recognize 20K classes, and (iii) scene features from a state-of-the-art CNN scene-detector trained to recognize 205 scenes. The fusion network achieves improvements in supervised activity and video categorization on two large-scale datasets - ActivityNet and FCVID. By examining and back propagating information through the fusion network, semantic relationships between video classes and objects/scenes can be discovered. The authors illustrate the effectiveness of this semantic representation through experiments on zero-shot action/video classification and clustering."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hashnet%3A_Deep_learning_to_hash_by_continuation/","title":"Hashnet: Deep learning to hash by continuation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hashnet%3A_Deep_learning_to_hash_by_continuation/#summary","title":"Summary","text":"<p>Summary: The paper presents HashNet, a novel deep architecture for deep learning to hash that learns exactly binary hash codes from imbalanced similarity data. It uses a continuation method with convergence guarantees to address the ill-posed gradient difficulty in optimization with sign activations. HashNet can generate exactly binary hash codes and outperforms existing deep learning to hash methods on standard benchmarks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hashnet%3A_Deep_learning_to_hash_by_continuation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hashnet%3A_Deep_learning_to_hash_by_continuation/#content","title":"Content","text":"<p> Learning to hash has been widely applied to approxi- mate nearest neighbor search for large-scale multimedia re- trieval, due to its computation ef\ufb01ciency and retrieval qual- ity. Deep learning to hash, which improves retrieval quality by end-to-end representation learning and hash encoding, has received increasing attention recently. Subject to the ill- posed gradient dif\ufb01culty in the optimization with sign acti- vations, existing deep learning to hash methods need to \ufb01rst learn continuous representations and then generate binary hash codes in a separated binarization step, which suffer from substantial loss of retrieval quality. This work presents HashNet, a novel deep architecture for deep learning to hash by continuation method with convergence guarantees, which learns exactly binary hash codes from imbalanced similarity data. Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state- of-the-art multimedia retrieval performance on standard benchmarks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq-v2%3A_Hessian_aware_trace-weighted_quantization_of_neural_networks/","title":"Hawq-v2: Hessian aware trace-weighted quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq-v2%3A_Hessian_aware_trace-weighted_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: HAWQ-V2 is a mixed-precision quantization method that automatically selects bit precisions without manual intervention. It uses a Pareto frontier based method and the average Hessian trace as the sensitivity metric. This Hessian based analysis for mixed-precision activation quantization produces the best object detection results and achieved state-of-the-art quantization results for InceptionV3, ResNet50, and SqueezeNext.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq-v2%3A_Hessian_aware_trace-weighted_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq-v2%3A_Hessian_aware_trace-weighted_quantization_of_neural_networks/#content","title":"Content","text":"<p>Quantization is a method to reduce the memory footprint and inference time of Neural Networks. However, low precision quantization can lead to significant model accuracy degradation. Mixed-precision quantization keeps more sensitive layers at higher precision, but the search space for this method is exponential in the number of layers. Hessian-based frameworks have been proposed to reduce the search space, but they have limitations. We present HAWQ-V2, which theoretically shows that the average Hessian trace is the right sensitivity metric and uses a Pareto frontier based method for automatic bit precision selection without manual intervention. HAWQ-V2 develops the first Hessian based analysis for mixed-precision activation quantization, which produces the best object detection results. State-of-the-art quantization results have been achieved for InceptionV3, ResNet50, and SqueezeNext, all without manual bit selection. Object detection results have outperformed direct uniform quantization and the recently proposed method of FQN."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq-v3%3A_Dyadic_neural_network_quantization/","title":"Hawq-v3: Dyadic neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq-v3%3A_Dyadic_neural_network_quantization/#summary","title":"Summary","text":"<p>Summary: The paper introduces HAWQ-V3, a mixed-precision integer-only quantization framework to improve the efficiency of neural network quantization by avoiding the hidden cost of conversion between floating point and quantized integer values.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq-v3%3A_Dyadic_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq-v3%3A_Dyadic_neural_network_quantization/#content","title":"Content","text":"<p> Current low-precision quantization algorithms often have the hidden cost of conversion back and forth from floating point to quantized integer values. This hidden cost limits the latency improvement realized by quantizing Neural Networks. To address this, we present HAWQ-V3, a novel mixed-precision integer-only quantization framework."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq%3A_Hessian_aware_quantization_of_neural_networks_with_mixed-precision/","title":"Hawq: Hessian aware quantization of neural networks with mixed-precision","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq%3A_Hessian_aware_quantization_of_neural_networks_with_mixed-precision/#summary","title":"Summary","text":"<p>Summary: The paper introduces Hessian AWare Quantization (HAWQ), which is a second-order quantization method that provides a systematic approach to determine the relative quantization precision of each layer based on the Hessian spectrum. The results demonstrate that HAWQ achieves similar or better accuracy with 8x activation compression ratio than recently proposed methods on ResNet20 and up to 1% higher accuracy with up to 14% smaller models on ResNet50 and Inception-V3. In addition, HAWQ can quantize SqueezeNext to just 1MB model size while maintaining above 68% top1 accuracy on ImageNet.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq%3A_Hessian_aware_quantization_of_neural_networks_with_mixed-precision/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hawq%3A_Hessian_aware_quantization_of_neural_networks_with_mixed-precision/#content","title":"Content","text":"<p> Model size and inference speed/power are major challenges in the deployment of neural networks. Uniformly quantizing a model to ultra-low precision leads to significant accuracy degradation. A promising solution is mixed-precision quantization, but there is no systematic way to determine the precision of different layers. Hessian AWare Quantization (HAWQ) is introduced as a second-order quantization method that provides a deterministic fine-tuning order for quantizing layers. HAWQ allows for the automatic selection of the relative quantization precision of each layer, based on the layer\u2019s Hessian spectrum. Results show that HAWQ achieves similar/better accuracy with 8x activation compression ratio on ResNet20 and up to 1% higher accuracy with up to 14% smaller models on ResNet50 and Inception-V3, compared to recently proposed methods. Furthermore, HAWQ can quantize SqueezeNext to just 1MB model size while achieving above 68% top1 accuracy on ImageNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hero%3A_Hessian-enhanced_robust_optimization_for_unifying_and_improving_generalization_and_quantization_performance/","title":"Hero: Hessian-enhanced robust optimization for unifying and improving generalization and quantization performance","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hero%3A_Hessian-enhanced_robust_optimization_for_unifying_and_improving_generalization_and_quantization_performance/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Hessian-enhanced robust optimization method called HERO to improve the generalization and quantization performance of neural network models for efficient deployment on mobile and edge devices. This is achieved by simultaneously minimizing Hessian eigenvalues and enhancing the model's robustness against bounded weight perturbation. The method shows significant improvements in test accuracy, training label perturbation, and post-training quantization accuracy for common model architectures on various datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hero%3A_Hessian-enhanced_robust_optimization_for_unifying_and_improving_generalization_and_quantization_performance/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hero%3A_Hessian-enhanced_robust_optimization_for_unifying_and_improving_generalization_and_quantization_performance/#content","title":"Content","text":"<p>With the recent demand of deploying neural network models on mobile and edge devices, it is desired to improve the model\u2019s generalizability on unseen testing data, as well as enhance the model\u2019s robustness under fixed-point quantization for efficient deployment. Minimizing the training loss, however, provides few guarantees on the generalization and quantization performance. In this work, we fulfill the need of improving generalization and quantization performance simultaneously by theoretically unifying them under the framework of improving the model\u2019s robustness against bounded weight perturbation and minimizing the eigenvalues of the Hessian matrix with respect to model weights. We therefore propose HERO, a Hessian-enhanced robust optimization method, to minimize the Hessian eigenvalues through a gradient-based training process, simultaneously improving the generalization and quantization performance. HERO enables up to a 3.8% gain on test accuracy, up to 30% higher accuracy under 80% training label perturbation, and the best post-training quantization accuracy across a wide range of precision, including a &gt;10% accuracy improvement over SGD-trained models for common model architectures on various datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/How_many_bits_does_it_take_to_quantize_your_neural_network%3F/","title":"How many bits does it take to quantize your neural network?","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/How_many_bits_does_it_take_to_quantize_your_neural_network%3F/#summary","title":"Summary","text":"<p>Summary: This paper investigates the impact of quantization on a neural network's robustness to adversarial attacks. The researchers found that both robustness and non-robustness are not monotonic with the number of bits used for the representation, and neither are preserved by quantization from a real-numbered network. To address this issue, the paper introduces a verification method for quantized neural networks using SMT solving over bit-vectors to account for their exact, bit-precise semantics. The researchers built a tool to apply this method to a MNIST dataset classifier and demonstrated that it outperforms existing methods for analyzing real-numbered networks and quantized networks. The paper also shows how the number of bits in quantization can increase gender bias in a predictor for students' grades.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/How_many_bits_does_it_take_to_quantize_your_neural_network%3F/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/How_many_bits_does_it_take_to_quantize_your_neural_network%3F/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hptq%3A_Hardware-friendly_post_training_quantization/","title":"Hptq: Hardware-friendly post training quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hptq%3A_Hardware-friendly_post_training_quantization/#summary","title":"Summary","text":"<p>Summary: This paper proposes a hardware-friendly post-training quantization framework for neural network quantization. The framework utilizes known quantization methods to meet the requirement for uniform, symmetric, and power-of-two quantizers for hardware efficiency. The proposed framework is evaluated on four computer vision tasks and shown to produce competitive results under hardware-friendly constraints.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hptq%3A_Hardware-friendly_post_training_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hptq%3A_Hardware-friendly_post_training_quantization/#content","title":"Content","text":"<p> This paper introduces a hardware-friendly post-training quantization (HPTQ) framework for neural network quantization, which addresses the requirement for uniform, symmetric, and power-of-two quantizers for hardware efficiency. The framework combines several known quantization methods and is evaluated on four computer vision tasks. The experiments show that competitive results can be obtained under the hardware-friendly constraints."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hsva%3A_Hierarchical_semantic-visual_adaptation_for_zero-shot_learning/","title":"Hsva: Hierarchical semantic-visual adaptation for zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hsva%3A_Hierarchical_semantic-visual_adaptation_for_zero-shot_learning/#summary","title":"Summary","text":"<p>Summary: The paper proposes a hierarchical semantic-visual adaptation (HSV A) framework to mitigate the distribution disagreement issue between semantic and visual domains in zero-shot learning. The proposed framework adopts a two-step adaptation process, structure adaptation, and distribution adaptation, and uses supervised adversarial discrepancy (SAD) learning and Wasserstein distance to align semantic and visual domains. The proposed framework outperforms existing methods on conventional and generalized zero-shot learning tasks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hsva%3A_Hierarchical_semantic-visual_adaptation_for_zero-shot_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hsva%3A_Hierarchical_semantic-visual_adaptation_for_zero-shot_learning/#content","title":"Content","text":"<p> Zero-shot learning (ZSL) is a technique to tackle the unseen class recognition problem by transferring semantic knowledge from seen classes to unseen ones. However, the existing common space learning methods in ZSL align semantic and visual domains by only mitigating distribution disagreement through one-step adaptation which is usually ineffective due to the heterogeneity of the feature representations in the two domains. To address this issue, the authors propose a hierarchical semantic-visual adaptation (HSV A) framework that aligns semantic and visual domains by adopting a two-step adaptation process, structure adaptation, and distribution adaptation. They use supervised adversarial discrepancy (SAD) learning to adversarially minimize the discrepancy between the predictions of two task-specific classifiers in the structure adaptation step. Finally, they minimize the Wasserstein distance between the latent multivariate Gaussian distributions to align visual and semantic distributions in the distribution adaptation step. The proposed framework achieves superior performance compared to existing methods on both conventional and generalized ZSL."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hybrid_8-bit_floating_point_%28HFP8%29_training_and_inference_for_deep_neural_networks/","title":"Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hybrid_8-bit_floating_point_%28HFP8%29_training_and_inference_for_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary:  The paper proposes a hybrid FP8 (HFP8) format and DNN end-to-end distributed training procedure that successfully trains deep learning models across a spectrum of applications without accuracy degradation by reducing the numerical precision of data and computation. The proposed techniques enable a new generation of 8-bit hardware for building and deploying neural network models without losing accuracy by simply fine-tuning batch normalization statistics.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hybrid_8-bit_floating_point_%28HFP8%29_training_and_inference_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hybrid_8-bit_floating_point_%28HFP8%29_training_and_inference_for_deep_neural_networks/#content","title":"Content","text":"<p> Reducing the numerical precision of data and computation is extremely effective in accelerating deep learning training workloads. Towards this end, 8-bit \ufb02oating point representations (FP8) were recently proposed for DNN training. However, its applicability was only demonstrated on a few selected models and signi\ufb01cant degradation is observed when popular networks such as MobileNet and Trans- former are trained using FP8. This degradation is due to the inherent precision requirement difference in the forward and backward passes of DNN training. Using theoretical insights, we propose a hybrid FP8 (HFP8) format and DNN end-to-end distributed training procedure. We demonstrate, using HFP8, the successful train- ing of deep learning models across a whole spectrum of applications including Image Classi\ufb01cation, Object Detection, Language and Speech without accuracy degradation. Finally, we demonstrate that, by using the new 8 bit format, we can directly quantize a pre-trained model down to 8-bits without losing accuracy by simply \ufb01ne-tuning batch normalization statistics. These novel techniques enable a new generations of 8-bit hardware that are robust for building and deploying neural network models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hyper-snn%3A_Towards_energy-efficient_quantized_deep_spiking_neural_networks_for_hyperspectral_image_classification/","title":"Hyper-snn: Towards energy-efficient quantized deep spiking neural networks for hyperspectral image classification","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hyper-snn%3A_Towards_energy-efficient_quantized_deep_spiking_neural_networks_for_hyperspectral_image_classification/#summary","title":"Summary","text":"<p> The paper proposed to use Spiking Neural Networks generated from iso-architecture Convolutional Neural Networks for accurate processing of hyperspectral images. During training and inference, analog pixel values directly applied to input layer of the SNN. The proposed technique achieves state-of-the-art accuracies with less computational energy on average over three HSI datasets than existing CNN methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hyper-snn%3A_Towards_energy-efficient_quantized_deep_spiking_neural_networks_for_hyperspectral_image_classification/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Hyper-snn%3A_Towards_energy-efficient_quantized_deep_spiking_neural_networks_for_hyperspectral_image_classification/#content","title":"Content","text":"<p> Hyperspectral images provide rich spatial-spectral information, but require energy-expensive 3-D Convolutional Neural Networks (CNNs) for accurate processing. To address this, we propose the use of Spiking Neural Networks (SNNs) generated from iso-architecture CNNs and trained with quantization-aware gradient descent. During training and inference, analog pixel values are directly applied to the input layer of the SNN. Our reduced-latency training technique combined with high activation sparsity yield significant improvements in computational efficiency, achieving state-of-the-art accuracies with substantially less computational energy on average over three HSI datasets than existing CNN methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/I-ViT%3A_integer-only_quantization_for_efficient_vision_transformer_inference/","title":"I-ViT: integer-only quantization for efficient vision transformer inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/I-ViT%3A_integer-only_quantization_for_efficient_vision_transformer_inference/#summary","title":"Summary","text":"<p> The paper proposes an integer-only quantization scheme, I-ViT, for Vision Transformers (ViTs) that approximates floating-point operations with Shiftmax and ShiftGELU methods, enabling efficient integer-only inference for ViTs. The proposed method achieves comparable or slightly higher accuracy than the full-precision baseline on benchmark models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/I-ViT%3A_integer-only_quantization_for_efficient_vision_transformer_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/I-ViT%3A_integer-only_quantization_for_efficient_vision_transformer_inference/#content","title":"Content","text":"<p>  In this paper, the authors propose I-ViT, an integer-only quantization scheme for Vision Transformers (ViTs), to enable efficient integer-only inference. The proposed method applies the Shiftmax and ShiftGELU methods to approximate the corresponding floating-point operations, allowing ViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. They evaluate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves comparable (or even slightly higher) accuracy to the full-precision (FP) baseline."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/IFQ-Net%3A_Integrated_fixed-point_quantization_networks_for_embedded_vision/","title":"IFQ-Net: Integrated fixed-point quantization networks for embedded vision","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/IFQ-Net%3A_Integrated_fixed-point_quantization_networks_for_embedded_vision/#summary","title":"Summary","text":"<p>Summary: The paper proposes a fixed-point network called Integrated Fixed-point Quantization Networks (IFQ-Net) for embedded vision tasks by converting floating-point data in a quantization network into fixed-point. The IFQ-Net gives significant savings on model size and runtime feature map memory with similar accuracy on ImageNet.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/IFQ-Net%3A_Integrated_fixed-point_quantization_networks_for_embedded_vision/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/IFQ-Net%3A_Integrated_fixed-point_quantization_networks_for_embedded_vision/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Illiterate_dall-e_learns_to_compose/","title":"Illiterate dall-e learns to compose","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Illiterate_dall-e_learns_to_compose/#summary","title":"Summary","text":"<p>The paper proposes a slot-based autoencoding architecture called SLATE1 that combines the best of both worlds of DALL-E and object-centric representation models to systematically generalize for zero-shot generation of images without text. SLATE1 learns object-centric representations, enabling systematic generalization in zero-shot image generation without the need for a text prompt."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Illiterate_dall-e_learns_to_compose/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Illiterate_dall-e_learns_to_compose/#content","title":"Content","text":"<p> Although DALLE has shown an impressive ability of composition-based systematic generalization in image generation, it requires the dataset of text-image pairs and the compositionality is provided by the text. In contrast, object-centric representation models like the Slot Attention model learn composable representations without the text prompt. However, unlike DALL E, its ability to systematically generalize for zero-shot generation is significantly limited. In this paper, we propose a simple but novel slot-based autoencoding architecture, called SLATE1, for combining the best of both worlds: learning object-centric representations that allows systematic generalization in zero-shot image generation without text. As such, this model can also be seen as an illiterate DALL E model."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Image_compression_with_neural_networks%E2%80%93a_survey/","title":"Image compression with neural networks\u2013a survey","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Image_compression_with_neural_networks%E2%80%93a_survey/#summary","title":"Summary","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Image_compression_with_neural_networks%E2%80%93a_survey/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Image_compression_with_neural_networks%E2%80%93a_survey/#content","title":"Content","text":"<p>Apart from the existing technology on image compression represented by series of JPEG, MPEG and H.26x standards, new technology such as neural networks and genetic algorithms are being developed to explore the future ofimage coding. Successful applications of neural networks to vector quantization have now become well established, andother aspects of neural network involvement in this area are stepping up to play signi \"cant roles in assisting with those traditional technologies. This paper presents an extensive survey on the development of neural networks for imagecompression which covers three categories: direct image compression by neural networks; neural network implementation of existing techniques, and neural network based technology which provide improvement over traditional algorithms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Impact_of_low-bitwidth_quantization_on_the_adversarial_robustness_for_embedded_neural_networks/","title":"Impact of low-bitwidth quantization on the adversarial robustness for embedded neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Impact_of_low-bitwidth_quantization_on_the_adversarial_robustness_for_embedded_neural_networks/#summary","title":"Summary","text":"<p>Summary: The article discusses the adversarial robustness of quantized neural networks for supervised image classification. The study found that weight quantization does not protect against adversarial attacks and can result in gradient masking. Poor transferability capacities were also observed due to quantization value shift and gradient misalignment. An ensemble-based defense method was suggested to address these issues.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Impact_of_low-bitwidth_quantization_on_the_adversarial_robustness_for_embedded_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Impact_of_low-bitwidth_quantization_on_the_adversarial_robustness_for_embedded_neural_networks/#content","title":"Content","text":"<p> In this article, the authors investigate the adversarial robustness of quantized neural networks for a supervised image classification task. The authors show that weight quantization does not provide any robust protection against adversarial attacks and leads to severe gradient masking. The authors also explore hypotheses to explain these results. Finally, the authors experimentally observe poor transferability capacities which they explain by a quantization value shift phenomenon and gradient misalignment. The article also highlights an ensemble-based defense method to tackle these issues."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_Low-Precision_Network_Quantization_via_Bin_Regularization/","title":"Improving Low-Precision Network Quantization via Bin Regularization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_Low-Precision_Network_Quantization_via_Bin_Regularization/#summary","title":"Summary","text":"<p>This paper proposes a novel weight regularization algorithm for improving the accuracy of low-precision neural network quantization. The proposed method achieves consistent improvements over state-of-the-art quantization-aware training methods for different low-precision networks, and in particular it improves the top-1 accuracy of 2-bit MobileNetV2 and MobileNetV3-Small by 3.9% and 4.9%, respectively, on ImageNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_Low-Precision_Network_Quantization_via_Bin_Regularization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_Low-Precision_Network_Quantization_via_Bin_Regularization/#content","title":"Content","text":"<p> Model quantization is an important mechanism for energy-efficient deployment of deep neural networks on resource-constrained devices by reducing the bit precision of weights and activations. However, it remains challenging to maintain high accuracy as bit precision decreases, especially for low-precision networks (e.g., 2-bit MobileNetV2). In this work, a novel weight regularization algorithm for improving low-precision network quantization is proposed. Experiments demonstrate that the proposed method achieves consistent improvements over the state-of-the-art quantization-aware training methods for different low-precision networks. Particularly, the proposed bin regularization improves LSQ for 2-bit MobileNetV2 and MobileNetV3-Small by 3.9% and 4.9% top-1 accuracy on ImageNet, respectively."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_adversarial_robustness_in_weight-quantized_neural_networks/","title":"Improving adversarial robustness in weight-quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_adversarial_robustness_in_weight-quantized_neural_networks/#summary","title":"Summary","text":"<p>This paper proposes a boundary-based retraining method to mitigate adversarial and quantization losses in neural networks. The method adopts a nonlinear mapping approach to defend against white-box gradient-based adversarial attacks and restores accuracy better than other baseline methods, especially after quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_adversarial_robustness_in_weight-quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_adversarial_robustness_in_weight-quantized_neural_networks/#content","title":"Content","text":"<p>Neural networks, whether full-precision or quantized, are vulnerable to adversarial attacks. In this paper, we propose a boundary-based retraining method to mitigate adversarial and quantization losses and adopt a nonlinear mapping method to defend against white-box gradient-based adversarial attacks. Our method can better restore accuracy after quantization than other baseline methods on both black-box and white-box adversarial attacks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_distribution_and_flexible_quantization_for_DCT_coefficients/","title":"Improving distribution and flexible quantization for DCT coefficients","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_distribution_and_flexible_quantization_for_DCT_coefficients/#summary","title":"Summary","text":"<p>This paper discusses the use of exponential power distributions (EPD) to model discrete cosine transform (DCT) coefficients in image compression. It also proposes a method for predicting distributions for DCT coefficients based on already decoded coefficients in neighboring blocks, and a quantization approach using optimized continuous quantization density functions for flexible and inexpensive non-uniform quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_distribution_and_flexible_quantization_for_DCT_coefficients/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_distribution_and_flexible_quantization_for_DCT_coefficients/#content","title":"Content","text":"<p>While it is a common knowledge that AC coefficients of Fourier-related transforms, like DCT-II of JPEG image compression, are from Laplace distribution, there was tested more general EPD (exponential power distribution) family. There is also discussed predicting distributions (as parameters) for DCT coefficients from already decoded coefficients in the current and neighboring DCT blocks. Especially for such continuous distributions, there is also discussed quantization approach through optimized continuous quantization density function q, which allows for flexible inexpensive choice of optimized (non-uniform) quantization, with rate-distortion control."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_neural_network_efficiency_via_post-training_quantization_with_adaptive_floating-point/","title":"Improving neural network efficiency via post-training quantization with adaptive floating-point","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_neural_network_efficiency_via_post-training_quantization_with_adaptive_floating-point/#summary","title":"Summary","text":"<p>The paper proposes a novel Adaptive Floating-Point (AFP) format for model quantization that offers a flexible configuration of exponent and mantissa segments, enabling significant model compression rates without sacrificing accuracy. Their experiments reveal that the AFP-encoded ResNet-50/MobileNet-v2 only suffers a small accuracy degradation of ~0.04/0.6% compared to its full-precision counterpart while outperforming the state-of-the-art works by 1.1% in accuracy with the same bit-width and reducing energy consumption by 11.2\u00d7, making it an impressive technique for inference. The authors also highlight that their proposed AFP format effectively eliminates the computationally intensive de-quantization step present in the dynamic quantization technique adopted by popular machine learning frameworks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_neural_network_efficiency_via_post-training_quantization_with_adaptive_floating-point/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_neural_network_efficiency_via_post-training_quantization_with_adaptive_floating-point/#content","title":"Content","text":"<p>Model quantization has emerged as a mandatory technique for efficient inference with advanced Deep Neural Networks (DNN) by representing model parameters with fewer bits. In this work, we propose a novel Adaptive Floating-Point (AFP) as a variant of standard IEEE-754 floating-point format, with flexible configuration of exponent and mantissa segments. Leveraging the AFP for model quantization could significantly enhance the model compression rate without accuracy degradation and model re-training. We also want to highlight that our proposed AFP could effectively eliminate the computationally intensive de-quantization step existing in the dynamic quantization technique adopted by the famous machine learning frameworks. Furthermore, we develop a framework to optimize and choose the adequate AFP configuration for each layer, thus maximizing the compression efficacy. Our experiments indicate that AFP-encoded ResNet-50/MobileNet-v2 only has ~0.04/0.6% accuracy degradation w.r.t its full-precision counterpart. It outperforms the state-of-the-art works by 1.1% in accuracy using the same bit-width while reducing the energy consumption by 11.2\u00d7, which is quite impressive for inference."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_post_training_neural_quantization%3A_Layer-wise_calibration_and_integer_programming/","title":"Improving post training neural quantization: Layer-wise calibration and integer programming","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_post_training_neural_quantization%3A_Layer-wise_calibration_and_integer_programming/#summary","title":"Summary","text":"<p>Summary: The article discusses post-training quantization methods that have gained attention due to their simplicity and requirement of a small unlabeled calibration set. However, such methods result in accuracy degradation below 8-bits. The article proposes minimizing quantization errors of each layer separately by optimizing its parameters over the calibration set, which is less susceptible to over-fitting and more powerful than previous methods. The article also suggests optimal allocation of bit-widths for each layer and model global statistics tuning to correct biases introduced during quantization, resulting in state-of-the-art results for both vision and text models, with less than 1% accuracy degradation on ResNet50 with 4-bit weights and activations in all layers except the smallest two.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_post_training_neural_quantization%3A_Layer-wise_calibration_and_integer_programming/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Improving_post_training_neural_quantization%3A_Layer-wise_calibration_and_integer_programming/#content","title":"Content","text":"<p>Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to \ufb01ne-tune the model without signi\ufb01cant over-\ufb01tting. Instead, these methods only use the calibration set to set the activations\u2019 dynamic ranges. However, such methods always resulted in signi\ufb01cant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set. We empirically demonstrate that this approach is: (1) much less susceptible to over-\ufb01tting than the standard \ufb01ne-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations\u2019 dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1% accuracy degradation \u2014 with 4-bit weights and activations in all layers, but the smallest two. We open sourced our code."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/In-Hindsight_Quantization_Range_Estimation_for_Quantized_Training/","title":"In-Hindsight Quantization Range Estimation for Quantized Training","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/In-Hindsight_Quantization_Range_Estimation_for_Quantized_Training/#summary","title":"Summary","text":"<p>This paper discusses how quantization of neural networks is an effective technique for faster and efficient execution on low-resource devices. The paper further explores the effectiveness of fully quantized training, such as quantizing backpropagation, and identifies effective gradient quantization as an open problem. The authors propose an alternative to dynamic quantization, called in-hindsight range estimation, which enables fast static quantization of gradients and activations, requiring minimal hardware support, and can be used with other advances in quantized training. The authors compare their method to existing ones and demonstrate its effectiveness with various architectures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/In-Hindsight_Quantization_Range_Estimation_for_Quantized_Training/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/In-Hindsight_Quantization_Range_Estimation_for_Quantized_Training/#content","title":"Content","text":"<p>Quantization techniques applied to the inference of deep neural networks have enabled fast and ef\ufb01cient execution on resource-constraint devices. The success of quantiza- tion during inference has motivated the academic commu- nity to explore fully quantized training, i.e. quantizing back- propagation as well. However, effective gradient quanti- zation is still an open problem. Gradients are unbounded and their distribution changes signi\ufb01cantly during training, which leads to the need for dynamic quantization. As we show, dynamic quantization can lead to signi\ufb01cant memory overhead and additional data traf\ufb01c slowing down train- ing. We propose a simple alternative to dynamic quanti- zation, in-hindsight range estimation, that uses the quanti- zation ranges estimated on previous iterations to quantize the present. Our approach enables fast static quantization of gradients and activations while requiring only minimal hardware support from the neural network accelerator to keep track of output statistics in an online fashion. It is in- tended as a drop-in replacement for estimating quantization ranges and can be used in conjunction with other advances in quantized training. We compare our method to existing methods for range estimation from the quantized training literature and demonstrate its effectiveness with a range of architectures, including MobileNetV2, on image classi\ufb01ca- tion benchmarks (Tiny ImageNet &amp; ImageNet)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_embedding_learning_via_zero-shot_translation/","title":"Incremental embedding learning via zero-shot translation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_embedding_learning_via_zero-shot_translation/#summary","title":"Summary","text":"<p>Summary: The paper proposes the use of an incremental learning method called ZSTCI to solve catastrophic forgetting in embedding networks for tasks such as face recognition and zero-shot learning. ZSTCI uses zero-shot translation to accurately estimate the semantic gap between two adjacent tasks. Experimental results on CUB-200-2011 and CIFAR100 datasets show the effectiveness of the proposed method.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_embedding_learning_via_zero-shot_translation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_embedding_learning_via_zero-shot_translation/#content","title":"Content","text":"<p>Modern deep learning methods have achieved great success in machine learning and computer vision fields by learning a set of pre-defined datasets. However, when these methods are applied to the real-world scenarios, the trained model quickly forgets the knowledge of old tasks due to the learning of new tasks, which is known as catastrophic forgetting. In this paper, the authors propose an incremental learning method called zero-shot translation class-incremental method (ZSTCI) to tackle the catastrophic forgetting problem in embedding networks, which are used in tasks such as image retrieval, face recognition, and zero-shot learning. ZSTCI leverages zero-shot translation to estimate the semantic gap between the embedding spaces of two adjacent tasks accurately. The authors conduct experiments on CUB-200-2011 and CIFAR100 datasets and prove the effectiveness of their method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_few-shot_learning_via_vector_quantization_in_deep_embedded_space/","title":"Incremental few-shot learning via vector quantization in deep embedded space","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_few-shot_learning_via_vector_quantization_in_deep_embedded_space/#summary","title":"Summary","text":"<p> This paper proposes a nonparametric method in deep embedded space to solve the challenge of incremental few-shot learning, where novel tasks have very few labelled training samples. The proposed method adds more reference vectors to the model using few-shot samples in each task, and outperforms other state-of-the-art methods according to experimental results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_few-shot_learning_via_vector_quantization_in_deep_embedded_space/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_few-shot_learning_via_vector_quantization_in_deep_embedded_space/#content","title":"Content","text":"<p>The capability of incremental few-shot learning is a challenging problem due to catastrophic forgetting. This challenge becomes greater when novel tasks contain very few labelled training samples. In this study, we propose a nonparametric method in deep embedded space to tackle incremental few-shot learning problems. The proposed method learns new tasks sequentially by adding more reference vectors to the model using few-shot samples in each novel task. Experimental results demonstrate that the proposed method outperforms other state-of-the-art methods in incremental learning."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_network_quantization%3A_Towards_lossless_cnns_with_low-precision_weights/","title":"Incremental network quantization: Towards lossless cnns with low-precision weights","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_network_quantization%3A_Towards_lossless_cnns_with_low-precision_weights/#summary","title":"Summary","text":"<p>Summary: This paper proposes a novel method called incremental network quantization (INQ) to convert pre-trained full-precision CNNs into low-precision versions with weights constrained to powers of two or zero. Iterative weight partition, group-wise quantization and re-training are used to incrementally enhance accuracy, resulting in similar or better accuracy than baseline models with 4-bit, 3-bit and 2-bit ternary weights. The method shows promise for deep CNNs on mobile or embedded devices.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_network_quantization%3A_Towards_lossless_cnns_with_low-precision_weights/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Incremental_network_quantization%3A_Towards_lossless_cnns_with_low-precision_weights/#content","title":"Content","text":"<p> This paper proposes incremental network quantization (INQ), a novel method for efficiently converting pre-trained full-precision convolutional neural networks (CNNs) into low-precision versions with weights constrained to powers of two or zero. Unlike existing methods that suffer a noticeable loss in accuracy, INQ employs three interdependent operations: weight partition, group-wise quantization, and re-training. A well-proven measure is used to divide pre-trained CNN model weights into two groups, where the first group forms a low-precision base and the second compensates for accuracy loss through quantization and is re-trained. These operations are iteratively repeated on the latest re-trained group until all the weights are converted into low-precision ones, incrementally enhancing accuracy. Extensive experiments testify to the efficacy of the proposed method on the ImageNet classification task using popular deep CNN architectures such as AlexNet, VGG-16, GoogleNet, and ResNets. Quantized models with 4-bit, 3-bit, and 2-bit ternary weights achieve similar or better accuracy than their 32-bit floating-point baseline. Moreover, the combination of network pruning and INQ further enhances performance. The proposed method sheds new light on how to apply deep CNNs to mobile or embedded devices."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Inferring_context_from_pixels_for_multimodal_image_classification/","title":"Inferring context from pixels for multimodal image classification","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Inferring_context_from_pixels_for_multimodal_image_classification/#summary","title":"Summary","text":"<p>This work proposes a framework for image classification models that consists of a phrase generator that maps image pixels to contextual phrases, and a multimodal model that uses both textual and visual features to produce labels. This approach shows improved performance and prediction interpretability on diverse benchmark datasets, including few-shot learning with minimally labeled concepts and baseline zero-shot learning on the ImageNet dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Inferring_context_from_pixels_for_multimodal_image_classification/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Inferring_context_from_pixels_for_multimodal_image_classification/#content","title":"Content","text":"<p>Image classification models take image pixels as input and predict labels in a predefined taxonomy. While contextual information (e.g. text surrounding an image) can provide valuable orthogonal signals to improve classification, the typical setting in literature assumes the unavailability of text and thus focuses on models that rely purely on pixels. In this work, we also focus on the setting where only pixels are available in the input. However, we demonstrate that if we predict textual information from pixels, we can subsequently use the predicted text to train models that improve overall performance. We propose a framework that consists of two main components: (1) a phrase generator that maps image pixels to a contextual phrase, and (2) a multimodal model that uses textual features from the phrase generator and visual features from the image pixels to produce labels in the output taxonomy. We evaluate our framework on diverse benchmark datasets (specifically, the WebVision dataset for evaluating multi-class classification and OpenImages dataset for evaluating multi-label classification), demonstrating performance improvements over approaches based exclusively on pixels and showcasing benefits in prediction interpretability. We additionally present results to demonstrate that our framework provides improvements in few-shot learning of minimally labeled concepts. We further demonstrate the unique benefits of the multimodal nature of our framework by utilizing intermediate image/text co-embeddings to perform baseline zero-shot learning on the ImageNet dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Information_Bottleneck%3A_Exact_Analysis_of_%28Quantized%29_Neural_Networks/","title":"Information Bottleneck: Exact Analysis of (Quantized) Neural Networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Information_Bottleneck%3A_Exact_Analysis_of_%28Quantized%29_Neural_Networks/#summary","title":"Summary","text":"<p> The study examines the impact of the information bottleneck principle on deep neural networks through mutual information between hidden layers and input and output. Different methods of binning result in divergent outcomes. The study monitors the dynamics of quantized neural networks, with exact computation of mutual information, to measure information flow without measurement errors. The study confirms initial IB results without any artifacts of binning, with the comprehension phase depending on the type of activation function, but the compression phase might not be seen in certain networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Information_Bottleneck%3A_Exact_Analysis_of_%28Quantized%29_Neural_Networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Information_Bottleneck%3A_Exact_Analysis_of_%28Quantized%29_Neural_Networks/#content","title":"Content","text":"<p>The information bottleneck (IB) principle has been suggested as a way to analyze deep neural networks. The learning dynamics are studied by inspecting the mutual information (MI) between the hidden layers and the input and output. Our study confirms that different ways of binning when computing the MI lead to qualitatively different results, either supporting or refusing IB conjectures. To resolve the controversy, we study the IB principle in settings where MI is non-trivial and can be computed exactly. We monitor the dynamics of quantized neural networks, that is, we discretize the whole deep learning system so that no approximation is required when computing the MI. This allows us to quantify the information flow without measurement errors. In this setting, we observed a fitting phase for all layers and a compression phase for the output layer in all experiments; the compression in the hidden layers was dependent on the type of activation function. Our study shows that the initial IB results were not artifacts of binning when computing the MI. However, the critical claim that the compression phase may not be observed for some networks also holds true."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Input-to-state_stabilization_in_probability_for_nonlinear_stochastic_systems_under_quantization_effects_and_communication_protocols/","title":"Input-to-state stabilization in probability for nonlinear stochastic systems under quantization effects and communication protocols","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Input-to-state_stabilization_in_probability_for_nonlinear_stochastic_systems_under_quantization_effects_and_communication_protocols/#summary","title":"Summary","text":"<p>Summary: The paper investigates the observer-based stabilization for a class of discrete-time nonlinear stochastic networked control systems with exogenous disturbances. The signal transmission is via a shared digital network, which considers both uniform quantization effects and stochastic communication protocols to reflect network-induced constraints. The input-to-state stability probability is introduced to characterize the dynamical behaviors of closed-loop stochastic NCS, establishing a theoretical framework to analyze the dynamics of the closed-loop system. The conditions for an observer-based controller with explicit expression of gain matrices are established, and the effectiveness is demonstrated through numerical simulation examples.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Input-to-state_stabilization_in_probability_for_nonlinear_stochastic_systems_under_quantization_effects_and_communication_protocols/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Input-to-state_stabilization_in_probability_for_nonlinear_stochastic_systems_under_quantization_effects_and_communication_protocols/#content","title":"Content","text":"<p>In this paper, the observer-based stabilization problem is investigated for a class of discrete-time nonlinear stochastic networked control systems (NCSs) with exogenousdisturbances. The signal transmission from the sensors to theobserver is implemented via a shared digital network, in whichboth uniform quantization effect and stochastic communicationprotocol (SCP) are taken into account to re\ufb02ect several network-induced constraints. The notion of input-to-state stability inprobability is introduced to describe the dynamical behaviorsof the closed-loop stochastic NCS that is effectively character-ized by a general nonlinear stochastic difference equation withMarkovian jumping parameters. A theoretical framework is \ufb01rstestablished to felicitate the dynamics analysis of the closed-loopsystem in virtue of the switched Lyapunov function method andthe stochastic analysis techniques. By making full use of the quan-tized measurement output under the scheduling of the SCP, theexistence conditions for an observer-based controller are estab-lished under which the closed-loop system is input-to-state stablein probability. Then, the explicit expression of the gain matricesof the desired controller is given by resorting to a set of feasi-ble solutions of certain matrix inequalities. The effectiveness of the theoretical results is demonstrated by a numerical simulation example."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Instance-aware_dynamic_neural_network_quantization/","title":"Instance-aware dynamic neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Instance-aware_dynamic_neural_network_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a dynamic quantization scheme to optimize bit-widths for each image individually by establishing a lightweight bit-controller trained with the neural network. Results show that this method improves performance and computational complexity without altering the network architecture.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Instance-aware_dynamic_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Instance-aware_dynamic_neural_network_quantization/#content","title":"Content","text":"<p>Quantization is a method to reduce the memory and computational costs of deep neural networks. However, the bit-width for each layer in most existing quantization methods is static, whereas natural images have diverse content, and using a universal quantization configuration for all samples is not optimal. To address this issue, we propose to conduct the low-bit quantization for each image individually, and develop a dynamic quantization scheme for exploring their optimal bit-widths. We establish a lightweight bit-controller, which is trained jointly with the given neural network to be quantized. During inference, the quantization configuration for an arbitrary image is determined by the bit-widths generated by the controller. Experimental results show that the proposed dynamic quantization method achieves state-of-art performance in terms of accuracy and computational complexity, without changing the network architecture."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Integer-arithmetic-only_Certified_Robustness_for_Quantized_Neural_Networks/","title":"Integer-arithmetic-only Certified Robustness for Quantized Neural Networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Integer-arithmetic-only_Certified_Robustness_for_Quantized_Neural_Networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes an integer randomized smoothing approach with quantization for certified robustness against adversarial perturbations that uses only integer arithmetic to overcome computational and memory challenges. The approach shows comparable accuracy and speedup compared to floating-point arithmetic certified robust methods on general-purpose CPUs and mobile devices on two datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Integer-arithmetic-only_Certified_Robustness_for_Quantized_Neural_Networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Integer-arithmetic-only_Certified_Robustness_for_Quantized_Neural_Networks/#content","title":"Content","text":"<p> Adversarial data examples have drawn significant attention from the machine learning and security communities. A line of work on tackling adversarial examples is certified robustness via randomized smoothing that can provide a theoretical robustness guarantee. However, such a mechanism usually uses floating-point arithmetic for calculations in inference and requires large memory footprints and daunting computational costs. To overcome these challenges, we propose an integer randomized smoothing approach with quantization to convert any classifier into a new smoothed classifier, which uses integer-only arithmetic for certified robustness against adversarial perturbations. We prove a tight robustness guarantee under \u21132-norm for the proposed approach. We show our approach can obtain a comparable accuracy and 4\u00d7 \u223c 5\u00d7 speedup over floating-point arithmetic certified robust methods on general-purpose CPUs and mobile devices on two distinct datasets (CIFAR-10 and Caltech-101)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Intraq%3A_Learning_synthetic_images_with_intra-class_heterogeneity_for_zero-shot_network_quantization/","title":"Intraq: Learning synthetic images with intra-class heterogeneity for zero-shot network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Intraq%3A_Learning_synthetic_images_with_intra-class_heterogeneity_for_zero-shot_network_quantization/#summary","title":"Summary","text":"<p>The paper introduces a new zero-shot quantization (ZSQ) method called IntraQ to address the issue where existing ZSQ methods fail to preserve intra-class heterogeneity in synthetic images, limiting performance improvement. The proposed method utilizes data synthesis to represent neural networks using low-bit integers without accessing real data."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Intraq%3A_Learning_synthetic_images_with_intra-class_heterogeneity_for_zero-shot_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Intraq%3A_Learning_synthetic_images_with_intra-class_heterogeneity_for_zero-shot_network_quantization/#content","title":"Content","text":"<p>Learning to synthesize data has emerged as a promising direction in zero-shot quantization (ZSQ), which represents neural networks by low-bit integer without accessing any of the real data. In this paper, we observe an interesting phenomenon of intra-class heterogeneity in real data and show that existing methods fail to retain this property in their synthetic images, which causes a limited performance increase. To address this issue, we propose a novel zero-shot quantization method referred to as IntraQ."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Is_in-domain_data_really_needed%3F_A_pilot_study_on_cross-domain_calibration_for_network_quantization/","title":"Is in-domain data really needed? A pilot study on cross-domain calibration for network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Is_in-domain_data_really_needed%3F_A_pilot_study_on_cross-domain_calibration_for_network_quantization/#summary","title":"Summary","text":"<p>This paper explores the possibility of using out-of-domain data to calibrate trained networks for post-training quantization. The results show that cross-domain calibration can result in stable performance of quantized models on multiple tasks in different image domains, suggesting that cross-domain knowledge can be borrowed for network quantization and compression."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Is_in-domain_data_really_needed%3F_A_pilot_study_on_cross-domain_calibration_for_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Is_in-domain_data_really_needed%3F_A_pilot_study_on_cross-domain_calibration_for_network_quantization/#content","title":"Content","text":"<p> Post-training quantization methods use a set of calibration data to compute quantization ranges for network parameters and activations. The calibration data usually comes from the training dataset which could be inaccessible due to sensitivity of the data. In this work, we want to study such a problem: can we use out-of-domain data to calibrate the trained networks without knowledge of the original dataset? We find cross-domain calibration leads to surprisingly stable performance of quantized models on 10 tasks in different image domains with 13 different calibration datasets. We believe our research opens the door to borrow cross-domain knowledge for network quantization and compression."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Isometric_propagation_network_for_generalized_zero-shot_learning/","title":"Isometric propagation network for generalized zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Isometric_propagation_network_for_generalized_zero-shot_learning/#summary","title":"Summary","text":"<p>  This paper proposes a new approach called the Isometric Propagation Network (IPN) for zero-shot learning, which leverages class representations and auto-generated graphs to align the semantic space of class attributes with the visual space of images. The IPN approach regularizes the two dynamic propagation procedures to be isometric by minimizing a consistency loss between them. The proposed approach achieves state-of-the-art performance on three popular ZSL benchmarks and provides superior results on two larger benchmarks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Isometric_propagation_network_for_generalized_zero-shot_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Isometric_propagation_network_for_generalized_zero-shot_learning/#content","title":"Content","text":"<p> Zero-shot learning (ZSL) is the process of classifying images of an unseen class based on a few attributes describing that class, without any access to training samples. The primary challenge with ZSL is aligning the representations in the semantic space of class attributes with the visual space of images. To address this issue, we introduce the Isometric Propagation Network (IPN), which propagates class representations on an auto-generated graph within each space. This approach strengthens the relation between classes within each space and aligns the class dependency in the two spaces. We propose regularizing the two dynamic propagation procedures to be isometric in terms of the two graphs\u2019 edge weights per step by minimizing a consistency loss between them. IPN achieves state-of-the-art performance on three popular ZSL benchmarks and provides superior results on two larger benchmarks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/It%27s_All_In_the_Teacher%3A_Zero-Shot_Quantization_Brought_Closer_to_the_Teacher/","title":"It's All In the Teacher: Zero-Shot Quantization Brought Closer to the Teacher","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/It%27s_All_In_the_Teacher%3A_Zero-Shot_Quantization_Brought_Closer_to_the_Teacher/#summary","title":"Summary","text":"<p>Summary: This paper focuses on model quantization, a technique used to reduce the resource requirements of deep neural networks, and proposes a new approach called AIT for zero-shot quantization. The method uses information from a full-precision teacher network to fine-tune the quantized network, addressing the difficulties of optimizing multiple loss terms and poor generalization capability due to synthetic samples of existing methods. Experiments show that AIT outperforms many existing methods and takes over the overall state-of-the-art position in the field.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/It%27s_All_In_the_Teacher%3A_Zero-Shot_Quantization_Brought_Closer_to_the_Teacher/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/It%27s_All_In_the_Teacher%3A_Zero-Shot_Quantization_Brought_Closer_to_the_Teacher/#content","title":"Content","text":"<p>Model quantization is considered as a promising method to greatly reduce the resource requirements of deep neural networks. To deal with the performance drop induced by quantization errors, a popular method is to use training data to fine-tune quantized networks. Zero-shot quantization addresses such problems, usually by taking information from the weights of a full-precision teacher network to compensate the performance drop of the quantized networks. This paper analyzes the loss surface of state-of-the-art zero-shot quantization techniques and provides several findings. In contrast to usual knowledge distillation problems, zero-shot quantization often suffers from 1) the difficulty of optimizing multiple loss terms together, and 2) the poor generalization capability due to the use of synthetic samples. Based on the observations, the paper proposes AIT, a simple yet powerful technique for zero-shot quantization, which addresses the aforementioned two problems. Experiments show that AIT outperforms the performance of many existing methods by a great margin, taking over the overall state-of-the-art position in the field."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Iterative_quantization%3A_A_procrustean_approach_to_learning_binary_codes_for_large-scale_image_retrieval/","title":"Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Iterative_quantization%3A_A_procrustean_approach_to_learning_binary_codes_for_large-scale_image_retrieval/#summary","title":"Summary","text":"<p>Summary: The paper proposes an iterative quantization algorithm for learning binary codes for image retrieval that minimizes quantization error. The binary codes outperform other state-of-the-art methods and performance improvements can result from transformation of data with a nonlinear kernel mapping prior to PCA or CCA. The algorithm is demonstrated on the ImageNet dataset for learning binary attributes or classemes.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Iterative_quantization%3A_A_procrustean_approach_to_learning_binary_codes_for_large-scale_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Iterative_quantization%3A_A_procrustean_approach_to_learning_binary_codes_for_large-scale_image_retrieval/#content","title":"Content","text":"<p> This paper proposes a method for learning binary codes for large-scale image retrieval. The proposed method aims to find a rotation of zero-centered data to minimize the quantization error of mapping the data to the vertices of a zero-centered binary hypercube. An alternating minimization algorithm, called iterative quantization (ITQ), is proposed to achieve this. The resulting binary codes outperform several other state-of-the-art methods. The paper also suggests that performance improvements can result from transformation of data with a nonlinear kernel mapping prior to PCA or CCA. Finally, an application of ITQ to learning binary attributes or \"classemes\" on the ImageNet dataset is demonstrated."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_learning_of_deep_retrieval_model_and_product_quantization_based_embedding_index/","title":"Joint learning of deep retrieval model and product quantization based embedding index","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_learning_of_deep_retrieval_model_and_product_quantization_based_embedding_index/#summary","title":"Summary","text":"<p>The paper presents a novel method called Poeem for embedding index in deep retrieval systems. It unifies the embedding learning and index building steps by utilizing gradient straight-through estimator, warm start strategy, optimal space decomposition and Givens rotation techniques in end-to-end training. The proposed method significantly improves retrieval accuracy and reduces indexing time to nearly zero, according to the experimental results. The approach is open-sourced for reproducibility and comparison purposes."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_learning_of_deep_retrieval_model_and_product_quantization_based_embedding_index/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_learning_of_deep_retrieval_model_and_product_quantization_based_embedding_index/#content","title":"Content","text":"<p>Embedding index that enables fast approximate nearest neighbor (ANN) search, serves as an indispensable component for state-of-the-art deep retrieval systems. Traditional approaches, often separating the two steps of embedding learning and index building, incur additional indexing time and decayed retrieval accuracy. In this paper, we propose a novel method called Poeem, which stands for product quantization based embedding index jointly trained with deep retrieval model, to unify the two separate steps within an end-to-end training, by utilizing a few techniques including the gradient straight-through estimator, warm start strategy, optimal space decomposition and Givens rotation. Extensive experimental results show that the proposed method not only improves retrieval accuracy significantly but also reduces the indexing time to almost none. We have open sourced our approach for the sake of comparison and reproducibility."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_neural_architecture_search_and_quantization/","title":"Joint neural architecture search and quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_neural_architecture_search_and_quantization/#summary","title":"Summary","text":"<p>Summary: This research paper presents a joint neural architecture search and quantization framework called JASQ, which uses a multi-objective evolutionary search algorithm to find compact neural network models suitable for mobile devices by balancing between model size and performance accuracy. The proposed method outperforms the methods that search only for architectures or only for quantization policies. Two models, JASQNet and JASQNet-Small, are obtained with joint search of architectures and quantization policies with 2.97% error rate and 0.9 MB on CIFAR-10.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_neural_architecture_search_and_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_neural_architecture_search_and_quantization/#content","title":"Content","text":"<p> In this research paper, the authors propose a joint neural architecture search and quantization framework, named JASQ, to enable the search for compact neural network models suitable for mobile devices. The method uses a multi-objective evolutionary search algorithm to find models that balance between model size and performance accuracy. The proposed approach outperforms the methods that search only for architectures or only for quantization policies. The experiments conducted show that under the balance between model size and performance accuracy, two models are obtained with joint search of architectures and quantization policies: a high-accuracy model and a small model, JASQNet and JASQNet-Small, that achieves 2.97% error rate with 0.9 MB on CIFAR-10."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_pruning_%26_quantization_for_extremely_sparse_neural_networks/","title":"Joint pruning &amp; quantization for extremely sparse neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_pruning_%26_quantization_for_extremely_sparse_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes a two-stage pruning and quantization pipeline for deep neural networks to achieve extreme sparsity for low cost and low power accelerator hardware. The pipeline includes a Taylor Score and a new fine-tuning mode, which alone showed superior results compared to the state-of-the-art when applied to ResNet on CIFAR10 and ImageNet. The pruning stage can cut up to 99% of memory demand and reduce hardware costs up to 99.9%."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_pruning_%26_quantization_for_extremely_sparse_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Joint_pruning_%26_quantization_for_extremely_sparse_neural_networks/#content","title":"Content","text":"<p> We investigate pruning and quantization for deep neural networks, specifically for stereo depth estimation, with the goal of achieving extreme sparsity for quantized networks to enable implementation on low cost and low power accelerator hardware. We propose a two-stage pruning and quantization pipeline with a Taylor Score and a new fine-tuning mode to achieve extreme sparsity without sacrificing performance. Our evaluation demonstrates that our pruning stage alone beats the state-of-the-art when applied to ResNet on CIFAR10 and ImageNet. Almost 99% of memory demand can be cut while hardware costs can be reduced up to 99.9%."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Knowledge_distillation_for_optimization_of_quantized_deep_neural_networks/","title":"Knowledge distillation for optimization of quantized deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Knowledge_distillation_for_optimization_of_quantized_deep_neural_networks/#summary","title":"Summary","text":"<p> This paper explores the optimization of quantized deep neural networks (QDNNs) using knowledge distillation (KD) technique with pre-trained teacher models. The experiments conducted in this study proved that the importance of the softmax distribution produced by the teacher network is more than its performance for effective KD training. Additionally, the paper introduces the Gradual Soft Loss Reducing (GSLR) technique for robust KD based QDNN optimization, which controls the mixing ratio of hard and soft losses during training. Section 2 describes how QDNN can be trained with KD and explains the importance of KD hyperparameters. Section 3 presents the experimental results, and Section 4 concludes the paper."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Knowledge_distillation_for_optimization_of_quantized_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Knowledge_distillation_for_optimization_of_quantized_deep_neural_networks/#content","title":"Content","text":"<p> Knowledge distillation (KD) technique is used for the optimization of quantized deep neural networks (QDNNs) by utilizing a pre-trained teacher model for training a student network. This paper explores several large floating-point models and quantized ones as the teacher and investigates the effect of hyperparameters for KD. The experiments show that the softmax distribution produced by the teacher network is more important than its performance for effective KD training. The paper proposes the gradual soft loss reducing (GSLR) technique for robust KD based QDNN optimization, which controls the mixing ratio of hard and soft losses during training. Section 2 describes how QDNN can be trained with KD and explains why the hyperparameters of KD are important. Section 3 shows the experimental results and Section 4 concludes the paper."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lagrange_structure_and_quantization/","title":"Lagrange structure and quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lagrange_structure_and_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a path-integral quantization method for dynamical systems that don't follow from the action principle, using the Lagrange structure which allows for a natural BRST description and the construction of an AKSZ-type topological sigma-model.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lagrange_structure_and_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lagrange_structure_and_quantization/#content","title":"Content","text":"<p>A path-integral quantization method is proposed for dynamical systems whose classical equations of motion do not necessarily follow from the action principle. The key new notion behind this quantization scheme is the Lagrange structure which is more general than the Lagrangian formalism in the same sense as Poisson geometry is more general than the symplectic one. The Lagrange structure is shown to admit a natural BRST description which is used to construct an AKSZ-type topological sigma-model."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lance%3A_efficient_low-precision_quantized_winograd_convolution_for_neural_networks_based_on_graphics_processing_units/","title":"Lance: efficient low-precision quantized winograd convolution for neural networks based on graphics processing units","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lance%3A_efficient_low-precision_quantized_winograd_convolution_for_neural_networks_based_on_graphics_processing_units/#summary","title":"Summary","text":"<p>Summary: This paper proposes an efficient low-precision quantized Winograd convolution algorithm, called LANCE, for accelerating deep convolutional neural networks. By combining the advantages of fast convolution and quantization techniques, LANCE performs efficiently in low-precision computation on graphics processing units. LANCE is tested on representative image classification datasets and shows up to 2.40 improvements in performance over full-precision convolution with trivial accuracy loss.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lance%3A_efficient_low-precision_quantized_winograd_convolution_for_neural_networks_based_on_graphics_processing_units/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lance%3A_efficient_low-precision_quantized_winograd_convolution_for_neural_networks_based_on_graphics_processing_units/#content","title":"Content","text":"<p>Accelerating deep convolutional neural networks has become an active topic and sparked an interest in academia and industry. In this paper, we propose an efficient low-precision quantized Winograd convolution algorithm, called LANCE, which combines the advantages of fast convolution and quantization techniques. By embedding linear quantization operations into the Winograd-domain, the fast convolution can be performed efficiently under low-precision computation on graphics processing units. We test neural network models with LANCE on representative image classification datasets, including SVHN, CIFAR, and ImageNet. The experimental results show that our 8-bit quantized Winograd convolution improves the performance by up to 2.40 over the full-precision convolution with trivial accuracy loss."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Large-scale_JPEG_image_steganalysis_using_hybrid_deep-learning_framework/","title":"Large-scale JPEG image steganalysis using hybrid deep-learning framework","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Large-scale_JPEG_image_steganalysis_using_hybrid_deep-learning_framework/#summary","title":"Summary","text":"<p>The paper proposes a hybrid deep-learning framework for JPEG steganalysis which involves hand-crafted and deep neural network stages to detect steganography. The integration of quantization and truncation into deep-learning steganalyzers boosts detection performance and is insensitive to JPEG blocking artifact alterations. The learned model can be easily transferred to different attacking targets and datasets, making it suitable for practical applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Large-scale_JPEG_image_steganalysis_using_hybrid_deep-learning_framework/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Large-scale_JPEG_image_steganalysis_using_hybrid_deep-learning_framework/#content","title":"Content","text":"<p> Adoption of deep learning in image steganalysis is still in its initial stage. In this paper we propose a generic hybrid deep-learning framework for JPEG steganalysis incorporating the domain knowledge behind rich steganalytic models. Our proposed framework involves two main stages. The \ufb01rst stage is hand-crafted, corresponding to the convolution phase and the quantization &amp; truncation phase of the rich models. The second stage is a compound deep neural network containing multiple deep subnets in which the model parameters are learned in the training procedure. We provided experimental evidences and theoretical reflections to argue that the integration of quantization and truncation into deep-learning steganalyzers do boost the detection performance by a clear margin. Furthermore, we demonstrate that our framework is insensitive to JPEG blocking artifact alterations, and the learned model can be easily transferred to a di \u000berent attacking target and even a different dataset. These properties are of critical importance in practical applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Large_scale_metric_learning_for_distance-based_image_classification/","title":"Large scale metric learning for distance-based image classification","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Large_scale_metric_learning_for_distance-based_image_classification/#summary","title":"Summary","text":"<p>Summary: The paper presents a new metric learning method for distance-based image classification that performs well on large datasets and outperforms other state-of-the-art methods. The proposed method is tested on several datasets and shows potential for real-world applications.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Large_scale_metric_learning_for_distance-based_image_classification/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Large_scale_metric_learning_for_distance-based_image_classification/#content","title":"Content","text":"<p>Large Scale Metric Learning for Distance-Based Image Classification is the topic of this research report. The focus is on finding a way to identify images through the use of metrics that take into account the distances between points. The authors present a new metric learning method that works well for large scale datasets, and show how it can be used in a distance-based image classification system. The system is tested on several datasets and the results show that the proposed method outperforms other state-of-the-art methods. The authors conclude by discussing the potential applications of such a method in real-world settings."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Layer-wise_data-free_cnn_compression/","title":"Layer-wise data-free cnn compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Layer-wise_data-free_cnn_compression/#summary","title":"Summary","text":"<p>Summary: The paper presents a method for compressing a pre-trained neural network without utilizing real data. They propose independent layer-wise compressions and generate training data using a pre-trained network. The method shows higher accuracy compared to existing works while utilizing much less compute for quantization compression. For pruning, the proposed method outperformed baselines with 1.5 times the sparsity rate at the same accuracy. Finally, they show how to combine their method with high-compute generative methods to achieve better results.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Layer-wise_data-free_cnn_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Layer-wise_data-free_cnn_compression/#content","title":"Content","text":"<p>We present a computationally efficient method for compressing a trained neural network without using real data. We break the problem of data-free network compression into independent layer-wise compressions. We show how to efficiently generate layer-wise training data using only a pretrained network. We use this data to perform independent layer-wise compressions on the pretrained network. We also show how to precondition the network to improve the accuracy of our layer-wise compression method. We present results for layer-wise compression using quantization and pruning. When quantizing, we compress with higher accuracy than related works while using orders of magnitude less compute. When compressing MobileNetV2 and evaluating on ImageNet, our method outperforms existing methods for quantization at all bit-widths, achieving a+0:34% improvement in 8-bit quantization, and a stronger improvement at lower bit-widths (up to a +28:50% improvement at 5 bits). When pruning, we outperform baselines of a similar compute envelope, achieving 1:5 times the sparsity rate at the same accuracy. We also show how to combine our efficient method with high-compute generative methods to improve upon their results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Layer_importance_estimation_with_imprinting_for_neural_network_quantization/","title":"Layer importance estimation with imprinting for neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Layer_importance_estimation_with_imprinting_for_neural_network_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes an accuracy-aware criterion for quantization of neural networks instead of using fixed low bit-width representation. It proposes mixed-precision quantization on a per-layer basis that requires careful tuning to maintain accuracy while achieving further compression and higher granularity. Additionally, the method suggests the use of imprinting per layer that acts as a proxy module for accuracy estimation in an efficient way.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Layer_importance_estimation_with_imprinting_for_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Layer_importance_estimation_with_imprinting_for_neural_network_quantization/#content","title":"Content","text":"<p> Neural network quantization has achieved a high compression rate using fixed low bit-width representation of weights and activations while maintaining the accuracy of the high-precision original network. However, mixed precision (per-layer bit-width precision) quantization requires careful tuning to maintain accuracy while achieving further compression and higher granularity than fixed-precision quantization. We propose an accuracy-aware criterion to quantify the layer's importance rank. Our method applies imprinting per layer which acts as a proxy module for accuracy estimation in an efficient way."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lcdet%3A_Low-complexity_fully-convolutional_neural_networks_for_object_detection_in_embedded_systems/","title":"Lcdet: Low-complexity fully-convolutional neural networks for object detection in embedded systems","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lcdet%3A_Low-complexity_fully-convolutional_neural_networks_for_object_detection_in_embedded_systems/#summary","title":"Summary","text":"<p>Summary: The paper proposes a fully-convolutional neural network called LCDet for object detection that is designed to work in embedded systems. It achieves similar accuracy to state-of-the-art CNN-based face detection methods while reducing model size by 3\u00d7 and memory-BW by 3\u22124\u00d7 compared to YOLO. Evaluation was done on publicly available datasets FDDB and Widerface.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lcdet%3A_Low-complexity_fully-convolutional_neural_networks_for_object_detection_in_embedded_systems/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lcdet%3A_Low-complexity_fully-convolutional_neural_networks_for_object_detection_in_embedded_systems/#content","title":"Content","text":"<p> Deep Convolutional Neural Networks (CNN) are the state-of-the-art performers for the object detection task. In this work, we propose LCDet , a fully-convolutional neural network for generic object detection that aims to work in em- bedded systems. Our experimental results show that the proposed method achieves comparative accuracy comparing with state-of- -the-art CNN-based face detection methods while reducing the model size by 3\u00d7and memory-BW by 3\u22124\u00d7compar- ing with one of the best real-time CNN-based object de- tector YOLO [23]. We evaluate the face detection perfor- mance on publicly available dataset FDDB and Widerface."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_Lookup_Table_for_Neural_Network_Quantization/","title":"Learnable Lookup Table for Neural Network Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_Lookup_Table_for_Neural_Network_Quantization/#summary","title":"Summary","text":"<p>Summary: This paper proposes a new method for neural network quantization using learnable lookup tables (LLTs), which reduces computational costs in the activation quantization process. The proposed method achieves state-of-the-art performance in various tasks and outperforms existing quantization methods that use pre-defined functions for quantizers.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_Lookup_Table_for_Neural_Network_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_Lookup_Table_for_Neural_Network_Quantization/#content","title":"Content","text":"<p>Neural network quantization is widely used to reduce the bit-width of weights and activations for efficient memory and computation. However, existing methods with pre-defined functions for quantizers introduce considerable computational overhead in the activation quantization process. This paper proposes the use of learnable lookup tables (LLTs) as quantizers to reduce computational costs. The quantization process is formulated as a simple lookup operation, and LLTs are trained in an end-to-end manner to fit distributions in different layers. The proposed method achieves state-of-the-art performance in image classification, image super-resolution, and point cloud classification tasks when compared with previous methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_companding_quantization_for_accurate_low-bit_neural_networks/","title":"Learnable companding quantization for accurate low-bit neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_companding_quantization_for_accurate_low-bit_neural_networks/#summary","title":"Summary","text":"<p> The authors present a novel non-uniform quantization method called learnable companding quantization (LCQ) for 2-, 3-, and 4-bit models. LCQ optimizes model weights and learnable companding functions that can non-uniformly control the quantization levels of weights and activations. The experimental results demonstrate favorable performance in reducing memory consumption while maintaining accuracy for image classification and object detection tasks. The 2-bit ResNet-50 model on ImageNet achieved a top-1 accuracy of 75.1% with only a 1.7% gap compared to full-precision models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_companding_quantization_for_accurate_low-bit_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_companding_quantization_for_accurate_low-bit_neural_networks/#content","title":"Content","text":"<p>Quantizing neural networks with extremely low-bit precision is a useful method for reducing memory consumption and improving inference speed, but it often results in reduced accuracy. In this paper, the authors propose a novel non-uniform quantization method called learnable companding quantization (LCQ) for 2-, 3-, and 4-bit models. LCQ optimizes model weights and learnable companding functions that can non-uniformly control the quantization levels of weights and activations. The authors also present a new weight normalization technique for stable training. Experimental results show that LCQ outperforms conventional methods and reduces the gap between quantized and full-precision models for image classification and object detection tasks. The 2-bit ResNet-50 model on ImageNet achieved a top-1 accuracy of 75.1% with only a 1.7% gap compared to full-precision models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_lookup_table_for_neural_network_quantization/","title":"Learnable lookup table for neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_lookup_table_for_neural_network_quantization/#summary","title":"Summary","text":"<p>This paper proposes a method of neural network quantization using learnable lookup tables (LLTs) that can be trained end-to-end, having very minimal computational cost. The approach has been tested on image classification, image super-resolution, and point cloud classification tasks, and it outperforms previous methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_lookup_table_for_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learnable_lookup_table_for_neural_network_quantization/#content","title":"Content","text":"<p> Neural network quantization has gained attention due to its ability to reduce computational and memory footprint. However, quantization error may affect the accuracy of the network. Many methods use pre-defined functions with learnable parameters to reduce quantization error, but these methods introduce considerable computational overhead. In this paper, a look-up operation is proposed for quantization, formulated as learnable lookup tables (LLTs). The LLTs can be trained in an end-to-end manner and have very small additional computational cost. The proposed method achieves state-of-the-art performance on image classification, image super-resolution, and point cloud classification tasks compared to previous methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learned_step_size_quantization/","title":"Learned step size quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learned_step_size_quantization/#summary","title":"Summary","text":"<p>The paper proposes a method for training deep neural networks with low precision operations by improving the quantizer configuration. The method learns step size quantization, achieves highest accuracy on ImageNet dataset, and can train 3-bit models that reach full precision baseline accuracy. It requires only a simple modification of existing training code and works using different levels of precision as needed for a given system."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learned_step_size_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learned_step_size_quantization/#content","title":"Content","text":"<p>Deep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is con\ufb01gured. Speci\ufb01cally, we introduce a novel means to estimate and scale the task loss gradient at each weight and activation layer\u2019s quantizer step size, such that it can be learned in conjunction with other network parameters. This approach works using different levels of precision as needed for a given system and requires only a simple modi\ufb01cation of existing training code."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_a_convolutional_neural_network_for_non-uniform_motion_blur_removal/","title":"Learning a convolutional neural network for non-uniform motion blur removal","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_a_convolutional_neural_network_for_non-uniform_motion_blur_removal/#summary","title":"Summary","text":"<p>Summary: The paper proposes a deep learning approach for removing non-uniform motion blur from an image, using a convolutional neural network to predict the probabilistic distribution of motion blur at the patch level, and a Markov random field model to infer a dense non-uniform motion blur field enforcing motion smoothness. The approach effectively estimates and removes complex non-uniform motion blur not well handled by previous methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_a_convolutional_neural_network_for_non-uniform_motion_blur_removal/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_a_convolutional_neural_network_for_non-uniform_motion_blur_removal/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_accurate_low-bit_deep_neural_networks_with_stochastic_quantization/","title":"Learning accurate low-bit deep neural networks with stochastic quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_accurate_low-bit_deep_neural_networks_with_stochastic_quantization/#summary","title":"Summary","text":"<p>The paper introduces the stochastic quantization algorithm for learning accurate low-bit deep neural networks. The algorithm quantizes a portion of elements or filters based on a stochastic probability inversely proportional to the quantization error, while keeping the other portion with full-precision. The accuracy of low-bit DNNs on various datasets and network structures can be improved significantly and consistently using SQ."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_accurate_low-bit_deep_neural_networks_with_stochastic_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_accurate_low-bit_deep_neural_networks_with_stochastic_quantization/#content","title":"Content","text":"<p>This paper proposes the stochastic quantization (SQ) algorithm for learning accurate low-bit deep neural networks (DNNs). The SQ algorithm quantizes a portion of elements/filters to low-bit with a stochastic probability inversely proportional to the quantization error, while keeping the other portion unchanged with full-precision. The quantized and full-precision portions are updated with corresponding gradients separately in each iteration. Experiments show that SQ can consistently and significantly improve the accuracy for different low-bit DNNs on various datasets and various network structures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_channel-wise_interactions_for_binary_convolutional_neural_networks/","title":"Learning channel-wise interactions for binary convolutional neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_channel-wise_interactions_for_binary_convolutional_neural_networks/#summary","title":"Summary","text":"<p>The paper proposes a binary convolutional neural network learning method that uses channel-wise interactions (CI-BCNN) to improve efficiency in inference. CI-BCNN mines channel-wise interactions to reduce inconsistencies in binary feature maps and retain information during inference. The proposed method outperforms existing binary neural networks with lower computational and storage requirements according to experiments conducted on CIFAR-10 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_channel-wise_interactions_for_binary_convolutional_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_channel-wise_interactions_for_binary_convolutional_neural_networks/#content","title":"Content","text":"<p>In this paper, we propose a channel-wise interaction based binary convolutional neural network learning method (CI-BCNN) for efficient inference. Our CI-BCNN mines the channel-wise interactions, through which prior knowledge is provided to alleviate inconsistency of signs in binary feature maps and preserves the information of input samples during inference. Extensive experiments on the CIFAR-10 and ImageNet datasets show that our method outperforms the state-of-the-art binary convolutional neural networks with less computational and storage cost."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_deep_binary_descriptor_with_multi-quantization/","title":"Learning deep binary descriptor with multi-quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_deep_binary_descriptor_with_multi-quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a deep learning method called DBD-MQ for visual matching. It considers binarization as a multi-quantization task and uses a KAEs network to jointly learn parameters and binarization functions. The method obtains discriminative binary descriptors with fine-grained multi-quantization and outperforms most existing binary feature descriptors in various visual analysis tasks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_deep_binary_descriptor_with_multi-quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_deep_binary_descriptor_with_multi-quantization/#content","title":"Content","text":"<p>In this paper, we propose an unsupervised feature learning method called deep binary descriptor with multi-quantization (DBD-MQ) for visual matching. Our DBD-MQ considers the binarization as a multi-quantization task and applies a K-AutoEncoders (KAEs) network to jointly learn the parameters and the binarization functions under a deep learning framework, so that discriminative binary descriptors can be obtained with a fine-grained multi-quantization. Extensive experimental results on different visual analysis including patch retrieval, image matching and image retrieval show that our DBD-MQ outperforms most existing binary feature descriptors."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_deep_parsimonious_representations/","title":"Learning deep parsimonious representations","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_deep_parsimonious_representations/#summary","title":"Summary","text":"<p> This paper proposes a clustering-based regularization technique that promotes parsimonious representations to facilitate generalization for deep networks and interpretability of the learned representations. The proposed approach can be optimized easily with flexibility supporting sample clustering, spatial clustering, as well as co-clustering, and is proven to be effective in various tasks such as unsupervised learning, classification, fine-grained categorization, and zero-shot learning."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_deep_parsimonious_representations/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_deep_parsimonious_representations/#content","title":"Content","text":"<p>In this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations. Towards this goal, we propose a clustering-based regularization that encourages parsimonious representations. Our k-means style objective is easy to optimize and flexible, supporting various forms of clustering, such as sample clustering, spatial clustering, as well as co-clustering. We demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classification, fine-grained categorization, and zero-shot learning."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_low-precision_neural_networks_without_straight-through_estimator_%28ste%29/","title":"Learning low-precision neural networks without straight-through estimator (ste)","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_low-precision_neural_networks_without_straight-through_estimator_%28ste%29/#summary","title":"Summary","text":"<p> This paper proposes a method called alpha-blending (AB) for quantizing neural networks to low precision using stochastic gradient descent (SGD) that replaces the quantized weight in the loss function with an affine combination of the quantized weight and the corresponding full-precision weight and gradually increases the non-trainable scalar coefficient from 0 to 1 during training, which results in improved top-1 accuracy compared to the Straight-Through Estimator (STE) based quantization technique."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_low-precision_neural_networks_without_straight-through_estimator_%28ste%29/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_low-precision_neural_networks_without_straight-through_estimator_%28ste%29/#content","title":"Content","text":"<p> The Straight-Through Estimator (STE) technique, which is commonly used for propagating gradients through the quantization function, is not fully understood theoretically. This paper proposes an alternative methodology called alpha-blending (AB) that quantizes neural networks to low precision using stochastic gradient descent (SGD). The AB method replaces the quantized weight in the loss function with an affine combination of the quantized weight and the corresponding full-precision weight with a non-trainable scalar coefficient. During training, this coefficient is gradually increased from 0 to 1, and the model is progressively converted from full-precision to low precision. The evaluation of the AB method shows improved top-1 accuracy compared to the results of STE based quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_to_quantize_deep_networks_by_optimizing_quantization_intervals_with_task_loss/","title":"Learning to quantize deep networks by optimizing quantization intervals with task loss","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_to_quantize_deep_networks_by_optimizing_quantization_intervals_with_task_loss/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method for reducing the bit-widths of deep networks using a trainable quantizer that discretizes activations and weights via optimized quantization intervals. The method can maintain accuracy of the full-precision network with a bit-width as low as 4-bit, and minimize accuracy degradation with further bit-width reduction. The method also allows for the quantization of pre-trained networks on heterogeneous datasets without access to their training data. The effectiveness of the method is demonstrated on various network architectures and datasets, achieving state-of-the-art accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_to_quantize_deep_networks_by_optimizing_quantization_intervals_with_task_loss/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_to_quantize_deep_networks_by_optimizing_quantization_intervals_with_task_loss/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_transferable_visual_models_from_natural_language_supervision/","title":"Learning transferable visual models from natural language supervision","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_transferable_visual_models_from_natural_language_supervision/#summary","title":"Summary","text":"<p>Summary: The paper presents a pre-training task of predicting captions from images to learn image representations that can be used in various computer vision tasks. The model shows competitive performance without the need for additional dataset-specific training, making it a promising alternative to traditional supervised learning methods. The code and pre-trained model weights have been released publicly.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_transferable_visual_models_from_natural_language_supervision/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Learning_transferable_visual_models_from_natural_language_supervision/#content","title":"Content","text":"<p> SOTA computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Least_squares_binary_quantization_of_neural_networks/","title":"Least squares binary quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Least_squares_binary_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>This paper introduces a unified framework to analyze different scaling strategies for binary quantization of weights and activations of deep neural networks. Their novel 2-bits quantization has the least square error with proofs for optimality and empirical error analysis. Their quantization algorithms can be implemented efficiently on the hardware using bitwise operations. They conducted experiments on the ImageNet dataset and showed a reduced accuracy gap when using the proposed least squares quantization algorithms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Least_squares_binary_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Least_squares_binary_quantization_of_neural_networks/#content","title":"Content","text":"<p>Quantizing weights and activations of deep neural networks results in significant improvement in inference efficiency at the cost of lower accuracy. A source of the accuracy gap between full precision and quantized models is the quantization error. In this work, we focus on the binary quantization, in which values are mapped to -1 and 1. We provide a unified framework to analyze different scaling strategies. Inspired by the pareto-optimality of 2-bits versus 1-bit quantization, we introduce a novel 2-bits quantization with provably least squares error. Our quantization algorithms can be implemented efficiently on the hardware using bitwise operations. We present proofs to show that our proposed methods are optimal, and also provide empirical error analysis. We conduct experiments on the ImageNet dataset and show a reduced accuracy gap when using the proposed least squares quantization algorithms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Leveraging_noise_and_aggressive_quantization_of_in-memory_computing_for_robust_dnn_hardware_against_adversarial_input_and_weight_attacks/","title":"Leveraging noise and aggressive quantization of in-memory computing for robust dnn hardware against adversarial input and weight attacks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Leveraging_noise_and_aggressive_quantization_of_in-memory_computing_for_robust_dnn_hardware_against_adversarial_input_and_weight_attacks/#summary","title":"Summary","text":"<p>Summary: The authors propose a new DNN training scheme that utilizes in-memory computing noise and partial sum quantization to improve the adversarial robustness of DNN hardware. They show that the noise induced by IMC can enhance DNN hardware robustness against adversarial attacks. The proposed scheme improves IMC DNN hardware robustness against black-box adversarial input attacks and bit-flip weight attacks, achieving up to 10.5% and 33.6% improvement, respectively. This study provides valuable insights for future research in this field.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Leveraging_noise_and_aggressive_quantization_of_in-memory_computing_for_robust_dnn_hardware_against_adversarial_input_and_weight_attacks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Leveraging_noise_and_aggressive_quantization_of_in-memory_computing_for_robust_dnn_hardware_against_adversarial_input_and_weight_attacks/#content","title":"Content","text":"<p>In this research paper, the authors propose a new deep neural network (DNN) training scheme that takes advantage of in-memory computing (IMC) noise and partial sum quantization to improve the adversarial robustness of DNN hardware against both adversarial input and weight attacks. The authors discovered that the noise induced by IMC can play a positive role in enhancing the robustness of DNN hardware against adversarial attacks. The paper presents experimental results showing that the proposed scheme effectively improves the robustness of IMC DNN hardware against black-box adversarial input attacks and bit-flip weight attacks, achieving up to 10.5% (CIFAR-10 accuracy) and 33.6% (number of bit-flips) improvement compared to conventional DNNs. The proposed scheme evaluates the effect of IMC hardware noise and aggressive partial sum quantization at the IMC crossbar for both adversarial input and weight attacks, and the results provide valuable insights for future research in this field."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lightweight_compression_of_neural_network_feature_tensors_for_collaborative_intelligence/","title":"Lightweight compression of neural network feature tensors for collaborative intelligence","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lightweight_compression_of_neural_network_feature_tensors_for_collaborative_intelligence/#summary","title":"Summary","text":"<p>Summary: The paper presents a lightweight compression technique designed to compress the activations of a split deep neural network layer without requiring any retraining. The proposed method uses simple and coarse scalar quantization along with clipping, binarization, and entropy coding to compress the activations. The compression technique was evaluated and found to compress 32-bit floating-point activations down to 0.6 to 0.8 bits while keeping the accuracy loss to less than 1%. The proposed compression was compared to the HEVC screen content coding extension and found to consistently provide better inference accuracy, by up to 1.3%. The results suggest that this technique is a promising option for coding a layer's activations in split neural networks for edge/cloud applications.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lightweight_compression_of_neural_network_feature_tensors_for_collaborative_intelligence/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lightweight_compression_of_neural_network_feature_tensors_for_collaborative_intelligence/#content","title":"Content","text":"<p> This paper presents a novel lightweight compression technique designed specifically to code the activations of a split deep neural network (DNN) layer for collaborative intelligence applications, without requiring any retraining. The proposed method uses simple and coarse scalar quantization along with clipping, binarization, and entropy coding to compress the activations. The performance of the compression technique was evaluated for popular object-detection and classification DNNs, with the 32-bit floating-point activations compressed down to 0.6 to 0.8 bits, while keeping the loss in accuracy to less than 1%. The proposed compression was compared to the HEVC screen content coding extension and found to consistently provide better inference accuracy, by up to 1.3%. The results suggest that the proposed technique is a promising option for coding a layer's activations in split neural networks for edge/cloud applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Locally_optimized_product_quantization_for_approximate_nearest_neighbor_search/","title":"Locally optimized product quantization for approximate nearest neighbor search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Locally_optimized_product_quantization_for_approximate_nearest_neighbor_search/#summary","title":"Summary","text":"<p>Summary: The paper presents a vector quantizer that achieves low distortion and fast search for approximate nearest neighbor (ANN) search in high dimensional spaces. It leverages the existing data structure used for non-exhaustive search, optimizes an individual product quantizer (PQ) per cell, and uses it to encode residuals. The optimization is done using a parametric solution, assuming normal distribution, which is quick to train. The method achieves state-of-the-art results on several public datasets, including a billion-scale one, with low space and time overhead.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Locally_optimized_product_quantization_for_approximate_nearest_neighbor_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Locally_optimized_product_quantization_for_approximate_nearest_neighbor_search/#content","title":"Content","text":"<p> We present a simple vector quantizer that combines low distortion with fast search and apply it to approximate nearest neighbor (ANN) search in high dimensional spaces. Leveraging the very same data structure that is used to provide non-exhaustive search, i.e., inverted lists or a multi-index, the idea is to locally optimize an individual product quantizer (PQ) per cell and use it to encode residuals. Local optimization is over rotation and space decomposition; interestingly, we apply a parametric solution that assumes a normal distribution and is extremely fast to train. With a reasonable space and time overhead that is constant in the data size, we set a new state-of-the-art on several public datasets, including a billion-scale one."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Logarithmic_unbiased_quantization%3A_Practical_4-bit_training_in_deep_learning/","title":"Logarithmic unbiased quantization: Practical 4-bit training in deep learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Logarithmic_unbiased_quantization%3A_Practical_4-bit_training_in_deep_learning/#summary","title":"Summary","text":"<p>Summary: This paper proposes a logarithmic unbiased quantization method that can quantize both the forward and backward phases of DNN training to achieve 4-bit precision without any overhead. The proposed method achieves state-of-the-art results in 4-bit training without the need for specific hardware support for non-standard quantization. The authors further improve the performance to only 0.32% degradation after three epochs of high-precision fine-tuning, combined with a variance reduction method.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Logarithmic_unbiased_quantization%3A_Practical_4-bit_training_in_deep_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Logarithmic_unbiased_quantization%3A_Practical_4-bit_training_in_deep_learning/#content","title":"Content","text":"<p> Quantization of weights and activations is a popular method to reduce the computational resources needed for training Deep Neural Networks (DNNs). However, 4-bit quantization only covers the forward phase of DNN training whereas quantization of the neural gradients, i.e., the loss gradients with respect to the outputs of intermediate neural layers, is also necessary to reduce the entire computational footprint. This paper suggests a logarithmic unbiased quantization (LUQ) method to quantize both the forward and backward phases of DNN training to achieve 4-bit precision without overhead. The proposed method achieves state-of-the-art results in 4-bit training without the need for specific hardware support for non-standard quantization. The authors further improve the performance to only 0.32% degradation after three epochs of high-precision fine-tuning, combined with a variance reduction method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Loss_aware_post-training_quantization/","title":"Loss aware post-training quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Loss_aware_post-training_quantization/#summary","title":"Summary","text":"<p> This paper analyzes the effects of quantization on loss landscape structure and demonstrates that quantizing layer parameters jointly can improve accuracy in post-training quantization methods for deep neural networks on resource-constrained devices. Results suggest that mild quantization does not greatly impact accuracy, but more aggressive quantization results in a non-separable loss landscape with steep curvature. The reference implementation is provided in the paper repository."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Loss_aware_post-training_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Loss_aware_post-training_quantization/#content","title":"Content","text":"<p> Neural network quantization enables the deployment of large models on resource-constrained devices. Current post-training quantization methods fall short in terms of accuracy for INT4 (or lower) but provide reasonable accuracy for INT8 (or above). In this work, we study the effect of quantization on the structure of the loss landscape. Additionally, we show that the structure is flat and separable for mild quantization, enabling straightforward post-training quantization methods to achieve good results. We show that with more aggressive quantization, the loss landscape becomes highly non-separable with steep curvature, making the selection of quantization parameters more challenging. Armed with this understanding, we design a method that quantizes the layer parameters jointly, enabling significant accuracy improvement over current post-training quantization methods. Reference implementation is available at the paper repository. Keywords: Deep Neural Networks, Quantization, Post-training Quantization, Compression."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-bit_quantization_needs_good_distribution/","title":"Low-bit quantization needs good distribution","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-bit_quantization_needs_good_distribution/#summary","title":"Summary","text":"<p>Summary: The paper proposes a low-bit quantization technique called the Scale-Clip technique which can reshape weights or activations dynamically. The Group-based Quantization algorithm is proposed to split filters into several groups, increasing performance for a low-bit model. The proposed Group-based Distribution Reshaping Quantization (GDRQ) framework, which integrates Scale-Clip with Group-based Quantization, achieves better performance than state-of-the-art quantization methods on various networks and vision tasks. The ResNet-50 model with 2-bit weights and 4-bit activations achieved less than 1% accuracy drop on ImageNet classification task, which is a new state-of-the-art.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-bit_quantization_needs_good_distribution/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-bit_quantization_needs_good_distribution/#content","title":"Content","text":"<p> Low-bit quantization is a challenge in maintaining high performance with limited model capacity. We propose the Scale-Clip technique, which can reshape weights or activations into a uniform-like distribution dynamically. To increase the model capability for a low-bit model, we propose the Group-based Quantization algorithm to split filters into several groups. Finally, we integrate Scale-Clip technique with Group-based Quantization algorithm and propose the Group-based Distribution Reshaping Quantization (GDRQ) framework. The experiments on various networks and vision tasks demonstrate that our framework achieves better performance than state-of-the-art quantization methods. The ResNet-50 model with 2-bit weights and 4-bit activations obtained by our framework achieves less than 1% accuracy drop on ImageNet classification task, which is a new state-of-the-art."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-bit_quantization_of_neural_networks_for_efficient_inference/","title":"Low-bit quantization of neural networks for efficient inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-bit_quantization_of_neural_networks_for_efficient_inference/#summary","title":"Summary","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-bit_quantization_of_neural_networks_for_efficient_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-bit_quantization_of_neural_networks_for_efficient_inference/#content","title":"Content","text":"<p> In this research paper, the authors propose a method for performing low-bit precision computations via neural network quantization, allowing for more efficient inference on limited hardware resources. The proposed approach formalizes the linear quantization task as a Minimum Mean Squared Error (MMSE) problem for both weights and activations, enabling low-bit precision inference without full network retraining. Specifically, the authors focus on 4 bits integer (INT4) quantization for deployment of pretrained models on limited hardware resources. Multiple experiments on various network architectures showed that the suggested method yields state of the art results with minimal loss of task accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-cost_stochastic_hybrid_multiplier_for_quantized_neural_networks/","title":"Low-cost stochastic hybrid multiplier for quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-cost_stochastic_hybrid_multiplier_for_quantized_neural_networks/#summary","title":"Summary","text":"<p>The article proposes a new stochastic hybrid multiplier for quantized neural networks that achieves a 7.7x energy reduction compared to binary implementation while maintaining slightly higher recognition error rates. The design also results in 4x, 9x, and 10x reduction in terms of area, power, and energy, respectively, as supported by experimental results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-cost_stochastic_hybrid_multiplier_for_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Low-cost_stochastic_hybrid_multiplier_for_quantized_neural_networks/#content","title":"Content","text":"<p>In this article, we propose a new stochastic multiplier with simple CMOS transistors called the stochastic hybrid multiplier for quantized neural networks. Experimental results indicate that our stochastic design achieves about 7.7x energy reduction compared to its counterpart binary implementation while maintaining slightly higher recognition error rates than the binary implementation. Our work derives at least 4x, 9x, and 10x reduction in terms of area, power, and energy, respectively."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lowino%3A_Towards_efficient_low-precision_winograd_convolutions_on_modern_cpus/","title":"Lowino: Towards efficient low-precision winograd convolutions on modern cpus","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lowino%3A_Towards_efficient_low-precision_winograd_convolutions_on_modern_cpus/#summary","title":"Summary","text":"<p>The paper proposes LoWino, a post-training quantization technique for reducing precision loss caused by Winograd transformation, to accelerate convolutional neural networks. Results show that LoWino achieves a 2.04\u00d7 speedup compared to state-of-the-art implementations while maintaining accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lowino%3A_Towards_efficient_low-precision_winograd_convolutions_on_modern_cpus/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lowino%3A_Towards_efficient_low-precision_winograd_convolutions_on_modern_cpus/#content","title":"Content","text":"<p>Low-precision computation is a widely supported method for accelerating convolutional neural networks, but it is not commonly used to speed up Winograd due to the numerical error caused by combining Winograd transformation and quantization. In this paper, the authors propose LoWino, a low-precision Winograd convolution approach based on post-training quantization, which employs a linear quantization method in the Winograd domain to reduce the precision loss caused by transformations. The authors also present an efficient implementation that adequately exploits the capability of low-precision computation on modern CPUs. Experimental results show that LoWino achieves up to 2.04\u00d7 speedup over state-of-the-art implementations in the vendor library while maintaining the accuracy at a reasonable level."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lq-nets%3A_Learned_quantization_for_highly_accurate_and_compact_deep_neural_networks/","title":"Lq-nets: Learned quantization for highly accurate and compact deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lq-nets%3A_Learned_quantization_for_highly_accurate_and_compact_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes to jointly train a quantized DNN and its associated quantizers to address the accuracy gap between quantized and full-precision models, instead of using fixed quantization schemes.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lq-nets%3A_Learned_quantization_for_highly_accurate_and_compact_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lq-nets%3A_Learned_quantization_for_highly_accurate_and_compact_deep_neural_networks/#content","title":"Content","text":"<p>Although weight and activation quantization is an e\ufb00ective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using \ufb01xed, handcrafted quantization schemes such as uniform or logarithmic quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lsq%2B%3A_Improving_low-bit_quantization_through_learnable_offsets_and_better_initialization/","title":"Lsq+: Improving low-bit quantization through learnable offsets and better initialization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lsq%2B%3A_Improving_low-bit_quantization_through_learnable_offsets_and_better_initialization/#summary","title":"Summary","text":"<p> The use of newer activation functions such as Swish, H-swish, and Mish can result in negative activation values, which leads to significant performance loss when using typical learnable quantization schemes. LSQ+ is proposed as an extension of LSQ, which introduces a trainable and asymmetric quantization scheme with scale and offset parameters that can learn to accommodate negative activations. LSQ+ also uses an MSE-based initialization scheme to alleviate the instability or variance in final performance that is commonly found in gradient-based learnable quantization schemes. LSQ+ outperforms LSQ for low-bit quantization of neural nets with Swish activations, showing state-of-the-art results for EfficientNet and MixNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lsq%2B%3A_Improving_low-bit_quantization_through_learnable_offsets_and_better_initialization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Lsq%2B%3A_Improving_low-bit_quantization_through_learnable_offsets_and_better_initialization/#content","title":"Content","text":"<p>Unlike ReLU, newer activation functions (like Swish, H- swish, Mish) that are frequently employed in popular efficient architectures can also result in negative activation values, with skewed positive and negative ranges. Typical learnable quantization schemes [ 5,7] assume unsigned quantization for activations and quantize all negative activations to zero which leads to significant loss in performance. Naively using signed quantization to accommodate these negative values requires an extra sign bit which is expensive for low-bit (2-, 3-, 4-bit) quantization. To solve this problem, we propose LSQ+, a natural extension of LSQ [ 7], wherein we introduce a general asymmetric quantization scheme with trainable scale and offset parameters that can learn to accommodate the negative activations. Gradient-based learnable quantization schemes also commonly suffer from high instability or variance in the final training performance, hence requiring a great deal of hyper-parameter tuning to reach a satisfactory performance. LSQ+ alleviates this problem by using an MSE-based initialization scheme for the quantization parameters. We show that this initialization leads to significantly lower variance in final performance across multiple training runs. Overall, LSQ+ shows state-of-the-art results for EfficientNet and MixNet and also significantly outperforms LSQ for low-bit quantization of neural nets with Swish activations (e.g.: 1.8% gain with W4A4 quantization and up to 5.6% gain with W2A2 quantization of EfficientNet-B0 on ImageNet dataset). To the best of our knowledge, ours is the first work to quantize such architectures to extremely low bit-widths."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/M6-Fashion%3A_High-Fidelity_Multi-modal_Image_Generation_and_Editing/","title":"M6-Fashion: High-Fidelity Multi-modal Image Generation and Editing","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/M6-Fashion%3A_High-Fidelity_Multi-modal_Image_Generation_and_Editing/#summary","title":"Summary","text":"<p>The paper proposes a two-stage framework called M6-Fashion which combines style prior knowledge and multimodal control for high-fidelity image generation and editing in the fashion industry. M6-Fashion separates style codes in both spatial and semantic dimensions for better image generation and has shown superior performance on various image generation and editing tasks on a large-scale clothing dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/M6-Fashion%3A_High-Fidelity_Multi-modal_Image_Generation_and_Editing/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/M6-Fashion%3A_High-Fidelity_Multi-modal_Image_Generation_and_Editing/#content","title":"Content","text":"<p>The fashion industry has diverse applications in multi-modal image generation and editing. It aims to create a desired high-fidelity image with the multi-modal conditional signal as guidance. In this paper, we adapt both style prior knowledge and flexibility of multi-modal control into one unified two-stage framework, M6-Fashion, focusing on the practical AI-aided Fashion design. M6-Fashion decouples style codes in both spatial and semantic dimensions to guarantee high-fidelity image generation. Extensive experiments on a large-scale clothing dataset M2C-Fashion demonstrates superior performances on various image generation and editing tasks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/MSP%3A_an_FPGA-specific_mixed-scheme%2C_multi-precision_deep_neural_network_quantization_framework/","title":"MSP: an FPGA-specific mixed-scheme, multi-precision deep neural network quantization framework","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/MSP%3A_an_FPGA-specific_mixed-scheme%2C_multi-precision_deep_neural_network_quantization_framework/#summary","title":"Summary","text":"<p>The paper discusses the need to deploy deep learning models onto edge devices with limited computing and storage resources. This has led to the usage of model compression techniques, primarily DNN quantization. The paper proposes a mixed-scheme DNN quantization method that incorporates both linear and non-linear number systems to boost the utilization of LUTs and DSPs on an FPGA. It also uses a quantization method that supports multiple precisions along the intra-layer dimension, which can uniform the hardware configurations for different layers to reduce computation overhead while preserving model accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/MSP%3A_an_FPGA-specific_mixed-scheme%2C_multi-precision_deep_neural_network_quantization_framework/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/MSP%3A_an_FPGA-specific_mixed-scheme%2C_multi-precision_deep_neural_network_quantization_framework/#content","title":"Content","text":"<p>With the tremendous success of deep learning, there exists imminent need to deploy deep learning models onto edge devices. To tackle the limited com- puting and storage resources in edge devices, model compression techniques have been widely used to trim deep neural network (DNN) models for on- device inference execution. This paper targets the commonly used FPGA (\feld programmable gate array) devices as the hardware platforms for DNN edge computing. We focus on the DNN quantization as the main model com- pression technique, since DNN quantization has been of great importance for the implementations of DNN models on the hardware platforms. The nov- elty of this work comes in twofold: (i) We propose a mixed-scheme DNN quantization method that incorporates both the linear and non-linear num- ber systems for quantization, with the aim to boost the utilization of the heterogeneous computing resources, i.e., LUTs (look up tables) and DSPs (digital signal processors) on an FPGA. Note that all the existing (single- scheme) quantization methods can only utilize one type of resources (either LUTs or DSPs for the MAC (multiply-accumulate) operations in deep learn- ing computations. (ii) We use a quantization method that supports multiple precisions along the intra-layer dimension, while the existing quantization methods apply multi-precision quantization along the inter-layer dimension. The intra-layer multi-precision method can uniform the hardware con\fgura- tions for di\u000berent layers to reduce computation overhead and at the same time preserve the model accuracy as the inter-layer approach."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/MWQ%3A_Multiscale_wavelet_quantized_neural_networks/","title":"MWQ: Multiscale wavelet quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/MWQ%3A_Multiscale_wavelet_quantized_neural_networks/#summary","title":"Summary","text":"<p>The paper proposes a multiscale wavelet quantization method for deep neural networks, which decomposes original data into multiscale frequency components and quantizes the components of different scales respectively. The proposed method exploits multiscale frequency and spatial information to reduce information loss caused by quantization in the spatial domain. The method has been applied for different applications, and showed stronger representation ability and effectiveness in quantized neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/MWQ%3A_Multiscale_wavelet_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/MWQ%3A_Multiscale_wavelet_quantized_neural_networks/#content","title":"Content","text":"<p>Model quantization can reduce the model size and computational latency, it has become an essential technique for the deployment of deep neural networks on resource-constrained hardware (e.g., mobile phones and embedded devices). The existing quantization methods mainly consider the numerical elements of the weights and activation values, ignoring the relationship between elements. The decline of representation ability and information loss usually lead to the performance degradation. Inspired by the characteristics of images in the frequency domain, we propose a novel multiscale wavelet quantization (MWQ) method. This method decomposes original data into multiscale frequency components by wavelet transform, and then quantizes the components of different scales, respectively. It exploits the multiscale frequency and spatial information to alleviate the information loss caused by quantization in the spatial domain. Because of the flexibility of MWQ, we demonstrate three applications (e.g., model compression, quantized network optimization, and information enhancement) on the ImageNet and COCO datasets. Experimental results show that our method has stronger representation ability and can play an effective role in quantized neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Memory-driven_mixed_low_precision_quantization_for_enabling_deep_network_inference_on_microcontrollers/","title":"Memory-driven mixed low precision quantization for enabling deep network inference on microcontrollers","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Memory-driven_mixed_low_precision_quantization_for_enabling_deep_network_inference_on_microcontrollers/#summary","title":"Summary","text":"<p> The paper presents a new method for deploying high-accuracy deep networks on microcontrollers with memory and computational limitations using mixed low-bitwidth compression and integer-only operations. The method cuts the number of bits of most memory-demanding layers and determines the minimum bit precision of every activation and weight tensor given the memory constraints of a device. The approach was applied on STM32H7 microcontroller with only 2MB flash memory and 512KB RAM and reported an improvement in top1 accuracy by 8% compared to previously published 8 bit implementations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Memory-driven_mixed_low_precision_quantization_for_enabling_deep_network_inference_on_microcontrollers/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Memory-driven_mixed_low_precision_quantization_for_enabling_deep_network_inference_on_microcontrollers/#content","title":"Content","text":"<p>This paper presents a novel methodology for deploying high-accuracy deep networks on microcontrollers with memory and computational limitations. The approach uses mixed low-bitwidth compression and integer-only operations for the inference graph. The method cuts the number of bits of the most memory-demanding layers to determine the minimum bit precision of every activation and weight tensor given the memory constraints of a device. The paper reports the latency-accuracy evaluation of mixed-precision MobilenetV1 family networks on a STM32H7 microcontroller with only 2MB of FLASH memory and 512kB of RAM. The results demonstrate an end-to-end deployment of an integer-only Mobilenet network with Top1 accuracy of 68%, which improves by 8% the Top1 accuracy with respect to previously published 8 bit implementations for microcontrollers."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Metric_learning_for_large_scale_image_classification%3A_Generalizing_to_new_classes_at_near-zero_cost/","title":"Metric learning for large scale image classification: Generalizing to new classes at near-zero cost","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Metric_learning_for_large_scale_image_classification%3A_Generalizing_to_new_classes_at_near-zero_cost/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel method for large scale image classification that can generalize to new classes with minimal cost using Mahalanobis distance metric in an embedded feature space. The method is efficient in terms of both memory and computation and outperforms existing methods while being more scalable. The approach is demonstrated on various large-scale datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Metric_learning_for_large_scale_image_classification%3A_Generalizing_to_new_classes_at_near-zero_cost/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Metric_learning_for_large_scale_image_classification%3A_Generalizing_to_new_classes_at_near-zero_cost/#content","title":"Content","text":"<p>Abstract: In this paper, we propose a new method for large scale image classification that can generalize to new classes at near-zero cost. The main idea is to learn a Mahalanobis distance metric in the embedded feature space, and use it to compare test images to labeled class prototypes. Our method is designed to be efficient in terms of both memory and computation, and can handle very large numbers of classes and images. We demonstrate our approach on several large-scale datasets, and show that it outperforms existing methods while being much more scalable.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Minimum_energy_quantized_neural_networks/","title":"Minimum energy quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Minimum_energy_quantized_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper discusses the minimum-energy optimization of Quantized Neural Networks (QNNs) that use low-precision weights and activations. Using a model for energy consumption in a generic hardware platform, the paper finds that BinaryNets or int4 implementations result in the minimum energy solution and outperform int8 networks up to 2^10 at iso-accuracy. The paper highlights the importance of explicit control over network quantization and suggests that cross-optimizing both the used algorithm and hardware architecture can enhance the performance of always-on embedded applications.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Minimum_energy_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Minimum_energy_quantized_neural_networks/#content","title":"Content","text":"<p>This work focuses on the minimum-energy optimization of Quantized Neural Networks (QNNs), which use low-precision weights and activations. The energy consumption of inference is modeled for a generic hardware platform to analyze and quantify the fundamental trade-off between the number of bits used in the QNN and energy-efficiency. The analysis shows that energy consumption varies orders of magnitude at iso-accuracy depending on the number of bits used in the QNN. The study suggests that in a typical system, BinaryNets or int4 implementations lead to the minimum energy solution, outperforming int8 networks up to 2^10 at iso-accuracy. The work contributes to explicit control over network quantization, enabling cross-optimizing both the used algorithm and the hardware architecture for always-on embedded applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mirror_descent_view_for_neural_network_quantization/","title":"Mirror descent view for neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mirror_descent_view_for_neural_network_quantization/#summary","title":"Summary","text":"<p>The authors propose a Mirror Descent framework for neural network quantization that enables them to derive mirror maps and updates based on projections. They also present a numerically stable implementation of Mirror Descent that achieves state-of-the-art performance on CIFAR-10/100, TinyImageNet, and ImageNet classification datasets with VGG-16, ResNet-18, and MobileNetV2 architectures. Quantization is a technique for network compression that helps save memory and inference time by restricting the parameters to take values from a small discrete set."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mirror_descent_view_for_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mirror_descent_view_for_neural_network_quantization/#content","title":"Content","text":"<p>In this research paper, the authors propose a Mirror Descent framework for neural network quantization, where quantization is a technique for network compression that restricts parameters to take values from a small discrete set, resulting in savings in memory and inference time. The authors introduce conditions on the projections that enable them to derive mirror maps and updates. They also present a numerically stable implementation of Mirror Descent, which yields state-of-the-art performance on CIFAR-10/100, TinyImageNet, and ImageNet classification datasets with VGG-16, ResNet-18, and MobileNetV2 architectures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mix_and_match%3A_A_novel_fpga-centric_deep_neural_network_quantization_framework/","title":"Mix and match: A novel fpga-centric deep neural network quantization framework","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mix_and_match%3A_A_novel_fpga-centric_deep_neural_network_quantization_framework/#summary","title":"Summary","text":"<p> This paper focuses on weight quantization as a hardware-friendly model compression approach for deep neural networks, which concentrates on different quantization schemes for different rows of the weight matrix compared to existing methods that use the same quantization scheme for all weights. The authors propose a quantization framework for FPGA devices, which improves performance better than exploiting DSPs for all multiplication operations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mix_and_match%3A_A_novel_fpga-centric_deep_neural_network_quantization_framework/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mix_and_match%3A_A_novel_fpga-centric_deep_neural_network_quantization_framework/#content","title":"Content","text":"<p>Deep Neural Networks (DNNs) have achieved extraordinary performance in various application domains. To support diverse DNN models, efficient implementations of DNN inference on edge-computing platforms, e.g., ASICs, FPGAs, and embedded systems, are extensively investigated. This paper focuses on weight quantization, a hardware-friendly model compression approach that is complementary to weight pruning. Unlike existing methods that use the same quantization scheme for all weights, we propose the first solution that applies different quantization schemes for different rows of the weight matrix.  We evaluate our FPGA-centric quantization framework across multiple application domains. With optimal SP2/fixed-point ratios on two FPGA devices, we achieve performance improvement of 2:1 and 4:1 compared to solely exploiting DSPs for all multiplication operations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mixed-precision_quantized_neural_network_with_progressively_decreasing_bitwidth_for_image_classification_and_object_detection/","title":"Mixed-precision quantized neural network with progressively decreasing bitwidth for image classification and object detection","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mixed-precision_quantized_neural_network_with_progressively_decreasing_bitwidth_for_image_classification_and_object_detection/#summary","title":"Summary","text":"<p> The paper proposes a mixed-precision quantized neural network with progressively decreasing bitwidth to improve the trade-off between accuracy and compression. This approach sets the parameter bitwidth homogeneously and balances the performance and compression. The higher-precision bottom layers improve the 1-bit network performance by better preserving the original image information, while the lower-precision posterior layers contribute to the regularization of k-bit networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mixed-precision_quantized_neural_network_with_progressively_decreasing_bitwidth_for_image_classification_and_object_detection/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mixed-precision_quantized_neural_network_with_progressively_decreasing_bitwidth_for_image_classification_and_object_detection/#content","title":"Content","text":"<p>Efficient model inference is an important issue in the deployment of deep neural network on resource constraint platforms. The parameter bitwidth is set homogeneously and there is a trade-off between superior performance and aggressive compression. A simple but effective mixed-precision quantized neural network with progressively decreasing bitwidth is proposed to improve the trade-off between accuracy and compression. The higher-precision bottom layers could boost the 1-bit network performance appreciably due to a better preservation of the original image information while the lower-precision posterior layers contribute to the regularization of k-bit networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Modality-Aware_Triplet_Hard_Mining_for_Zero-shot_Sketch-Based_Image_Retrieval/","title":"Modality-Aware Triplet Hard Mining for Zero-shot Sketch-Based Image Retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Modality-Aware_Triplet_Hard_Mining_for_Zero-shot_Sketch-Based_Image_Retrieval/#summary","title":"Summary","text":"<p>The paper focuses on Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) problem and introduces a cross-modality metric learning approach for it. The metric learning viewpoint is helpful in improving ZS-SBIR in two aspects, i.e., through good practices in deep metric learning and ensuring modality gap suppression. They propose a new method named Modality-Aware Triplet Hard Mining (MATHM) that enhances the baseline with three types of pairwise learning and an adaptive weighting method. The experimental results confirm that MATHM brings significant improvement based on the strong baseline and sets up new state-of-the-art performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Modality-Aware_Triplet_Hard_Mining_for_Zero-shot_Sketch-Based_Image_Retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Modality-Aware_Triplet_Hard_Mining_for_Zero-shot_Sketch-Based_Image_Retrieval/#content","title":"Content","text":"<p>This paper tackles the Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) problem from the viewpoint of cross- modality metric learning. This task has two characteristics: 1) the zero-shot setting requires a metric space with good within-class compactness and the between-class discrepancy for recognizing the novel classes and 2) the sketch query and the photo gallery are in different modalities. The metric learning viewpoint benefits ZS-SBIR from two aspects. First, it facilitates improvement through recent good practices in deep metric learning (DML). By combining two fundamental learning approaches in DML, e.g., classification training and pairwise training, we set up a strong baseline for ZS-SBIR. Second, it provides an insight that properly suppressing the modality gap is critical. To this end, we design a novel method named Modality-Aware Triplet Hard Mining (MATHM). MATHM enhances the baseline with three types of pairwise learning, e.g., a cross-modality sample pair, a within-modality sample pair, and their combination. We also design an adaptive weighting method to balance these three components during training dynamically. Experimental results confirm that MATHM brings another round of significant improvement based on the strong baseline and sets up new state-of-the-art performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Model_compression_via_distillation_and_quantization/","title":"Model compression via distillation and quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Model_compression_via_distillation_and_quantization/#summary","title":"Summary","text":"<p>Summary: This paper proposes two methods for compressing deep neural networks for resource-conscious environments, named quantized distillation and differentiable quantization. The proposed methods utilize weight quantization and distillation of larger networks into compressed student networks. The results of the experiments demonstrate that shallow quantized students can attain comparable accuracy to full-precision teacher models, with up to ten times compression and almost linear inference speedup with depth reduction.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Model_compression_via_distillation_and_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Model_compression_via_distillation_and_quantization/#content","title":"Content","text":"<p>This paper proposes two new compression methods for executing deep models in resource-constrained environments, such as mobile or embedded devices. The methods leverage weight quantization and distillation of larger networks, called \u201cteachers,\u201d into compressed \u201cstudent\u201d networks. The first method is called quantized distillation and incorporates distillation loss, expressed with respect to the teacher network, into the training of a smaller student network whose weights are quantized to a limited set of levels. The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. Experiments on convolutional and recurrent architectures show that quantized shallow students can reach similar accuracy levels to state-of-the-art full-precision teacher models, while providing up to order of magnitude compression, and inference speedup that is almost linear in the depth reduction."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Monte_Carlo_gradient_quantization/","title":"Monte Carlo gradient quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Monte_Carlo_gradient_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes Monte Carlo Gradient Quantization (MCGQ) method to leverage both sparsity and quantization for compressing gradients of neural networks during training. The proposed method shows faster convergence and better performance than existing quantization methods in image classification and language modeling. MCGQ with low-bit-width quantization and high sparsity levels surpasses existing compression methods' rates.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Monte_Carlo_gradient_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Monte_Carlo_gradient_quantization/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mr._BiQ%3A_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error/","title":"Mr. BiQ: Post-Training Non-Uniform Quantization Based on Minimizing the Reconstruction Error","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mr._BiQ%3A_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error/#summary","title":"Summary","text":"<p>Summary: The paper proposes a post-training non-uniform quantization method, termed Mr.BiQ, for compressing neural networks, including Transformer models. Mr.BiQ optimizes quantization parameters jointly with the weights and allows for multiple data formats for activations. Results show improved accuracy, with up to 5.35 p.p. improvement in CNNs, up to 4.23 p.p. improvement in Vision Transformers, and up to 3.37 point improvement in Transformers for NLP.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mr._BiQ%3A_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Mr._BiQ%3A_Post-Training_Non-Uniform_Quantization_Based_on_Minimizing_the_Reconstruction_Error/#content","title":"Content","text":"<p> Post-training quantization is an effective method to compress a neural network, but it has only been discussed and demonstrated in uniform quantization on convolutional neural networks. This paper proposes a new post-training non-uniform quantization method, called Mr.BiQ, for low bit-width quantization even on Transformer models. The proposed method optimizes the quantization parameters directly and jointly with the weights, allowing for multiple data formats for activations. Experimental results show significant improvement in accuracy for various models, with up to 5.35 p.p. improvement in CNNs, up to 4.23 p.p. improvement in Vision Transformers, and up to 3.37 point improvement in Transformers for NLP."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Multi-precision_quantized_neural_networks_via_encoding_decomposition_of_%7B-1%2C%2B_1%7D/","title":"Multi-precision quantized neural networks via encoding decomposition of {-1,+ 1}","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Multi-precision_quantized_neural_networks_via_encoding_decomposition_of_%7B-1%2C%2B_1%7D/#summary","title":"Summary","text":"<p>The paper proposes an encoding scheme that decomposes quantized neural networks (QNNs) into multi-branch binary networks using f+1, -1g. This scheme uses bitwise operations (xnor and bitcount) to achieve model compression, computational acceleration, and resource saving. The proposed mechanism is very suitable for the use of FPGA and ASIC in terms of data storage and computation, which provides a feasible idea for smart chips. The method was tested on large-scale image classification tasks (e.g. ImageNet) and object detection tasks and was found to achieve almost the same performance as its full-precision counterparts, even with low-bit encoding."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Multi-precision_quantized_neural_networks_via_encoding_decomposition_of_%7B-1%2C%2B_1%7D/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Multi-precision_quantized_neural_networks_via_encoding_decomposition_of_%7B-1%2C%2B_1%7D/#content","title":"Content","text":"<p>The training of deep neural networks (DNNs) requires intensive resources both for computation and for storage performance. Thus, DNNs cannot be efficiently applied to mobile phones and embedded devices, which seriously limits their applicability in industry applications. To address this issue, we propose a novel encoding scheme of using f+1, -1g to decompose quantized neural networks (QNNs) into multi-branch binary networks, which can be efficiently implemented by bitwise operations (xnor and bitcount) to achieve model compression, computational acceleration, and resource saving. Based on our method, users can easily achieve different encoding precisions arbitrarily according to their requirements and hardware resources. The proposed mechanism is very suitable for the use of FPGA and ASIC in terms of data storage and computation, which provides a feasible idea for smart chips. We validate the effectiveness of our method on both large-scale image classification tasks (e.g. ImageNet) and object detection tasks. In particular, our method with low-bit encoding can still achieve almost the same performance as its full-precision counterparts."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Navigating_Local_Minima_in_Quantized_Spiking_Neural_Networks/","title":"Navigating Local Minima in Quantized Spiking Neural Networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Navigating_Local_Minima_in_Quantized_Spiking_Neural_Networks/#summary","title":"Summary","text":"<p> The paper discusses the challenges faced by Spiking and Quantized Neural Networks when trained using error backpropagation. It proposes a solution to overcome the absence of gradient signals by periodically boosting the Learning Rate (LR) during training. A cosine-annealed LR schedule coupled with weight-independent adaptive moment estimation is applied to Quantized Spiking Neural Networks (QSNNs), and a rigorous empirical evaluation of this technique is performed on high precision and 4-bit quantized SNNs across three datasets, demonstrating close to state-of-the-art performance on the more complex datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Navigating_Local_Minima_in_Quantized_Spiking_Neural_Networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Navigating_Local_Minima_in_Quantized_Spiking_Neural_Networks/#content","title":"Content","text":"<p> Spiking and Quantized Neural Networks (NNs) face challenges when trained using error backpropagation due to the absence of gradient signals when applying hard thresholds. To overcome this, biased gradient estimators are used, such as surrogate gradients and Straight-Through Estimators (STEs). However, such noise increases the difficulty of finding optima in loss landscapes, especially during later stages of optimization. By periodically boosting the Learning Rate (LR) during training, the network can navigate unexplored solution spaces that would otherwise be difficult to reach due to local minima, barriers, or flat surfaces. This paper presents a systematic evaluation of a cosine-annealed LR schedule coupled with weight-independent adaptive moment estimation as applied to Quantized Spiking Neural Networks (QSNNs). They provide a rigorous empirical evaluation of this technique on high precision and 4-bit quantized SNNs across three datasets, demonstrating close to state-of-the-art performance on the more complex datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Network_quantization_with_element-wise_gradient_scaling/","title":"Network quantization with element-wise gradient scaling","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Network_quantization_with_element-wise_gradient_scaling/#summary","title":"Summary","text":"<p> The paper proposes an alternative method to the straight-through estimator (STE) for network quantization called element-wise gradient scaling (EWGS). The authors adjust a scaling factor adaptively using Hessian information of a network and show experimental results on image classification datasets with diverse network architectures under a wide range of bit-width settings, demonstrating the effectiveness of their method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Network_quantization_with_element-wise_gradient_scaling/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Network_quantization_with_element-wise_gradient_scaling/#content","title":"Content","text":"<p> Network quantization aims to reduce the bit-widths of weights and/or activations to implement deep neural networks with limited hardware resources. In this paper, the authors propose an alternative to the straight-through estimator (STE) called element-wise gradient scaling (EWGS), which trains a quantized network better than the STE in terms of stability and accuracy. They adjust a scaling factor adaptively using Hessian information of a network and show experimental results on image classification datasets with diverse network architectures under a wide range of bit-width settings, demonstrating the effectiveness of their method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Neural_network_compression_framework_for_fast_model_inference/","title":"Neural network compression framework for fast model inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Neural_network_compression_framework_for_fast_model_inference/#summary","title":"Summary","text":"<p>The paper introduces Neural Network Compression Framework (NNCF), a PyTorch-based framework that implements various network compression methods such as quantization, sparsity, filter pruning, and binarization. These methods allow creating hardware-friendly models that can be efficiently run on general-purpose or specialized deep learning accelerators while maintaining the model's accuracy. This framework can be used with provided training samples or integrated into an existing training code with minimal adaptations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Neural_network_compression_framework_for_fast_model_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Neural_network_compression_framework_for_fast_model_inference/#content","title":"Content","text":"<p>We present a new PyTorch-based framework for neural network compression with \ufb01ne-tuning named Neural Network Compression Framework (NNCF). It leverages recent advances of various network compression methods and implements some of them, namely quantization, sparsity, \ufb01lter pruning and binarization. These methods allow producing more hardware-friendly models that can be e\ufb03ciently run on general-purpose hardware computation units (CPU, GPU) or specialized deep learning accelerators. We show that the implemented methods and their combinations can be successfully applied to a wide range of architectures and tasks to accelerate inference while preserving the original model\u2019s accuracy. The framework can be used in conjunction with the supplied training samples or as a standalone package that can be seamlessly integrated into the existing training code with minimal adaptations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Non-uniform_dnn_structured_subnets_sampling_for_dynamic_inference/","title":"Non-uniform dnn structured subnets sampling for dynamic inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Non-uniform_dnn_structured_subnets_sampling_for_dynamic_inference/#summary","title":"Summary","text":"<p>Summary: The paper proposes a run-time dynamic neural network structure by using a novel DNN sub-network sampling method via non-uniform channel selection for subnets generation. This method allows users to trade off between power, speed, computing load, and accuracy on-the-fly after deployment, depending on the dynamic requirements or specifications of the given system. The proposed model outperforms the same sub-nets trained individually and other related works on CIFAR-10 and ImageNet datasets using ResNets. Furthermore, it achieves latency trade-off among 13.4, 24.6, 41.3, 62.1 (ms) and 30.5, 38.7, 51, 65.4 (ms) for GPU with 128 batch-size and CPU, respectively, on ImageNet using ResNet18.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Non-uniform_dnn_structured_subnets_sampling_for_dynamic_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Non-uniform_dnn_structured_subnets_sampling_for_dynamic_inference/#content","title":"Content","text":"<p> With the success of Deep Neural Networks (DNN), many recent works have been focusing on developing hardware-accelerator for power and resource-limited system via model compression techniques, such as quantization, pruning, low-rank approximation and etc. However, almost all existing compressed DNNs are \ufb01xed after deployment, which lacks run-time adaptive structure to adapt to its dynamic hardware resource allocation, power budget, throughput requirement, as well as dynamic workload. As the countermeasure, to construct a novel run-time dynamic DNN structure, we propose a novel DNN sub-network sampling method via non-uniform channel selection for subnets generation. Thus, the user can trade off between power, speed, computing load, and accuracy on-the-fly after the deployment, depending on the dynamic requirements or specifications of the given system. We verify the proposed model on both CIFAR-10 and ImageNet dataset using ResNets, which outperforms the same sub-nets trained individually and other related works. It shows that our method can achieve latency trade-off among 13.4, 24.6, 41.3, 62.1(ms) and 30.5, 38.7, 51, 65.4(ms) for GPU with 128 batch-size and CPU, respectively, on ImageNet using ResNet18."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Nonuniform-to-Uniform_Quantization%3A_Towards_Accurate_Quantization_via_Generalized_Straight-Through_Estimation/","title":"Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Nonuniform-to-Uniform_Quantization%3A_Towards_Accurate_Quantization_via_Generalized_Straight-Through_Estimation/#summary","title":"Summary","text":"<p>This paper introduces a method called Nonuniform-to-Uniform Quantization (N2UQ) that aims to maintain the superior representation capacity of non-uniform quantization methods while remaining hardware-friendly and efficient for model inference. The method learns flexible in-equidistant input thresholds to better fit underlying distributions while quantizing real-valued inputs into equidistant output levels. G-STE is used to train quantized network with the learnable input thresholds. Additionally, entropy preserving regularization is considered to reduce information loss in weight quantization. N2UQ outperforms state-of-the-art non-uniform quantization methods by a margin of 0.5\u223c1.7% on ImageNet, even with uniformly quantized weights and activations. Code and models are available at https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Nonuniform-to-Uniform_Quantization%3A_Towards_Accurate_Quantization_via_Generalized_Straight-Through_Estimation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Nonuniform-to-Uniform_Quantization%3A_Towards_Accurate_Quantization_via_Generalized_Straight-Through_Estimation/#content","title":"Content","text":"<p>The nonuniform quantization strategy for compressing neural networks usually achieves better performance than its counterpart, i.e., uniform strategy, due to its superior representational capacity. However, many nonuniform quantization methods overlook the complicated projection process in implementing the nonuniformly quantized weights/activations, which incurs non-negligible time and space overhead in hardware deployment. In this study, we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can maintain the strong representation ability of nonuniform methods while being hardware-friendly and efficient as the uniform quantization for model inference. We achieve this through learning the flexible in-equidistant input thresholds to better fit the underlying distribution while quantizing these real-valued inputs into equidistant output levels. To train the quantized network with learnable input thresholds, we introduce a generalized straight-through estimator (G-STE) for intractable backward derivative calculation w.r.t. threshold parameters. Additionally, we consider entropy preserving regularization to further reduce information loss in weight quantization. Even under this adverse constraint of imposing uniformly quantized weights and activations, our N2UQ outperforms state-of-the-art nonuniform quantization methods by 0.5\u223c1.7% on ImageNet, demonstrating the contribution of N2UQ design. Code and models are available at: https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Nonuniform-to-uniform_quantization%3A_Towards_accurate_quantization_via_generalized_straight-through_estimation/","title":"Nonuniform-to-uniform quantization: Towards accurate quantization via generalized straight-through estimation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Nonuniform-to-uniform_quantization%3A_Towards_accurate_quantization_via_generalized_straight-through_estimation/#summary","title":"Summary","text":"<p>The paper proposes Nonuniform-to-Uniform Quantization (N2UQ), a method that can maintain the representation ability of nonuniform quantization while being hardware-friendly for model inference by learning flexible in-equidistant input thresholds to quantize inputs efficiently into equidistant output levels. The proposed approach outperforms state-of-the-art nonuniform quantization methods in ImageNet by 0.5\u223c1.7%."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Nonuniform-to-uniform_quantization%3A_Towards_accurate_quantization_via_generalized_straight-through_estimation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Nonuniform-to-uniform_quantization%3A_Towards_accurate_quantization_via_generalized_straight-through_estimation/#content","title":"Content","text":"<p>The nonuniform quantization strategy for compressing neural networks usually achieves better performance than its counterpart, i.e., uniform strategy, due to its superior representational capacity. However, many nonuniform quantization methods overlook the complicated projection process in implementing the nonuniformly quantized weights/activations, which incurs non-negligible time and space overhead in hardware deployment. In this study, we propose Nonuniform-to-Uniform Quantization (N2UQ), a method that can maintain the strong representation ability of nonuniform methods while being hardware-friendly and efficient as the uniform quantization for model inference. We achieve this through learning the flexible in-equidistant input thresholds to better fit the underlying distribution while quantizing these real-valued inputs into equidistant output levels. To train the quantized network with learnable input thresholds, we introduce a generalized straight-through estimator (G-STE) for intractable backward derivative calculation w.r.t. threshold parameters. Additionally, we consider entropy preserving regularization to further reduce information loss in weight quantization. Even under this adverse constraint of imposing uniformly quantized weights and activations, our N2UQ outperforms state-of-the-art nonuniform quantization methods by 0.5\u223c1.7% on ImageNet, demonstrating the contribution of N2UQ design. Code and models are available at: https://github.com/liuzechun/Nonuniform-to-Uniform-Quantization.&gt;"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Octo%3A%7BINT8%7D_Training_with_Loss-aware_Compensation_and_Backward_Quantization_for_Tiny_On-device_Learning/","title":"Octo:{INT8} Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Octo%3A%7BINT8%7D_Training_with_Loss-aware_Compensation_and_Backward_Quantization_for_Tiny_On-device_Learning/#summary","title":"Summary","text":"<p> The paper proposes a unified scheme called Octo to automatically learn INT8 calibration parameters, compensate for losses caused by quantization and perform backward quantization to train DNN models with INT8 weights and activations. Octo is shown to achieve higher accuracy, lower latency, lower memory usage and faster convergence than state-of-the-art methods while preserving model interpretability. The proposed method aims to enable the deployment of tiny DNN models on edge devices."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Octo%3A%7BINT8%7D_Training_with_Loss-aware_Compensation_and_Backward_Quantization_for_Tiny_On-device_Learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Octo%3A%7BINT8%7D_Training_with_Loss-aware_Compensation_and_Backward_Quantization_for_Tiny_On-device_Learning/#content","title":"Content","text":"<p>In this paper, we propose Octo, a unified scheme that automatically learns INT8  calibration parameters, compensates for losses caused by quantization, and performs backward quantization to train DNN models with INT8 weights and activations. Our experiments on datasets and networks of various sizes show that Octo can achieve higher accuracy, lower latency, lower memory usage, and faster convergence than state-of-the-art methods while preserving model interpretability. With Octo, we aim to facilitate the deployment of tiny DNN models on edge devices."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On-device_training_under_256kb_memory/","title":"On-device training under 256kb memory","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On-device_training_under_256kb_memory/#summary","title":"Summary","text":"<p>Summary: The paper discusses an algorithm-system co-design framework to enable on-device training with limited memory resources, specifically 256KB, which is beneficial in protecting user privacy since data does not need to be transferred to the cloud.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On-device_training_under_256kb_memory/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On-device_training_under_256kb_memory/#content","title":"Content","text":"<p>On-device training enables the model to adapt to new data collected from the sensors by \ufb01ne-tuning a pre-trained model. Users can bene\ufb01t from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_learning_semantic_representations_for_million-scale_free-hand_sketches/","title":"On learning semantic representations for million-scale free-hand sketches","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_learning_semantic_representations_for_million-scale_free-hand_sketches/#summary","title":"Summary","text":"<p> The paper proposes a dual-branch CNN-RNN network architecture for learning semantic representations for million-scale free-hand sketches. The proposed architecture is used for hashing retrieval and zero-shot recognition on million-scale sketches, and the authors conduct experiments on millions of sketches and outperform the state-of-the-art results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_learning_semantic_representations_for_million-scale_free-hand_sketches/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_learning_semantic_representations_for_million-scale_free-hand_sketches/#content","title":"Content","text":"<p> In this paper, the authors propose a dual-branch CNN-RNN network architecture to learn semantic representations for million-scale free-hand sketches. They explore two practical settings, hashing retrieval and zero-shot recognition on million-scale sketches using this architecture. They propose a deep hashing model for sketch retrieval and a deep embedding model for sketch zero-shot recognition. The authors conducted experiments on millions of sketches using their proposed models and outperformed the state-of-the-art results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_periodic_functions_as_regularizers_for_quantization_of_neural_networks/","title":"On periodic functions as regularizers for quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_periodic_functions_as_regularizers_for_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>The paper proposes a method for quantizing neural network models by adding periodic functions as regularizers during training. The resulting quantized models maintain their accuracy and exhibit the same accuracy as the original ones on CIFAR-10 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_periodic_functions_as_regularizers_for_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_periodic_functions_as_regularizers_for_quantization_of_neural_networks/#content","title":"Content","text":"<p> Deep learning models require a significant amount of resources during training and inference, particularly when smaller devices like smartphones or embedded systems are used. Due to the limitations of these devices' size, temperature, and power budget, quantizing the model parameters into discrete integer numbers is a common technique to decrease their storage and compute requirements. This paper proposes an algorithm based on periodic functions like continuous trigonometric sine or cosine, and non-continuous hat functions as regularizers, to perform quantization of neural network models. By adding the sum over the model parameters during training, the weights are pushed into discrete points, which can be encoded as integers. The frequency and amplitude hyper-parameters of these functions can be adjusted, and the resulting quantized models maintain their accuracy. The experimentation showed that models quantized using this technique exhibit the same accuracy as the original ones on CIFAR-10 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_adversarial_robustness_of_quantized_neural_networks/","title":"On the adversarial robustness of quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_adversarial_robustness_of_quantized_neural_networks/#summary","title":"Summary","text":"<p>This paper explores the impact of quantization on the adversarial robustness of neural networks. The study found that quantization can improve or degrade adversarial robustness depending on the attack strength. This highlights the need to understand the trade-offs between model compression and robustness against adversarial attacks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_adversarial_robustness_of_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_adversarial_robustness_of_quantized_neural_networks/#content","title":"Content","text":"<p>Reducing the size of neural network models is a critical step in moving AI from a cloud-centric to an edge-centric (i.e. on-device) compute paradigm. However, it is currently unclear how model compression techniques may affect the robustness of AI algorithms against adversarial attacks. This paper explores the effect of quantization, one of the most common compression techniques, on the adversarial robustness of neural networks. Results indicate that for simple gradient-based attacks, quantization can either improve or degrade adversarial robustness depending on the attack strength."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_capacity-achieving_input_of_channels_with_phase_quantization/","title":"On the capacity-achieving input of channels with phase quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_capacity-achieving_input_of_channels_with_phase_quantization/#summary","title":"Summary","text":"<p>Summary: The paper studies the optimal input distributions for four different channel models with multi-bit phase quantization. They show that the capacity-achieving distribution for a complex Gaussian channel with b-bit phase-quantized output is a rotated 2b-phase shift keying (PSK) and this continues to hold under noncoherent fast fading Rician channels with b-bit phase quantization when line-of-sight (LoS) is present. Furthermore, they identify 2\u00192b-symmetry and constant amplitude as the necessary and sufficient conditions for the ergodic capacity-achieving input distribution, which a 2b-PSK satisfies.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_capacity-achieving_input_of_channels_with_phase_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_capacity-achieving_input_of_channels_with_phase_quantization/#content","title":"Content","text":"<p>Several information-theoretic studies have identified the capacity-achieving input distributions for different fading channels with 1-bit in-phase and quadrature output quantization, but an exact characterization of the capacity-achieving input distribution for channels with multi-bit phase quantization has not been provided. In this paper, the authors consider four different channel models with multi-bit phase quantization and identify the optimal input distribution for each channel model. They prove that the capacity-achieving distribution for a complex Gaussian channel with b-bit phase-quantized output is a rotated 2b-phase shift keying (PSK). The analysis is then extended to multiple fading scenarios. They show that the optimality of rotated 2b-PSK continues to hold under noncoherent fast fading Rician channels with b-bit phase quantization when line-of-sight (LoS) is present. When channel state information (CSI) is available at the receiver, they identify 2\u00192b-symmetry and constant amplitude as the necessary and sufficient conditions for the ergodic capacity-achieving input distribution, which a 2b-PSK satisfies. Finally, an optimum power control scheme is presented which achieves ergodic capacity when CSI is also available at the transmitter. The authors aim to extend the capacity results of 1-bit I/Q quantization to multi-bit quantization, specifically multi-bit phase quantization instead of conventional I/Q quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_universal_approximability_and_complexity_bounds_of_quantized_relu_neural_networks/","title":"On the universal approximability and complexity bounds of quantized relu neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_universal_approximability_and_complexity_bounds_of_quantized_relu_neural_networks/#summary","title":"Summary","text":"<p> The paper studies the representation power of quantized neural networks and proves the universal approximability of quantized ReLU networks on a wide class of functions. Upper bounds are provided on the number of weights and memory size for a given approximation error bound and the bit-width of weights for function-independent and function-dependent structures. For an approximation error bound of \u03b5, the number of weights needed by a quantized network is no more than O(log5(1/\u03b5)) times that of an unquantized network. This overhead is much lower than the lower bound of the number of weights needed for the error bound, validating the empirical success of quantization techniques. The paper provides the first in-depth study on the complexity bounds of quantized neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_universal_approximability_and_complexity_bounds_of_quantized_relu_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/On_the_universal_approximability_and_complexity_bounds_of_quantized_relu_neural_networks/#content","title":"Content","text":"<p>Compression is a key step to deploy large neural networks on resource-constrained platforms. As a popular compression technique, quantization constrains the num- ber of distinct weight values and thus reducing the number of bits required to represent and store each weight. In this paper, we study the representation power of quantized neural networks. First, we prove the universal approximability of quantized ReLU networks on a wide class of functions. Then we provide upper bounds on the number of weights and the memory size for a given approximation error bound and the bit-width of weights for function-independent and function- dependent structures. Our results reveal that, to attain an approximation error bound of\u000f, the number of weights needed by a quantized network is no more thanO(log5(1=\u000f)) times that of an unquantized network. This overhead is of much lower order than the lower bound of the number of weights needed for the error bound, supporting the empirical success of various quantization techniques. To the best of our knowledge, this is the \ufb01rst in-depth study on the complexity bounds of quantized neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Once_quantization-aware_training%3A_High_performance_extremely_low-bit_architecture_search/","title":"Once quantization-aware training: High performance extremely low-bit architecture search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Once_quantization-aware_training%3A_High_performance_extremely_low-bit_architecture_search/#summary","title":"Summary","text":"<p> The paper proposes a joint training method combining Network Architecture Search and quantization with a shared step size to achieve accurate quantization in extremely low-bit cases. The method also introduces a bit-inheritance scheme to transfer quantized models to lower bits, reducing time cost and improving accuracy. State-of-the-art results are achieved compared to various architectures under different bit-widths, and quantization-friendly architectures are identified. The interaction between quantization and neural architectures is analyzed, and the authors release codes and models to the public."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Once_quantization-aware_training%3A_High_performance_extremely_low-bit_architecture_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Once_quantization-aware_training%3A_High_performance_extremely_low-bit_architecture_search/#content","title":"Content","text":"<p> In this paper, the authors propose a method for combining Network Architecture Search with quantization to enhance quantization accuracy in extremely low-bit cases. They introduce the joint training of architecture and quantization with a shared step size to acquire a large number of quantized models. A bit-inheritance scheme is also introduced to transfer quantized models to lower bits, reducing time cost and improving quantization accuracy. The proposed method achieves state-of-the-art results compared to various architectures under different bit-widths. The authors identify quantization-friendly architectures and analyze the interaction between quantization and neural architectures. Codes and models are released to the public."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_Loss_for_Quantization%3A_Deep_Hashing_with_Discrete_Wasserstein_Distributional_Matching/","title":"One Loss for Quantization: Deep Hashing with Discrete Wasserstein Distributional Matching","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_Loss_for_Quantization%3A_Deep_Hashing_with_Discrete_Wasserstein_Distributional_Matching/#summary","title":"Summary","text":"<p>Summary: The paper discusses the importance of producing balanced hash codes with low quantization error in deep supervised hashing methods. It argues that existing quantization approaches are not effective in achieving these objectives and proposes a computationally efficient distributional distance to reformulate the task of learning balanced codes with low quantization error. The proposed approach can be integrated into existing supervised hashing methods, resulting in substantially improved performance.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_Loss_for_Quantization%3A_Deep_Hashing_with_Discrete_Wasserstein_Distributional_Matching/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_Loss_for_Quantization%3A_Deep_Hashing_with_Discrete_Wasserstein_Distributional_Matching/#content","title":"Content","text":"<p>Image hashing is a principled approximate nearest neighbor approach to find similar items to a query in a large collection of images. Hashing aims to learn a binary-output function that maps an image to a binary vector. For optimal retrieval performance, producing balanced hash codes with low-quantization error to bridge the gap between the learning stage\u2019s continuous relaxation and the inference stage\u2019s discrete quantization is important. However, in the existing deep supervised hashing methods, coding balance and low-quantization error are difficult to achieve and involve several losses. We argue that this is because the existing quantization approaches in these methods are heuristically constructed and not effective to achieve these objectives. This paper considers an alternative approach to learning the quantization constraints. The task of learning balanced codes with low quantization error is reformulated as matching the learned distribution of the continuous codes to a pre-defined discrete, uniform distribution. This is equivalent to minimizing the distance between two distributions. We then propose a computationally efficient distributional distance by leveraging the discrete property of the hash functions. This distributional distance is a valid distance and enjoys lower time and sample complexities. The proposed single-loss quantization objective can be integrated into any existing supervised hashing method to improve code balance and quantization error. Experiments confirm that the proposed approach substantially improves the performance of several representative hashing methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_model_for_all_quantization%3A_A_quantized_network_supporting_hot-swap_bit-width_adjustment/","title":"One model for all quantization: A quantized network supporting hot-swap bit-width adjustment","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_model_for_all_quantization%3A_A_quantized_network_supporting_hot-swap_bit-width_adjustment/#summary","title":"Summary","text":"<p> The paper proposes a method for training neural network models that can be effectively used for model quantization of bit-widths ranging from 8-bit to 1-bit. The proposed technique uses multiscale quantization with wavelet decomposition and reconstruction to diversify weights, which leads to significant improvement in performance in ultra-low bit-width situations. The method provides specific quantization strategies to different candidates, which can be hot-swapped, and eliminates the need for fine-tuning the quantized model or minimizing the quantization noise. The experimental results show that the proposed method performs comparably to specialized models previously trained at the same precision in ImageNet and COCO datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_model_for_all_quantization%3A_A_quantized_network_supporting_hot-swap_bit-width_adjustment/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_model_for_all_quantization%3A_A_quantized_network_supporting_hot-swap_bit-width_adjustment/#content","title":"Content","text":"<p> As an effective technique to achieve implementation of deep neural networks on edge devices, model quantization has been successfully applied in many practical applications. In this work, a method is proposed to train a model for all quantization that supports diverse bit-widths (e.g., from 8-bit to 1-bit) to satisfy the online quantization bit-width adjustment without the need for fine-tuning the quantized model or minimizing the quantization noise. The proposed method provides specific quantization strategies for different candidates through multiscale quantization, which is hot-swappable. Wavelet decomposition and reconstruction are used to increase the diversity of weights, thus significantly improving the performance of each quantization candidate, especially at ultra-low bit-widths (e.g., 3-bit, 2-bit, and 1-bit). Experimental results on ImageNet and COCO demonstrate that the proposed method achieves accuracy comparable to that of dedicated models trained at the same precision."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_weight_bitwidth_to_rule_them_all/","title":"One weight bitwidth to rule them all","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_weight_bitwidth_to_rule_them_all/#summary","title":"Summary","text":"<p>Summary: The paper proposes aligning all weights to the same model size using a width-multiplier to achieve better accuracy compared to mixed-precision quantization with different bitwidths. It suggests that a single weight bitwidth throughout the network shows superior results for model compression.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_weight_bitwidth_to_rule_them_all/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/One_weight_bitwidth_to_rule_them_all/#content","title":"Content","text":"<p>Weight quantization is a widely used technique for deep ConvNets to improve the parameter efficiency of ConvNets while maintaining accuracy. However, different tasks may result in different bitwidths for quantization, which creates complexity for software and hardware support. In this paper, we propose to align all weights to the same model size using a width-multiplier, and we show that using a single weight bitwidth throughout the network can achieve better accuracy compared to mixed-precision quantization targeting zero accuracy degradation when both have the same model size. We also suggest that when the number of channels becomes a target hyperparameter, a single weight bitwidth throughout the network shows superior results for model compression."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Opq%3A_Compressing_deep_neural_networks_with_one-shot_pruning-quantization/","title":"Opq: Compressing deep neural networks with one-shot pruning-quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Opq%3A_Compressing_deep_neural_networks_with_one-shot_pruning-quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a One-shot Pruning-Quantization (OPQ) method to compress and allocate compressions like sparsity and quantization codebook in neural networks that uses pre-trained model only. The paper suggests a unified channel-wise quantization method that enforces all channels to share a common codebook to reduce extra overhead in traditional channel-wise quantization. The experiments on AlexNet/MobileNet-V1/ResNet-50 show that OPQ method achieves high compression rates, more efficiency, and improved accuracy than the existing state-of-the-art methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Opq%3A_Compressing_deep_neural_networks_with_one-shot_pruning-quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Opq%3A_Compressing_deep_neural_networks_with_one-shot_pruning-quantization/#content","title":"Content","text":"<p>As Deep Neural Networks (DNNs) usually are overparam- eterized and have millions of weight parameters, it is chal- lenging to deploy these large DNN models on resource- constrained hardware platforms, e.g., smartphones. Numerous network compression methods such as pruning and quant- ization are proposed to reduce the model size signi\ufb01cantly, of which the key is to \ufb01nd suitable compression allocation (e.g., pruning sparsity and quantization codebook) of each layer. Existing solutions obtain the compression allocation in an iterative/manual fashion while \ufb01netuning the compressed model, thus suffering from the ef\ufb01ciency issue. Different from the prior art, we propose a novel One-shot Pruning- Quantization (OPQ) in this paper, which analytically solves the compression allocation with pre-trained weight parame- ters only. During \ufb01netuning, the compression module is \ufb01xed and only weight parameters are updated. To our knowledge, OPQ is the \ufb01rst work that reveals pre-trained model is suf- \ufb01cient for solving pruning and quantization simultaneously, without any complex iterative/manual optimization at the \ufb01netuning stage. Furthermore, we propose a uni\ufb01ed channel- wise quantization method that enforces all channels of each layer to share a common codebook, which leads to low bit- rate allocation without introducing extra overhead brought by traditional channel-wise quantization. Comprehensive exper- iments on ImageNet with AlexNet/MobileNet-V1/ResNet-50 show that our method improves accuracy and training ef\ufb01- ciency while obtains signi\ufb01cantly higher compression rates compared to the state-of-the-art."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_Brain_Compression%3A_A_framework_for_accurate_post-training_quantization_and_pruning/","title":"Optimal Brain Compression: A framework for accurate post-training quantization and pruning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_Brain_Compression%3A_A_framework_for_accurate_post-training_quantization_and_pruning/#summary","title":"Summary","text":"<p>The paper introduces a new framework for compression of deep neural networks that covers weight pruning and quantization in a unified setting. The approach is based on an efficient realization of the Optimal Brain Surgeon (OBS) framework and extends to cover weight quantization. The method improves upon the practical performance of existing post-training methods and enables the accurate compound application of both pruning and quantization in a post-training setting. The experimental results show significant improvements in the compression-accuracy trade-offs of existing methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_Brain_Compression%3A_A_framework_for_accurate_post-training_quantization_and_pruning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_Brain_Compression%3A_A_framework_for_accurate_post-training_quantization_and_pruning/#content","title":"Content","text":"<p>We introduce a new compression framework for deep neural networks (DNNs) which covers both weight pruning and quantization in a uni\ufb01ed setting, is time- and space-ef\ufb01cient and considerably improves upon the practical performance of existing post-training methods. Our approach is based on an exact and ef\ufb01cient realization of the classical Optimal Brain Surgeon (OBS) framework of [LeCun, Denker, and Solla, 1990] extended to also cover weight quantization at the scale of modern DNNs. Our experimental results show that it can significantly improve upon the compression-accuracy trade-offs of existing post-training methods, and that it can enable the accurate compound application of both pruning and quantization in a post-training setting."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_clipping_and_magnitude-aware_differentiation_for_improved_quantization-aware_training/","title":"Optimal clipping and magnitude-aware differentiation for improved quantization-aware training","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_clipping_and_magnitude-aware_differentiation_for_improved_quantization-aware_training/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method called Optimally Clipped Tensors and Vectors (OCTA V) for determining optimal clipping scalars using the fast Newton-Raphson method in quantization-aware training (QAT). The proposed method improves accuracy by reducing noise in quantization operations. The paper also proposes magnitude-aware differentiation to further improve accuracy. Experimental results show that OCTA V-enabled QAT achieves state-of-the-art accuracy on multiple tasks with low precision.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_clipping_and_magnitude-aware_differentiation_for_improved_quantization-aware_training/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_clipping_and_magnitude-aware_differentiation_for_improved_quantization-aware_training/#content","title":"Content","text":"<p> Data clipping is proposed as a method to reduce noise in quantization operations and improve accuracy in quantization-aware training (QAT). However, current methods for setting clipping threshold scalars are not optimal. In this paper, the authors propose an algorithm called Optimally Clipped Tensors and Vectors (OCTA V) that recursively determines the optimal clipping scalars using the fast Newton-Raphson method. Additionally, common gradient estimation techniques in QAT are shown to have limitations and magnitude-aware differentiation is proposed to further improve accuracy. Experimental results demonstrate that OCTA V-enabled QAT achieves state-of-the-art accuracy on multiple tasks with low precision (4-to-6-bits) without modifications to the baseline training recipe."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_gradient_quantization_condition_for_communication-efficient_distributed_training/","title":"Optimal gradient quantization condition for communication-efficient distributed training","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_gradient_quantization_condition_for_communication-efficient_distributed_training/#summary","title":"Summary","text":"<p> The paper discusses the issue of costly communication in deep neural network training with multiple devices in computer vision applications. Gradient quantization is proposed as a solution to reduce communication costs, however, it can lead to quantization error and a resulting model performance degradation. The paper introduces two novel quantization schemes, biased BinGrad and unbiased ORQ, for binary and multi-level gradient quantization respectively, which determine the optimal quantization levels through the deduced optimal condition. CIFAR and ImageNet datasets with popular convolutional neural networks were used in the experiments, and the proposed methods were found to be superior."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_gradient_quantization_condition_for_communication-efficient_distributed_training/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_gradient_quantization_condition_for_communication-efficient_distributed_training/#content","title":"Content","text":"<p> The communication of gradients is costly for training deep neural networks with multiple devices in computer vision applications. Gradient quantization is one of the common methods to reduce communication costs, but it can lead to quantization error in the training and result in model performance degradation. In this work, we deduce the optimal condition of both the binary and multi-level gradient quantization for any gradient distribution. Based on the optimal condition, we develop two novel quantization schemes: biased BinGrad and unbiased ORQ for binary and multi-level gradient quantization respectively, which dynamically determine the optimal quantization levels. Extensive experimental results on CIFAR and ImageNet datasets with several popular convolutional neural networks show the superiority of our proposed methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_precoding_for_multiuser_MIMO_systems_with_phase_quantization_and_PSK_modulation_via_branch-and-bound/","title":"Optimal precoding for multiuser MIMO systems with phase quantization and PSK modulation via branch-and-bound","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_precoding_for_multiuser_MIMO_systems_with_phase_quantization_and_PSK_modulation_via_branch-and-bound/#summary","title":"Summary","text":"<p>Summary:  The paper proposes an optimal precoding algorithm for MIMO systems through phase quantization and PSK modulation to maximize the minimum distance of the decision threshold at receivers using a branch-and-bound strategy, which results in low bit error rates. It is generalizable for phase quantizers with an arbitrary number of phases using PSK modulation, and it outperforms existing methods. The proposed approach demonstrates significantly lower complexity than exhaustive search.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_precoding_for_multiuser_MIMO_systems_with_phase_quantization_and_PSK_modulation_via_branch-and-bound/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_precoding_for_multiuser_MIMO_systems_with_phase_quantization_and_PSK_modulation_via_branch-and-bound/#content","title":"Content","text":"<p> In this study, the authors propose an optimal precoding algorithm for MIMO systems with phase quantization and PSK modulation, where the algorithm maximizes the minimum distance to the decision threshold at the receivers using a branch-and-bound strategy. The proposed algorithm is constrained to constant envelope signals and phase quantization, and is superior to existing methods in terms of bit error rate. The authors generalize their work for phase quantizers with an arbitrary number of phases at the transmit antennas and PSK modulation, which is non-trivial due to the complexity of PSK symbols. The proposed precoder is optimal in terms of the Max-Min DDT criterion, obtained by using a sophisticated branch-and-bound strategy. The authors present numerical results showing that their proposed approach has significantly lower complexity than exhaustive search."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_quantization_using_scaled_codebook/","title":"Optimal quantization using scaled codebook","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_quantization_using_scaled_codebook/#summary","title":"Summary","text":"<p> The paper focuses on the problem of quantizing scalar data points with a fixed codebook containing K entries that can be rescaled. The authors derive an algorithm that is guaranteed to find the optimal quantization parameters for any fixed codebook regardless of data distribution. The algorithm has a time complexity of O(NKlogK). The authors also apply their algorithm to various neural network quantization problems and show its effectiveness."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_quantization_using_scaled_codebook/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimal_quantization_using_scaled_codebook/#content","title":"Content","text":"<p> We study the problem of quantizing Nsorted, scalar data points with a fixed codebook containing K entries that are allowed to be rescaled. By studying the properties of the optimal quantizer, we derive an O(NKlogK) algorithm that is guaranteed to find the optimal quantization parameters for any fixed codebook regardless of data distribution. We apply our algorithm to synthetic and real-world neural network quantization problems and demonstrate the effectiveness of our approach."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimizing_jpeg_quantization_for_classification_networks/","title":"Optimizing jpeg quantization for classification networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimizing_jpeg_quantization_for_classification_networks/#summary","title":"Summary","text":"<p>Summary: The paper explores using customized Q-tables optimized for specific vision networks in JPEG compression for deep learning. A simple sorted random sampling method can exceed the performance of the standard JPEG Q-table, and hyper-parameter tuning techniques can improve the compression rate by 10% to 200% when accuracy is fixed or boost accuracy up to 2% at the same compression rate.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimizing_jpeg_quantization_for_classification_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimizing_jpeg_quantization_for_classification_networks/#content","title":"Content","text":"<p> Deep learning requires lossy image compression, specifically JPEG compression. The degree of quantization in JPEG is controlled by the Q-table, which determines the quality of the encoded image and compression ratio. In this work, we explore whether customized Q-tables optimized for specific vision networks can provide better quality-size trade-offs than those designed for human perception or minimal distortion. We reconstruct ImageNet test sets with higher resolution to test the effect of JPEG compression under novel Q-tables. We find that a simple sorted random sampling method can exceed the performance of the standard JPEG Q-table, and hyper-parameter tuning techniques can improve the compression rate by 10% to 200% when accuracy is fixed or boost accuracy up to 2% at the same compression rate."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimizing_medical_image_classification_models_for_edge_devices/","title":"Optimizing medical image classification models for edge devices","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimizing_medical_image_classification_models_for_edge_devices/#summary","title":"Summary","text":"<p>Summary: This paper investigates the use of model quantization and GPU-acceleration to improve the efficiency of machine learning algorithms for medical diagnostics on edge devices. Quantization techniques were employed and tested, which resulted in a 2-4x reduction in model size and small decreases in mean AUC-ROC score of 0.0%-0.9%. Integer quantization was found to improve inference latency by up to 57% on ARM architectures, but significant latency increases were observed on x86 processors. The use of GPU acceleration improved inference latency, but kernel launch overhead outweighed the benefits. Overall, optimization of diagnostic models has the potential to increase their usefulness in low-resource environments, but improvements are dependent on the context and architecture of the devices being used.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimizing_medical_image_classification_models_for_edge_devices/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Optimizing_medical_image_classification_models_for_edge_devices/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/PSAQ-ViT_V2%3A_Towards_Accurate_and_General_Data-Free_Quantization_for_Vision_Transformers/","title":"PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/PSAQ-ViT_V2%3A_Towards_Accurate_and_General_Data-Free_Quantization_for_Vision_Transformers/#summary","title":"Summary","text":"<p> PSAQ-ViT V2 is a data-free quantization framework proposed in this research paper for vision transformers, which uses an adaptive teacher-student strategy and patch similarity metric, and is compatible with a wide range of vision tasks and models, achieving competitive results without real-world data."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/PSAQ-ViT_V2%3A_Towards_Accurate_and_General_Data-Free_Quantization_for_Vision_Transformers/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/PSAQ-ViT_V2%3A_Towards_Accurate_and_General_Data-Free_Quantization_for_Vision_Transformers/#content","title":"Content","text":"<p> In this research paper, the authors propose a more accurate and general data-free quantization framework for vision transformers called PSAQ-ViT V2. It is built on top of patch similarity metric introduced in PSAQ-ViT and includes an adaptive teacher-student strategy which significantly improves the accuracy of the quantized model. The proposed scheme is compatible with a broad range of vision tasks and models without relying on auxiliary category guidance. The proposed approach is evaluated on various models on image classification, object detection, and semantic segmentation tasks, and consistently achieves competitive results without access to real-world data, showing potential as a powerful baseline on data-free quantization for ViTs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/PTQ4ViT%3A_Post-Training_Quantization_Framework_for_Vision_Transformers/","title":"PTQ4ViT: Post-Training Quantization Framework for Vision Transformers","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/PTQ4ViT%3A_Post-Training_Quantization_Framework_for_Vision_Transformers/#summary","title":"Summary","text":"<p>This paper proposes the twin uniform quantization method and a Hessian guided metric to evaluate scaling factors in order to compress neural networks for vision transformers. They also develop an efficient framework called PTQ4ViT for fast quantization. Experimental results show near-lossless prediction accuracy achieved with less than 0.5% drop at 8-bit quantization on the ImageNet classification task."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/PTQ4ViT%3A_Post-Training_Quantization_Framework_for_Vision_Transformers/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/PTQ4ViT%3A_Post-Training_Quantization_Framework_for_Vision_Transformers/#content","title":"Content","text":"<p>Quantization is a powerful technique to compress neural networks, but previous post-training quantization methods did not perform well on vision transformers. In this paper, we propose the twin uniform quantization method to reduce the quantization error, and a Hessian guided metric to evaluate different scaling factors. We also develop an efficient framework, PTQ4ViT, that enables fast quantization of vision transformers. Experiments show that the quantized vision transformers achieve near-lossless prediction accuracy, with less than 0.5% drop at 8-bit quantization, on the ImageNet classification task."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pact%3A_Parameterized_clipping_activation_for_quantized_neural_networks/","title":"Pact: Parameterized clipping activation for quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pact%3A_Parameterized_clipping_activation_for_quantized_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper presents a novel activation quantization scheme called PACT, which optimizes the activation clipping parameter alpha during training to find the right quantization scale. This technique enables quantizing activations to arbitrary bit precisions while still achieving better accuracy than existing quantization schemes. The paper demonstrates that both weights and activations can be quantized to 4-bits of precision, achieving accuracy comparable to full precision networks across multiple models and datasets. The proposed reduced-precision computational units can also enable significant inferencing performance improvement due to a reduction in accelerator compute engine size and on-chip memory usage.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pact%3A_Parameterized_clipping_activation_for_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pact%3A_Parameterized_clipping_activation_for_quantized_neural_networks/#content","title":"Content","text":"<p>  This paper proposes a novel quantization scheme for activations during training, called PArameterized Clipping acTivation (PACT). PACT enables neural networks to work well with ultra low precision weights and activations without any significant accuracy degradation. The activation clipping parameter alpha is optimized during training to find the right quantization scale. This technique allows quantizing activations to arbitrary bit precisions, while achieving much better accuracy relative to published state-of-the-art quantization schemes. The paper demonstrates that both weights and activations can be quantized to 4-bits of precision while still achieving accuracy comparable to full precision networks across a range of popular models and datasets. The reduced-precision computational units can also enable a super-linear improvement in inferencing performance due to a significant reduction in the area of accelerator compute engines coupled with the ability to retain the quantized model and activation data in on-chip memories."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pams%3A_Quantized_super-resolution_via_parameterized_max_scale/","title":"Pams: Quantized super-resolution via parameterized max scale","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pams%3A_Quantized_super-resolution_via_parameterized_max_scale/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new quantization scheme called PArameterized Max Scale (PAMS) for super-resolution (SR) tasks that adaptively explores the upper bound of the quantization range and introduces a structured knowledge transfer (SKT) loss for fine-tuning of the quantized network. Experiments showed that PAMS scheme can compress and accelerate the existing SR models and achieve state-of-the-art results on the Set5 benchmark dataset.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pams%3A_Quantized_super-resolution_via_parameterized_max_scale/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pams%3A_Quantized_super-resolution_via_parameterized_max_scale/#content","title":"Content","text":"<p> Deep convolutional neural networks have become standard for super-resolution (SR) task. However, the huge memory cost and computation overhead limit their practical deployment especially on resource-limited devices. Previous quantization approaches have relied on fixed-point operations, which may lead to significant performance loss when both weights and activations are quantized. To address these issues, a new quantization scheme is proposed named PArameterized Max Scale (PAMS) that adaptively explores the upper bound of the quantization range by applying the trainable truncated parameter. A structured knowledge transfer (SKT) loss is also introduced for fine-tuning of the quantized network. Experiments showed that the proposed PAMS scheme can well compress and accelerate the existing SR models, and achieve a new state-of-the-art on the Set5 benchmark. Keywords: Super Resolution Network Quantization"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Parallelized_Rate-Distortion_Optimized_Quantization_Using_Deep_Learning/","title":"Parallelized Rate-Distortion Optimized Quantization Using Deep Learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Parallelized_Rate-Distortion_Optimized_Quantization_Using_Deep_Learning/#summary","title":"Summary","text":"<p> This work proposes a neural network-based approach to address the limitations of Rate-Distortion Optimized Quantization (RDOQ) algorithms in real-time hardware encoders. The approach focuses on learning to trade-off rate and distortion during offline supervised training, and it yields a significant reduction in bit-rate with small increases in distortion. The neural network-based approach uses standard arithmetic operations that can be executed on existing neural network hardware, and it does not require additional area-on-chip for dedicated RDOQ circuitry. The evaluation of two classes of neural networks, fully-convolutional and auto-regressive, as post-quantization steps show that they reach 45% of the performance of the iterative HM RDOQ algorithm and achieve 1.64% BD-rate savings on luminosity compared to the HM scalar quantization (SQ) anchor."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Parallelized_Rate-Distortion_Optimized_Quantization_Using_Deep_Learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Parallelized_Rate-Distortion_Optimized_Quantization_Using_Deep_Learning/#content","title":"Content","text":"<p>Rate-Distortion Optimized Quantization (RDOQ) has played an important role in the coding performance of recent video compression standards such as H.264/A VC, H.265/HEVC, VP9 and A V1. This scheme yields significant reductions in bit-rate at the expense of relatively small increases in distortion. Typically, RDOQ algorithms are prohibitively expensive to implement on real-time hardware encoders due to their sequential nature and their need to frequently obtain entropy coding costs. This work addresses this limitation using a neural network-based approach, which learns to trade-off rate and distortion during offline supervised training. As these networks are based solely on standard arithmetic operations that can be executed on existing neural network hardware, no additional area-on-chip needs to be reserved for dedicated RDOQ circuitry. We train two classes of neural networks, a fully-convolutional network and an auto-regressive network , and evaluate each as a post-quantization step designed to refine cheap quantization schemes such as scalar quantization (SQ). Both network architectures are designed to have a low computational overhead. After training they are integrated into the HM 16.20 implementation of HEVC, and their video coding performance is evaluated on a subset of the H.266/VVC SDR common test sequences. Comparisons are made to RDOQ and SQ implementations in HM 16.20. Our method achieves 1.64% BD-rate savings on luminosity compared to the HM SQ anchor, and on average reaches 45% of the performance of the iterative HM RDOQ algorithm."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pareto-optimal_quantized_resnet_is_mostly_4-bit/","title":"Pareto-optimal quantized resnet is mostly 4-bit","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pareto-optimal_quantized_resnet_is_mostly_4-bit/#summary","title":"Summary","text":"<p>Summary: This paper investigates the effects of quantization on ResNet models and their compute cost-quality tradeoff curves. The results suggest that 4-bit quantized models yield the best Pareto curve, which dominates the bfloat16 compute cost-quality tradeoff curve. The paper achieves state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training. The authors motivate further research into optimal numeric formats for quantization and the development of machine learning accelerators supporting these formats.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pareto-optimal_quantized_resnet_is_mostly_4-bit/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pareto-optimal_quantized_resnet_is_mostly_4-bit/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Permute%2C_quantize%2C_and_fine-tune%3A_Efficient_compression_of_neural_networks/","title":"Permute, quantize, and fine-tune: Efficient compression of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Permute%2C_quantize%2C_and_fine-tune%3A_Efficient_compression_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a technique named PQF which is used to compress large neural networks for deployment in resource-constrained platforms. PQF uses rate-distortion theory to search for permutations of the network weights that yield functionally equivalent, easier-to-quantize networks. PQF shows promising results in reducing the gap with the uncompressed model in image classification, object detection, and segmentation tasks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Permute%2C_quantize%2C_and_fine-tune%3A_Efficient_compression_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Permute%2C_quantize%2C_and_fine-tune%3A_Efficient_compression_of_neural_networks/#content","title":"Content","text":"<p> Compressing large neural networks for deployment in resource-constrained platforms is crucial. Vector quantization (VQ) has emerged as an efficient technique that compresses multiple parameters into a single code. However, determining which parameters to compress jointly poses a problem. In this paper, we propose Permute, Quantize and Fine-tune (PQF), which leverages rate-distortion theory to search for permutations of the network weights that yield functionally equivalent, easier-to-quantize networks. PQF searches for permutations, codes and codebooks that minimize the reconstruction error of the network weights and uses gradient-based optimization to recover the accuracy of the uncompressed network. PQF shows promising results, reducing the gap with the uncompressed model by 40 to 70% w.r.t. the current state of the art, in image classification, object detection, and segmentation tasks. All the experiments can be reproduced using the code provided in https://github.com/uber-research/permute-quantize-finetune."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Position-based_scaled_gradient_for_model_quantization_and_pruning/","title":"Position-based scaled gradient for model quantization and pruning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Position-based_scaled_gradient_for_model_quantization_and_pruning/#summary","title":"Summary","text":"<p>Summary: The article proposes PSG (position-based scaled gradient) that scales the gradient based on the weight vector position, which acts as a regularizer to the weight vectors that is favorable for model compression domains such as quantization and pruning. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart, resulting in the versatile deployment of a model either as an uncompressed mode or as a compressed mode. The proposed PSG is effective in both domains of pruning and quantization even for extremely low bits, as demonstrated in experimental results on CIFAR-10/100 and ImageNet datasets. The code for PSG is released in Github.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Position-based_scaled_gradient_for_model_quantization_and_pruning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Position-based_scaled_gradient_for_model_quantization_and_pruning/#content","title":"Content","text":"<p> We propose the position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly. First, we theoretically show that applying PSG to the standard gradient descent (GD), which is called PSGD, is equivalent to the GD in the warped weight space, a space made by warping the original weight space via an appropriately designed invertible function. Second, we empirically show that PSG acting as a regularizer to the weight vectors is favorable for model compression domains such as quantization and pruning. PSG reduces the gap between the weight distributions of a full-precision model and its compressed counterpart. This enables the versatile deployment of a model either as an uncompressed mode or as a compressed mode depending on the availability of resources. The experimental results on CIFAR-10/100 and ImageNet datasets show the effectiveness of the proposed PSG in both domains of pruning and quantization even for extremely low bits. The code is released in Github2."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-Training_Sparsity-Aware_Quantization/","title":"Post-Training Sparsity-Aware Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-Training_Sparsity-Aware_Quantization/#summary","title":"Summary","text":"<p>Summary: This paper proposes a method called SPARQ for reducing computational and hardware resources required to run deep neural networks without significant accuracy degradation. SPARQ uses unstructured and dynamic activation sparsity in different representation granularities, such as employing 4-bit quantization by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, and quantizing pairs of 8-bit activations with one equal to zero by opportunistically using the other's 4-bit budget. The method achieves minor accuracy degradation and can be implemented in hardware, making it a promising alternative to traditional quantization methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-Training_Sparsity-Aware_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-Training_Sparsity-Aware_Quantization/#content","title":"Content","text":"<p> In this paper, a sparsity-aware quantization method (SPARQ) is proposed as an alternative to traditional post-training uniform quantization (PTQ) methods for reducing computational and hardware resources required to run deep neural networks (DNNs), while minimizing accuracy degradation. SPARQ leverages unstructured and dynamic activation sparsity in different representation granularities, such as employing 4-bit quantization by dynamically examining the bits of 8-bit values and choosing a window of 4 bits, and quantizing pairs of 8-bit activations with one equal to zero by opportunistically using the other's 4-bit budget. SPARQ achieves minor accuracy degradation and can be practically implemented in hardware, making it a promising alternative to traditional PTQ methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_BatchNorm_recalibration/","title":"Post-training BatchNorm recalibration","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_BatchNorm_recalibration/#summary","title":"Summary","text":"<p>Summary: The paper shows that recalibration of batch normalization layers' running mean and running variance statistics can recover model performance. The paper also revisits non-blocking simultaneous multithreading (NB-SMT) and explains that NB-SMT trades accuracy for performance by occasionally accommodating multiple threads into a shared MAC unit.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_BatchNorm_recalibration/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_BatchNorm_recalibration/#content","title":"Content","text":"<p>We show that substantial model performance can be recouped by post-training recalibration of the batch normalization layers\u2019 running mean and running variance statistics, given the presence of NB-SMT. We revisit non-blocking simultaneous multithreading (NB-SMT) introduced previously by Shomron and Weiser. NB-SMT trades accuracy for performance by occasionally \"squeezing\" more than one thread into a shared multiply-and-accumulate (MAC) unit."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_deep_neural_network_pruning_via_layer-wise_calibration/","title":"Post-training deep neural network pruning via layer-wise calibration","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_deep_neural_network_pruning_via_layer-wise_calibration/#summary","title":"Summary","text":"<p>The paper proposes a weight pruning method to compress deep neural networks. The method achieves high accuracy with commodity hardware, such as desktop CPUs or edge devices. A data-free extension is proposed that uses synthetic fractal images to achieve a state-of-the-art result for neural network pruning with a 50% sparsity rate. The method also achieved a 65% sparsity rate for ResNet50 on ImageNet with a 1% top@1 accuracy drop. The code is available as a part of the OpenVINOTM Post-Training Optimization tool."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_deep_neural_network_pruning_via_layer-wise_calibration/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_deep_neural_network_pruning_via_layer-wise_calibration/#content","title":"Content","text":"<p>We present a post-training weight pruning method for deep neural networks that achieves accuracy levels tolerable for the production setting and that is sufficiently fast to be run on commodity hardware such as desktop CPUs or edge devices. We propose a data-free extension of the approach for computer vision models based on automatically-generated synthetic fractal images. We obtain state-of-the-art results for data-free neural network pruning, with \u223c1.5% top@1 accuracy drop for a ResNet50 on ImageNet at 50% sparsity rate. When using real data, we are able to get a ResNet50 model on ImageNet with 65% sparsity rate in 8-bit precision in a post-training setting with a \u223c1% top@1 accuracy drop. We release the code as a part of the OpenVINOTM Post-Training Optimization tool1."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_cross-platform_learned_image_compression/","title":"Post-training quantization for cross-platform learned image compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_cross-platform_learned_image_compression/#summary","title":"Summary","text":"<p>Summary: The paper proposes a solution to the non-deterministic calculation problem in image compression techniques with post-training quantization. The method maintains superior rate-distortion performance and allows for consistent cross-platform inference, making the application of learned image compression more promising.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_cross-platform_learned_image_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_cross-platform_learned_image_compression/#content","title":"Content","text":"<p> It has been observed that learned image compression techniques are more efficient than conventional image coding techniques and are practical in industrial applications. However, a significant issue that needs to be addressed is the non-deterministic calculation that leads to cross-platform inconsistency in probability prediction, causing decoding to fail. In this paper, we propose a solution to this problem by introducing post-training quantization, which makes model inference integer-arithmetic-only. Our method is much simpler than existing approaches and still maintains the superior rate-distortion performance of learned image compression. We further refine the discretization of the entropy parameters and extend deterministic inference to fit Gaussian mixture models. With our proposed method, the current state-of-the-art image compression models can infer in a cross-platform consistent manner, which makes the further application of learned image compression more promising."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_neural_networks_with_provable_guarantees/","title":"Post-training quantization for neural networks with provable guarantees","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_neural_networks_with_provable_guarantees/#summary","title":"Summary","text":"<p>Summary: The paper presents a generalized post-training neural network quantization method, GPFQ, which replaces network weights with quantized counterparts to achieve significant reductions in computation cost, memory usage, and power consumption. The authors propose modifications to promote weight sparsity and analyze associated errors, showing that the relative square error decays linearly with the number of weights for single-layer networks. The method is evaluated on various architectures and tested on ImageNet, demonstrating minor loss in accuracy compared to unquantized models. Standard modifications, such as bias correction and mixed precision quantization, also improve accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_neural_networks_with_provable_guarantees/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_neural_networks_with_provable_guarantees/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_vision_transformer/","title":"Post-training quantization for vision transformer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_vision_transformer/#summary","title":"Summary","text":"<p> The paper presents a post-training quantization algorithm that reduces memory storage and computational costs of vision transformers. The quantization task is optimized for low-bit quantization intervals for weights and inputs, with a ranking loss introduced to preserve the functionality of the attention mechanism. The proposed method outperforms state-of-the-art post-training quantization algorithms and is verified on several benchmark models and datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_vision_transformer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_for_vision_transformer/#content","title":"Content","text":"<p>In this paper, we present an effective post-training quantization algorithm for reducing the memory storage and computational costs of vision transformers. Basically, the quantization task can be regarded as finding the optimal low-bit quantization intervals for weights and inputs, respectively. To preserve the functionality of the attention mechanism, we introduce a ranking loss into the conventional quantization objective that aims to keep the relative order of the self-attention results after quantization. The effectiveness of the proposed method is verified on several benchmark models and datasets, which outperforms the state-of-the-art post-training quantization algorithms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_is_all_you_need_to_perform_cross-platform_learned_image_compression/","title":"Post-training quantization is all you need to perform cross-platform learned image compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_is_all_you_need_to_perform_cross-platform_learned_image_compression/#summary","title":"Summary","text":"<p>The paper proposes post-training quantization to solve the cross-platform inconsistencies in learned image compression. The proposed method simplifies the process yet provides a superior rate-distortion performance. The method allows the state-of-the-art compression models to infer cross-platform consistently, promising the development and practice of learned image compression."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_is_all_you_need_to_perform_cross-platform_learned_image_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_is_all_you_need_to_perform_cross-platform_learned_image_compression/#content","title":"Content","text":"<p>It has been witnessed that learned image compression has outperformed conventional image coding techniques and tends to be practical in industrial applications. One of the most critical issues that need to be considered is the non-deterministic calculation, which makes the probability prediction cross-platform inconsistent and frustrates successful decoding. We propose to solve this problem by introducing well-developed post-training quantization and making the model inference integer-arithmetic-only, which is much simpler than presently existing training and fine-tuning based approaches yet still keeps the superior rate-distortion performance of learned image compression. With our proposed methods, the current state-of-the-art image compression models can infer in a cross-platform consistent manner, which makes the further development and practice of learned image compression more promising."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_with_multiple_points%3A_Mixed_precision_without_mixed_precision/","title":"Post-training quantization with multiple points: Mixed precision without mixed precision","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_with_multiple_points%3A_Mixed_precision_without_mixed_precision/#summary","title":"Summary","text":"<p>Summary: The paper proposes a multipoint quantization method for the post-training quantization problem of discretizing the weights of pre-trained deep neural networks without re-training the model. They construct the multipoint quantization with an efficient greedy selection procedure and adaptively decide the number of low precision points on each quantized weight vector based on the error of its output. The method can be implemented by common operands with almost no memory and computation overhead. The proposed method outperforms a range of state-of-the-art methods on ImageNet classification, and it can be generalized to more challenging tasks like PASCAL VOC object detection.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_with_multiple_points%3A_Mixed_precision_without_mixed_precision/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post-training_quantization_with_multiple_points%3A_Mixed_precision_without_mixed_precision/#content","title":"Content","text":"<p> We consider the post-training quantization problem, which discretizes the weights of pre-trained deep neural networks without re-training the model. We propose multipoint quantization, a quantization method that approximates a full-precision weight vector using a linear combination of multiple vectors of low-bit numbers. Computationally, we construct the multipoint quantization with an efficient greedy selection procedure and adaptively decide the number of low precision points on each quantized weight vector based on the error of its output. Empirically, our method can be implemented by common operands, bringing almost no memory and computation overhead. We show that our method outperforms a range of state-of-the-art methods on ImageNet classification and it can be generalized to more challenging tasks like PASCAL VOC object detection."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post_training_4-bit_quantization_of_convolutional_networks_for_rapid-deployment/","title":"Post training 4-bit quantization of convolutional networks for rapid-deployment","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post_training_4-bit_quantization_of_convolutional_networks_for_rapid-deployment/#summary","title":"Summary","text":"<p>Summary: This paper presents a practical 4-bit post-training quantization approach for convolutional neural networks, that does not require training of quantized model or full dataset. The approach proposes three complementary methods for minimizing quantization errors at the tensor level, and achieves high accuracy just a few percent less than state-of-the-art baseline across various convolutional models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post_training_4-bit_quantization_of_convolutional_networks_for_rapid-deployment/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Post_training_4-bit_quantization_of_convolutional_networks_for_rapid-deployment/#content","title":"Content","text":"<p>Convolutional neural networks can be computationally expensive, requiring significant memory and storage resources. Neural network quantization can reduce the amount of intermediate results, but the process often requires the full dataset and time-consuming fine-tuning to recover accuracy lost after quantization. This paper presents a practical 4-bit post-training quantization approach that does not involve training the quantized model or the availability of the full dataset. The paper proposes three complementary methods for minimizing quantization errors at the tensor level, two of which obtain a closed-form analytical solution. The approach achieves accuracy that is just a few percent less than the state-of-the-art baseline across a range of convolutional models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pqk%3A_Model_compression_via_pruning%2C_quantization%2C_and_knowledge_distillation/","title":"Pqk: Model compression via pruning, quantization, and knowledge distillation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pqk%3A_Model_compression_via_pruning%2C_quantization%2C_and_knowledge_distillation/#summary","title":"Summary","text":"<p> The paper proposes a model compression technique called PQK that uses pruning, quantization, and knowledge distillation to create power-efficient and lightweight deep neural networks for edge devices. PQK has two phases, in phase one iterative pruning and quantization-aware training are used to create a lightweight model. In phase two, a teacher network is created by adding unimportant weights to a pruned network, which is used to train the pruned network as a student network. The effectiveness of PQK is verified on keyword spotting and image recognition."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pqk%3A_Model_compression_via_pruning%2C_quantization%2C_and_knowledge_distillation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pqk%3A_Model_compression_via_pruning%2C_quantization%2C_and_knowledge_distillation/#content","title":"Content","text":"<p>  We propose a model compression method called PQK, which stands for pruning, quantization, and knowledge distillation, to make Deep Neural Networks (DNNs) power efficient and lightweight for edge devices with limited computational resources. Unlike traditional pruning and knowledge distillation, PQK makes use of unimportant weights pruned in the pruning process to make a teacher network for training a better student network without pre-training the teacher model. PQK has two phases. Phase 1 exploits iterative pruning and quantization-aware training to make a lightweight and power-efficient model. In phase 2, we make a teacher network by adding unimportant weights unused in phase 1 to a pruned network. By using this teacher network, we train the pruned network as a student network. We apply our method to the recognition model and verify the effectiveness of PQK on keyword spotting (KWS) and image recognition."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Precision_gating%3A_Improving_neural_network_efficiency_with_dynamic_dual-precision_activations/","title":"Precision gating: Improving neural network efficiency with dynamic dual-precision activations","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Precision_gating%3A_Improving_neural_network_efficiency_with_dynamic_dual-precision_activations/#summary","title":"Summary","text":"<p>Summary: The paper proposes a trainable dynamic dual-precision quantization technique called Precision Gating (PG) for reducing the computational cost of deep neural networks. PG computes most features in low precision and only important features in higher precision, resulting in almost no accuracy loss. The proposed approach works well with various DNNs, including CNNs and RNNs, and achieves excellent results on ImageNet, mobile-friendly networks and LSTM on the Penn Tree Bank dataset. Compared to 8-bit uniform quantization, PG reduces the computational cost by 2.7 times with a 1.2% improvement in perplexity per word.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Precision_gating%3A_Improving_neural_network_efficiency_with_dynamic_dual-precision_activations/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Precision_gating%3A_Improving_neural_network_efficiency_with_dynamic_dual-precision_activations/#content","title":"Content","text":"<p> We propose precision gating (PG), an end-to-end trainable dynamic dual-precision quantization technique for deep neural networks. PG computes most features in a low precision and only a small proportion of important features in a higher precision to preserve accuracy. The proposed approach is applicable to a variety of DNN architectures and significantly reduces the computational cost of DNN execution with almost no accuracy loss. Our experiments indicate that PG achieves excellent results on CNNs, including statically compressed mobile-friendly networks such as ShuffleNet. Compared to the state-of-the-art prediction-based quantization schemes, PG achieves the same or higher accuracy with 2.4 times less compute on ImageNet. PG furthermore applies to RNNs. Compared to 8-bit uniform quantization, PG obtains a 1.2% improvement in perplexity per word with 2.7 times computational cost reduction on LSTM on the Penn Tree Bank dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Problems_in_large-scale_image_classification/","title":"Problems in large-scale image classification","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Problems_in_large-scale_image_classification/#summary","title":"Summary","text":"<p> The paper discusses issues in large-scale image classification and proposes solutions using hashing based large-scale similarity search, cross-class transfer active learning, and zero-shot learning. The proposed Collective Matrix Factorization Hashing framework combines multi-modality features for better retrieval performance. The results show promising performance on benchmarks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Problems_in_large-scale_image_classification/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Problems_in_large-scale_image_classification/#content","title":"Content","text":"<p>The research paper discusses problems in large-scale image classification due to the ever-growing number of images and classes. The paper focuses on two problems namely, large-scale similarity search and zero-shot/few-shots learning. It discusses the challenges in measuring similarity between large-scale images and training supervised classification models for newly emerging classes with limited labeled samples. To address these issues, the paper proposes hashing based large-scale similarity search, cross-class transfer active learning and zero-shot learning. The hashing approach uses binary codes to preserve the similarity structure, which increases efficiency. The paper also proposes a novel Collective Matrix Factorization Hashing framework to combine multi-modality features for better retrieval performance. The paper concludes with promising results on benchmarks to validate the proposed approaches."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_for_nearest_neighbor_search/","title":"Product quantization for nearest neighbor search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_for_nearest_neighbor_search/#summary","title":"Summary","text":"<p>The paper presents a product quantization based method for approximate nearest neighbor search that partitions the space into low dimensional subspaces and quantizes them independently, resulting in efficient nearest neighbor search. The approach shows superior accuracy compared to state-of-the-art methods for SIFT and GIST image descriptors and is validated on a large dataset of two billion vectors."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_for_nearest_neighbor_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_for_nearest_neighbor_search/#content","title":"Content","text":"<p>This paper introduces a product quantization based approach for approximate nearest neighbor search. The idea is to decomposes the space into a Cartesian product of low dimensional subspaces and to quantize each subspace separately. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy outperforming three state-of-the-art approaches. The scalability of our approach is validated on a dataset of two billion vectors."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_network_for_fast_image_retrieval/","title":"Product quantization network for fast image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_network_for_fast_image_retrieval/#summary","title":"Summary","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_network_for_fast_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_network_for_fast_image_retrieval/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_network_for_fast_visual_search/","title":"Product quantization network for fast visual search","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_network_for_fast_visual_search/#summary","title":"Summary","text":"<p>Summary: The paper introduces product quantization network (PQN), a neural network layer that enables the construction of a discriminative and compact image representation for fast and accurate image retrieval. The network also extends to residual product quantization network (RPQN) and temporal product quantization network (TPQN), which achieve higher accuracy and speed up video retrieval by exploiting temporal consistency respectively. The proposed method is experimented on multiple public benchmark datasets and shows state-of-the-art performance in fast image and video retrieval.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_network_for_fast_visual_search/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Product_quantization_network_for_fast_visual_search/#content","title":"Content","text":"<p> Product quantization has been widely used in fast image retrieval due to its effectiveness of coding high-dimensional visual features. By constructing the approximation function, we extend the hard-assignment quantization to soft-assignment quantization. Thanks to the differentiable property of the soft-assignment quantization, the product quantization operation can be integrated as a layer in a convolutional neural network, constructing the proposed product quantization network (PQN). Meanwhile, by extending the triplet loss to the asymmetric triplet loss, we directly optimize the retrieval accuracy of the learned representation based on asymmetric similarity measurement. Utilizing PQN, we can learn a discriminative and compact image representation in an end-to-end manner, which further enables a fast and accurate image retrieval. By revisiting residual quantization, we further extend the proposed PQN to residual product quantization network (RPQN). Benefited from the residual learning triggered by residual quantization, RPQN achieves a higher accuracy than PQN using the same computation cost. Moreover, we extend PQN to temporal product quantization network (TPQN) by exploiting temporal consistency in videos to speed up the video retrieval. It integrates frame-wise feature learning, frame-wise features aggregation, and video-level feature quantization in a single neural network. Comprehensive experiments conducted on multiple public benchmark datasets demonstrate the state-of-the-art performance of the proposed PQN, RPQN, and TPQN in fast image and video retrieval."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Profit%3A_A_novel_training_method_for_sub-4-bit_mobilenet_models/","title":"Profit: A novel training method for sub-4-bit mobilenet models","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Profit%3A_A_novel_training_method_for_sub-4-bit_mobilenet_models/#summary","title":"Summary","text":"<p>Summary: The paper discusses the challenge of activation instability induced by weight quantization (AIWQ) in sub-4-bit quantization of mobile networks. The authors propose a training method called PROgressive-Freezing Iterative Training (PROFIT) to alleviate the AIWQ problem. Additionally, they propose a differentiable and unified quantization method (DuQ) and a negative padding idea for asymmetric activation functions such as h-swish. The proposed methods are evaluated on MobileNet-v1, v2, and v3 on ImageNet and show comparable accuracy to full precision baseline. The proposed method outperforms the state-of-the-art method by a large margin on the 3-bit quantization of MobileNet-v3.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Profit%3A_A_novel_training_method_for_sub-4-bit_mobilenet_models/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Profit%3A_A_novel_training_method_for_sub-4-bit_mobilenet_models/#content","title":"Content","text":"<p>4-bit and lower precision mobile models are required due to the ever-increasing demand for better energy efficiency\u200c in mobile devices. In this work, we report that the activation instability induced by weight quantization (AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To alleviate the AIWQ problem, we propose a novel training method called PROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze layers whose weights are affected by the instability problem stronger than the other layers. We also propose a differentiable and unified quantization method (DuQ) and a negative padding idea to support asymmetric activation functions such as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1, v2, and v3 on ImageNet and report that 4-bit quantization offers comparable (within 1.48 % top-1 accuracy) accuracy to full precision baseline. In the ablation study of the 3-bit quantization of MobileNet-v3, our proposed method outperforms the state-of-the-art method by a large margin, 12.86 % of top-1 accuracy. The quantized model and source code is available at https://github.com/EunhyeokPark/PROFIT. Keywords: Mobile network, quantization, activation distribution, h-swish activation"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Progressive_cross-modal_semantic_network_for_zero-shot_sketch-based_image_retrieval/","title":"Progressive cross-modal semantic network for zero-shot sketch-based image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Progressive_cross-modal_semantic_network_for_zero-shot_sketch-based_image_retrieval/#summary","title":"Summary","text":"<p>This paper proposes a novel progressive cross-modal semantic network for better zero-shot sketch-based image retrieval (ZS-SBIR) performance. The proposed method first explicitly aligns the sketch and image features to semantic features, then projects them to a common space for subsequent retrieval. The paper also employs cross-reconstruction loss to encourage aligned features to capture complete knowledge about the two modalities and multi-modal Euclidean loss that guarantees similarity between the retrieval features from a sketch-image pair. The proposed approach outperforms state-of-the-art competitors by more than 3% on the Sketchy dataset and about 6% on the TU-Berlin dataset in terms of retrieval accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Progressive_cross-modal_semantic_network_for_zero-shot_sketch-based_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Progressive_cross-modal_semantic_network_for_zero-shot_sketch-based_image_retrieval/#content","title":"Content","text":"<p>Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal retrieval task that involves searching natural images through the use of free-hand sketches under the zero-shot scenario. Most previous methods project the sketch and image features into a low-dimensional common space for efficient retrieval and alignment to their semantic features (e.g., category-level word vectors) in order to transfer knowledge from seen to unseen classes. However, the lack of explicit alignment leads to unsatisfactory zero-shot retrieval performance. To address this issue, we propose a novel progressive cross-modal semantic network that first explicitly aligns the sketch and image features to semantic features, then projects the aligned features to a common space for subsequent retrieval. We further employ cross-reconstruction loss to encourage the aligned features to capture complete knowledge about the two modalities, along with multi-modal Euclidean loss that guarantees similarity between the retrieval features from a sketch-image pair. Our proposed approach outperforms state-of-the-art competitors to a remarkable extent by more than 3% on the Sketchy dataset and about 6% on the TU-Berlin dataset in terms of retrieval accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Projection_convolutional_neural_networks_for_1-bit_cnns_via_discrete_back_propagation/","title":"Projection convolutional neural networks for 1-bit cnns via discrete back propagation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Projection_convolutional_neural_networks_for_1-bit_cnns_via_discrete_back_propagation/#summary","title":"Summary","text":"<p> The paper introduced Projection Convolutional Neural Networks (PCNNs) with a discrete backpropagation via projection (DBPP) method to optimize the performance of binarized neural networks (BNNs). The authors achieved highly compressed CNNs using the projection function to efficiently solve the discrete backpropagation problem. The authors also developed a set of diverse quantized kernels that improved compression of full-precision kernels. The PCNNs showed better classification performance compared to other state-of-the-art BNNs on the ImageNet and CIFAR datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Projection_convolutional_neural_networks_for_1-bit_cnns_via_discrete_back_propagation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Projection_convolutional_neural_networks_for_1-bit_cnns_via_discrete_back_propagation/#content","title":"Content","text":"<p> In this paper, the Projection Convolutional Neural Networks (PCNNs) with a discrete back propagation via projection (DBPP) method is introduced to improve the performance of binarized neural networks (BNNs). The contributions of the paper include exploiting the projection function to efficiently solve the discrete back propagation problem leading to a new highly compressed CNNs (termed PCNNs) for the first time; learning a set of diverse quantized kernels that compress the full-precision kernels in a more efficient way than those proposed previously by exploiting multiple projections; and achieving the best classification performance compared to other state-of-the-art BNNs on the ImageNet and CIFAR datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Propagating_asymptotic-estimated_gradients_for_low_bitwidth_quantized_neural_networks/","title":"Propagating asymptotic-estimated gradients for low bitwidth quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Propagating_asymptotic-estimated_gradients_for_low_bitwidth_quantized_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new method called Asymptotic-Quantized Estimator (AQE) to train non-differentiable quantized neural networks (QNNs) smoothly while maintaining the smoothness condition of the graph. The output function gradually approaches the quantized value during the training process, and at the end of training, the weights and activations are quantized to low-precision. The proposed MINW-Net achieves comparable results to other state-of-the-art QNNs on ImageNet dataset. AQE outperforms Straight-Through Estimator (STE) and is well-defined.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Propagating_asymptotic-estimated_gradients_for_low_bitwidth_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Propagating_asymptotic-estimated_gradients_for_low_bitwidth_quantized_neural_networks/#content","title":"Content","text":"<p> The paper proposes a novel Asymptotic-Quantized Estimator (AQE) to estimate the gradient during the back-propagation process of the non-differentiable quantized neural networks (QNNs) training. The smoothness condition of the graph is maintained during the training process, and the output function gradually approaches the quantized value. At the end of training, the weights and activations have been quantized to low-precision, making the graph flow \"non-smooth\" non-linearities, which can be used to introduce the M-bit Inputs and N-bit Weights Network (MINW-Net) with low-precision weights and activations. AQE is well-defined and performs better than Straight-Through Estimator (STE). The proposed MINW-Net achieves comparable results with other state-of-the-art QNNs on ImageNet dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Proxquant%3A_Quantized_neural_networks_via_proximal_operators/","title":"Proxquant: Quantized neural networks via proximal operators","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Proxquant%3A_Quantized_neural_networks_via_proximal_operators/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new approach for quantizing neural networks called ProxQuant. ProxQuant uses a principled, regularized approach to optimize low-precision weights using the prox-gradient method. Back-propagation is performed on the full-precision vector and a prox-operator is applied to encourage quantizedness. Theoretical analyses show that ProxQuant converges to stationary points under mild smoothness assumptions, outperforming state-of-the-art results on binary quantization for ResNets and LSTMs, and equivalent to state-of-the-art for multi-bit quantization.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Proxquant%3A_Quantized_neural_networks_via_proximal_operators/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Proxquant%3A_Quantized_neural_networks_via_proximal_operators/#content","title":"Content","text":"<p>To make deep neural networks feasible in resource-constrained environments (such as mobile devices), it is beneficial to quantize models by using low-precision weights. One common technique for quantizing neural networks is the straight-through gradient method, which enables back-propagation through the quantization mapping. Despite its empirical success, little is understood about why the straight-through gradient method works. Building upon a novel observation that the straight-through gradient method is in fact identical to Nesterov's dual-averaging algorithm on a quantization constrained optimization problem, we propose a more principled alternative approach, called ProxQuant, that formulates quantized network training as a regularized learning problem instead and optimizes it via the prox-gradient method. ProxQuant does back-propagation on the underlying full-precision vector and applies an efficient prox-operator in between stochastic gradient steps to encourage quantizedness. For quantizing ResNets and LSTMs, ProxQuant outperforms state-of-the-art results on binary quantization and is on par with state-of-the-art on multi-bit quantization. We further perform theoretical analyses showing that ProxQuant converges to stationary points under mild smoothness assumptions, whereas variants such as lazy prox-gradient method can fail to converge in the same setting."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ptq4vit%3A_Post-training_quantization_for_vision_transformers_with_twin_uniform_quantization/","title":"Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ptq4vit%3A_Post-training_quantization_for_vision_transformers_with_twin_uniform_quantization/#summary","title":"Summary","text":"<p>Summary: This paper proposes a new method for quantizing neural networks called twin uniform quantization method and a Hessian-guided scaling factor evaluation. This method addresses the significant accuracy drops found in previous post-training quantization methods for vision transformers, leading to near-lossless prediction accuracy at 8-bit quantization. The authors develop a fast quantization framework called PTQ4ViT and present experimental results on the ImageNet classification task.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ptq4vit%3A_Post-training_quantization_for_vision_transformers_with_twin_uniform_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ptq4vit%3A_Post-training_quantization_for_vision_transformers_with_twin_uniform_quantization/#content","title":"Content","text":"<p>Quantization is a powerful method to compress neural networks, but previous post-training quantization methods have resulted in significant accuracy drops for vision transformers even at 8-bit quantization. In this paper, the authors propose the twin uniform quantization method and a Hessian-guided scaling factor evaluation to reduce quantization error on activation values. They also develop a fast quantization framework, PTQ4ViT. Experimental results show near-lossless prediction accuracy (less than 0.5% drop at 8-bit quantization) for quantized vision transformers on the ImageNet classification task."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Push_for_quantization%3A_Deep_fisher_hashing/","title":"Push for quantization: Deep fisher hashing","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Push_for_quantization%3A_Deep_fisher_hashing/#summary","title":"Summary","text":"<p>The paper explores the use of discrete binary codes for efficient storage and processing of high-dimensional data, and how deep learning methods can be optimized for image hashing using gradient-based methods. The authors introduce a margin on distances between dissimilar image pairs and maximize binary distances between classes while minimizing binary distance within the same class (inspired by Fisher LDA) to optimize maximum class separability in binary space. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 show that the proposed method yields compact codes that compare favorably to the current state of the art."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Push_for_quantization%3A_Deep_fisher_hashing/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Push_for_quantization%3A_Deep_fisher_hashing/#content","title":"Content","text":"<p>Current massive datasets demand light-weight access for analysis. Discrete hashing methods are thus bene\ufb01cial because they map high-dimensional data to compact binary codes that are ef\ufb01cient to store and process, while preserving semantic similarity. To optimize powerful deep learning methods for image hashing, gradient-based methods are required. Binary codes, however, are discrete and thus have no continuous derivatives. Relaxing the problem by solving it in a continuous space and then quantizing the solution is not guaranteed to yield separable binary codes. The quantization needs to be included in the optimization. In this paper we push for quantization: We optimize maximum class separability in the binary space. We introduce a margin on distances between dissimilar image pairs as measured in the binary space. In addition to pair-wise distances, we draw inspiration from Fisher\u2019s Linear Discriminant Analysis (Fisher LDA) to maximize the binary distances between classes and at the same time minimize the binary distance of images within the same class. Experiments on CIFAR-10, NUS-WIDE and ImageNet100 demonstrate compact codes comparing favorably to the current state of the art."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pushing_the_limits_of_narrow_precision_inferencing_at_cloud_scale_with_microsoft_floating_point/","title":"Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pushing_the_limits_of_narrow_precision_inferencing_at_cloud_scale_with_microsoft_floating_point/#summary","title":"Summary","text":"<p>Summary: The paper explores the effectiveness of Microsoft Floating Point (MSFP) as a datatype for cloud-scale inferencing on custom hardware. MSFP16 incurs 3% lower cost compared to B\ufb02oat16 and MSFP12 has 4% lower cost compared to INT8 while delivering similar or better accuracy. It supports various deep learning models without modification, incurs negligible impact on accuracy, and can be integrated with a mature cloud production pipeline. The study demonstrates MSFP's effectiveness on production scenarios, including web search, question-answering, and image classification.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pushing_the_limits_of_narrow_precision_inferencing_at_cloud_scale_with_microsoft_floating_point/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Pushing_the_limits_of_narrow_precision_inferencing_at_cloud_scale_with_microsoft_floating_point/#content","title":"Content","text":"<p>&lt;Abstract:  In this paper, we explore the limits of Microsoft Floating Point (MSFP), a new class of datatypes developed for production cloud-scale inferencing on custom hardware. Through the co-evolution of hardware design and algorithms, MSFP16 incurs 3  lower cost compared to B\ufb02oat16 and MSFP12 has 4 lower cost compared to INT8 while delivering a comparable or better accuracy. MSFP incurs negligible impact to accuracy (&lt;1%), requires no changes to the model topology, and is integrated with a mature cloud production pipeline. MSFP supports various classes of deep learning models including CNNs, RNNs, and Transformers without modi\ufb01cation. Finally, we characterize the accuracy and implementation of MSFP and demonstrate its ef\ufb01cacy on a number of production scenarios, including models that power major online scenarios such as web search, question-answering, and image classi\ufb01cation.&gt;</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-ViT%3A_Accurate_and_Fully_Quantized_Low-bit_Vision_Transformer/","title":"Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-ViT%3A_Accurate_and_Fully_Quantized_Low-bit_Vision_Transformer/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method to address the performance drop in low-bit vision transformers caused by information distortion during quantization. The authors developed an information rectification module and a distribution guided distillation scheme, resulting in a fully quantized ViT achieving better performance than prior arts. The Q-ViT can theoretically accelerate the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterparts.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-ViT%3A_Accurate_and_Fully_Quantized_Low-bit_Vision_Transformer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-ViT%3A_Accurate_and_Fully_Quantized_Low-bit_Vision_Transformer/#content","title":"Content","text":"<p> The large pre-trained vision transformers have proven effective in various computer vision tasks, but are limited in their deployment on resource-constrained devices due to expensive computational and memory costs. Quantization is a method that reduces computation and memory consumption by using low-bit parameters and bit-wise operations. However, low-bit ViTs usually suffer from a significant performance drop compared to their real-valued counterparts. In this work, the authors identify the bottleneck for severe performance drop comes from the information distortion of the low-bit quantized self-attention map. They then develop an information rectification module and a distribution guided distillation scheme for fully quantized vision transformers to effectively eliminate such distortion, leading to a fully quantized ViTs. Experimental results show that their method achieves better performance than prior arts, where the Q-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterparts."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-ViT%3A_Fully_Differentiable_Quantization_for_Vision_Transformer/","title":"Q-ViT: Fully Differentiable Quantization for Vision Transformer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-ViT%3A_Fully_Differentiable_Quantization_for_Vision_Transformer/#summary","title":"Summary","text":"<p> The proposed Q-ViT is a fully differentiable quantization technique for vision transformers that utilizes learnable parameters for quantization scales and bit-widths, with head-wise bit-width to decrease the size of Q-ViT while maintaining performance. The study also introduces a novel technique called switchable scale to address the convergence problem in the joint training of bit-widths and quantization scales. MSA and GELU are identified as important elements for ViT quantization, and experiments on different ViT models, such as DeiT and Swin Transformer, demonstrate the effectiveness of Q-ViT, which can achieve 3-bit quantization without significant loss in performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-ViT%3A_Fully_Differentiable_Quantization_for_Vision_Transformer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-ViT%3A_Fully_Differentiable_Quantization_for_Vision_Transformer/#content","title":"Content","text":"<p>In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT. Our method uses learnable parameters for both quantization scales and bit-widths. We leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. We also propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. Our study shows that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. We perform extensive experiments on different ViT models, such as DeiT and Swin Transformer, and demonstrate the effectiveness of our quantization method. Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-rater%3A_Non-convex_optimization_for_post-training_uniform_quantization/","title":"Q-rater: Non-convex optimization for post-training uniform quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-rater%3A_Non-convex_optimization_for_post-training_uniform_quantization/#summary","title":"Summary","text":"<p>The paper proposes a new post-training uniform quantization technique, which considers non-convexity and optimizes hyper-parameters using the task loss. This approach shows higher model accuracy, especially for low-bit quantization, in extensive experiments using different models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-rater%3A_Non-convex_optimization_for_post-training_uniform_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-rater%3A_Non-convex_optimization_for_post-training_uniform_quantization/#content","title":"Content","text":"<p>Various post-training uniform quantization methods have usually been studied based on convex optimization. In this paper, we propose a new post-training uniform quantization technique considering non-convexity. We empirically show that hyper-parameters for clipping and rounding of weights and activations can be explored by monitoring task loss. Then, an optimally searched set of hyper-parameters is frozen to proceed to the next layer such that an incremental non-convex optimization is enabled for post-training quantization. Throughout extensive experimental results using various models, our proposed technique presents higher model accuracy, especially for a low-bit quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-spinn%3A_A_framework_for_quantizing_spiking_neural_networks/","title":"Q-spinn: A framework for quantizing spiking neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-spinn%3A_A_framework_for_quantizing_spiking_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper proposes Q-SpiNN, a new quantization framework that quantizes different parameters of Spiking Neural Networks depending on their significance to the accuracy, explores different combinations of quantization schemes, and selects the Pareto-optimal model for a good memory-accuracy trade-off. Q-SpiNN reduces memory footprint while maintaining accuracy across different datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-spinn%3A_A_framework_for_quantizing_spiking_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Q-spinn%3A_A_framework_for_quantizing_spiking_neural_networks/#content","title":"Content","text":"<p>A prominent technique for reducing the memory footprint of Spiking Neural Networks (SNNs) without decreasing the accuracy significantly is quantization. However, the state-of-the-art only focus on employing the weight quantization directly from a specific quantization scheme, i.e., either the post-training quantization (PTQ) or the in-training quantization (ITQ), and do not consider (1) quantizing other SNN parameters (e.g., neurons\u2019 membrane potential), (2) exploring different combinations of quantization approaches (i.e., quantization schemes, precision levels, and rounding schemes), and (3) selecting the SNN model with a good memory-accuracy trade-off at the end. Towards this, we propose Q-SpiNN, a novel quantization framework for memory-ef\ufb01cient SNNs. The key mechanisms of the Q-SpiNN are: (1) employing quantization for different SNN parameters based on their significance to the accuracy, (2) exploring different combinations of quantization schemes, precision levels, and rounding schemes to \ufb01nd efficient SNN model candidates, and (3) developing an algorithm that quantifies the benefit of the memory-accuracy trade-off obtained by the candidates and selects the Pareto-optimal one. The experimental results show that, for the unsupervised network, the Q-SpiNN reduces the memory footprint by ca. 4x, while maintaining the accuracy within 1% from the baseline on the MNIST dataset. For the supervised network, the Q-SpiNN reduces the memory by ca. 2x, while keeping the accuracy within 2% from the baseline on the DVS-Gesture dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QDrop%3A_randomly_dropping_quantization_for_extremely_low-bit_post-training_quantization/","title":"QDrop: randomly dropping quantization for extremely low-bit post-training quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QDrop%3A_randomly_dropping_quantization_for_extremely_low-bit_post-training_quantization/#summary","title":"Summary","text":"<p>Summary: This paper proposes a new approach to improve the accuracy of post-training quantization (PTQ) in extremely low-bit settings by incorporating activation quantization into PTQ reconstruction. The paper establishes a theoretical framework and introduces a simple yet effective approach called QD ROP. This approach randomly drops the quantization of activations during PTQ and produces superior results in computer vision and natural language processing tasks. QD ROP achieves state-of-the-art results in PTQ and can push the limit of PTQ to 2-bit activation for the first time with an accuracy boost of up to 51.49%.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QDrop%3A_randomly_dropping_quantization_for_extremely_low-bit_post-training_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QDrop%3A_randomly_dropping_quantization_for_extremely_low-bit_post-training_quantization/#content","title":"Content","text":"<p>Recently, post-training quantization (PTQ) has driven much attention to produce efficient neural networks without long-time retraining. Despite its low cost, current PTQ works tend to fail under the extremely low-bit setting. In this study, we pioneeringly confirm that properly incorporating activation quantization into the PTQ reconstruction benefits the final accuracy. To deeply understand the inherent reason, a theoretical framework is established, indicating that the flatness of the optimized low-bit model on calibration and test data is crucial. Based on the conclusion, a simple yet effective approach dubbed as QD ROP is proposed, which randomly drops the quantization of activations during PTQ. Extensive experiments on various tasks including computer vision (image classification, object detection) and natural language processing (text classification and question answering) prove its superiority. With QD ROP, the limit of PTQ is pushed to the 2-bit activation for the first time and the accuracy boost can be up to 51.49%. Without bells and whistles, QD ROP establishes a new state of the art for PTQ."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGAN%3A_Quantized_generative_adversarial_networks/","title":"QGAN: Quantized generative adversarial networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGAN%3A_Quantized_generative_adversarial_networks/#summary","title":"Summary","text":"<p>The paper introduces QGAN, a novel quantization method with EM algorithms, for GANs which can quantize GAN models to 1-bit or 2-bit representations with comparable quality to original models. This technique enables real-world deployment of GANs on smartphones, which was difficult previously due to the intensive memory requirements and computations of GANs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGAN%3A_Quantized_generative_adversarial_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGAN%3A_Quantized_generative_adversarial_networks/#content","title":"Content","text":"<p> The intensive computations and memory requirements of GANs make their real-world deployment on smartphones difficult. While CNNs and RNNs have been reduced in size through quantization, GANs have not been studied using this technique due to issues with the effectiveness of quantization algorithms and the instability of training GAN models. This paper introduces QGAN, a new quantization method based on EM algorithms, for GANs. Using CIFAR-10 and CelebA datasets, QGAN is able to quantize GAN models to 1-bit or 2-bit representations with results of quality comparable to original models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGTC%3A_Accelerating_Quantized_Graph_Neural_Networks_via_GPU_Tensor_Core/","title":"QGTC: Accelerating Quantized Graph Neural Networks via GPU Tensor Core","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGTC%3A_Accelerating_Quantized_Graph_Neural_Networks_via_GPU_Tensor_Core/#summary","title":"Summary","text":"<p>The paper proposes QGTC, a Tensor Core-based computing framework that supports any-bitwidth computation for quantized graph neural networks (QGNNs) on GPUs. The authors introduced a novel quantized low-bit arithmetic design, TC-tailored CUDA kernel design, and an effective bandwidth-optimized subgraph packing strategy to maximize transferring efficiency. QGTC integrated with Pytorch, achieving evident inference speedup compared to the state-of-the-art DGL framework across diverse settings."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGTC%3A_Accelerating_Quantized_Graph_Neural_Networks_via_GPU_Tensor_Core/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGTC%3A_Accelerating_Quantized_Graph_Neural_Networks_via_GPU_Tensor_Core/#content","title":"Content","text":"<p>Over the most recent years, quantized graph neural network(QGNN) attracts lots of research and industry attention due to its high robustness and low computation and memory overhead. Unfortunately, the performance gains of QGNN have never been realized on modern GPU platforms. To this end, we propose the first Tensor Core (TC) based computing framework, QGTC, to support any-bitwidth computation for QGNNs on GPUs. We introduce a novel quantized low-bit arithmetic design based on the low-bit data representation and bit-decomposed computation. We craft a novel TC-tailored CUDA kernel design by incorporating 3D-stacked bit compression, zero-tile jumping, and non-zero tile reuse technique to improve the performance systematically. We incorporate an effective bandwidth-optimized subgraph packing strategy to maximize the transferring efficiency between CPU host and GPU device. We integrate QGTC with Pytorch for better programmability and extensibility. Extensive experiments demonstrate that QGTC can achieve evident inference speedup (on average 2.7\u00d7) compared with the state-of-the-art DGL framework across diverse settings."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGTC%3A_accelerating_quantized_graph_neural_networks_via_GPU_tensor_core/","title":"QGTC: accelerating quantized graph neural networks via GPU tensor core","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGTC%3A_accelerating_quantized_graph_neural_networks_via_GPU_tensor_core/#summary","title":"Summary","text":"<p>Summary: The paper introduces a Tensor Core framework, QGTC, that supports any-bitwidth computation for quantized graph neural networks (QGNNs) on GPUs. The framework utilizes low-bit arithmetic design based on low-bit data representation and bit-decomposed computation, along with other optimizations to improve performance. It is integrated with Pytorch for better programmability and extensibility. The experiments show that QGTC achieves better inference speed than the state-of-the-art DGL framework.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGTC%3A_accelerating_quantized_graph_neural_networks_via_GPU_tensor_core/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QGTC%3A_accelerating_quantized_graph_neural_networks_via_GPU_tensor_core/#content","title":"Content","text":"<p>  This paper proposes QGTC, a Tensor Core based computing framework that supports any-bitwidth computation for quantized graph neural networks (QGNNs) on GPUs. The framework includes a novel quantized low-bit arithmetic design based on low-bit data representation and bit-decomposed computation, along with several other system-level optimizations to improve performance. QGTC is integrated with Pytorch for better programmability and extensibility. Experiments demonstrate that QGTC achieves inference speedup compared to the state-of-the-art DGL framework."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QONNX%3A_Representing_Arbitrary-Precision_Quantized_Neural_Networks/","title":"QONNX: Representing Arbitrary-Precision Quantized Neural Networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QONNX%3A_Representing_Arbitrary-Precision_Quantized_Neural_Networks/#summary","title":"Summary","text":"<p>The paper presents extensions to the Open Neural Network Exchange (ONNX) format to represent low precision quantization in existing ONNX-based quantization formats using integer clipping, resulting in two new backward-compatible variants. It also introduces a novel higher-level ONNX format called Quantized ONNX (QONNX) that introduces three new operators to represent uniform quantization, enabling targeting a wider variety of platforms. Utilities for working with QONNX and examples of its usage in the FINN and hls4ml toolchains are presented. Finally, the paper introduces the QONNX model zoo to share low-precision quantized neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QONNX%3A_Representing_Arbitrary-Precision_Quantized_Neural_Networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QONNX%3A_Representing_Arbitrary-Precision_Quantized_Neural_Networks/#content","title":"Content","text":"<p>We present extensions to the Open Neural Network Exchange (ONNX) intermediate representation format to represent arbitrary-precision quantized neural networks. We first introduce support for low precision quantization in existing ONNX-based quantization formats by leveraging integer clipping, resulting in two new backward-compatible variants: the quantized operator format with clipping and quantize-clip-dequantize (QCDQ) format. We then introduce a novel higher-level ONNX format called quantized ONNX (QONNX) that introduces three new operators\u2014 Quant, BipolarQuant, and Trunc\u2014in order to represent uniform quantization. By keeping the QONNX IR high-level and flexible, we enable targeting a wider variety of platforms. We also present utilities for working with QONNX, as well as examples of its usage in the FINN and hls4ml toolchains. Finally, we introduce the QONNX model zoo to share low-precision quantized neural networks. Index Terms \u2014quantized neural network, intermediate representation, FPGA."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QReg%3A_On_Regularization_Effects_of_Quantization/","title":"QReg: On Regularization Effects of Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QReg%3A_On_Regularization_Effects_of_Quantization/#summary","title":"Summary","text":"<p> This paper explores the effects of weight quantization as a form of regularization in deep neural network training. The authors provide an analytical study and empirical results to prove their hypothesis. They conclude that weight quantization is correlated with regularization and propose that 8-bit quantization is a reliable form of regularization in various vision tasks and models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QReg%3A_On_Regularization_Effects_of_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QReg%3A_On_Regularization_Effects_of_Quantization/#content","title":"Content","text":"<p> In this paper, the effects of quantization in DNN training are studied. The authors hypothesize that weight quantization is a form of regularization, and the amount of regularization is correlated with the quantization level. They confirm their hypothesis by providing analytical study and empirical results. The magnitude of the noise caused by weight quantization is shown to be correlated with the level of quantization. An extensive list of experiments is performed, and it is demonstrated that the regularization effects of quantization can be seen in various vision tasks and models, over various datasets. The authors propose that 8-bit quantization provides a reliable form of regularization in different vision tasks and models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QUANOS%3A_adversarial_noise_sensitivity_driven_hybrid_quantization_of_neural_networks/","title":"QUANOS: adversarial noise sensitivity driven hybrid quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QUANOS%3A_adversarial_noise_sensitivity_driven_hybrid_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: The authors propose QUANOS, a framework that performs layer-specific hybrid quantization based on Adversarial Noise Sensitivity (ANS) to enhance the adversarial robustness of Deep Neural Networks (DNNs). They identify a novel noise stability metric for DNNs, ANS, which allows for a principled way of determining the optimal bit-width per layer that incurs adversarial robustness as well as energy-efficiency with minimal loss in accuracy. QUANOS outperforms homogeneously quantized 8-bit precision baseline in terms of adversarial robustness while yielding improved compression and energy savings at iso-accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QUANOS%3A_adversarial_noise_sensitivity_driven_hybrid_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QUANOS%3A_adversarial_noise_sensitivity_driven_hybrid_quantization_of_neural_networks/#content","title":"Content","text":"<p> Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial attacks, which limits their deployment in real-world applications. In this paper, the authors propose QUANOS, a framework that performs layer-specific hybrid quantization based on Adversarial Noise Sensitivity (ANS). They identify a novel noise stability metric for DNNs, ANS, which allows for a principled way of determining the optimal bit-width per layer that incurs adversarial robustness as well as energy-efficiency with minimal loss in accuracy. QUANOS outperforms homogeneously quantized 8-bit precision baseline in terms of adversarial robustness, while yielding improved compression and energy savings at iso-accuracy. At iso-compression rate, QUANOS yields significantly higher adversarial robustness than similar sized baseline against strong white-box attacks. Combining QUANOS with state-of-the-art defense methods outperforms the state-of-the-art in robustness against very strong attacks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QUENN%3A_QUantization_engine_for_low-power_neural_networks/","title":"QUENN: QUantization engine for low-power neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QUENN%3A_QUantization_engine_for_low-power_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper presents a framework and engine, LPDNN and QUENN respectively, for deploying deep neural networks on embedded devices using quantization techniques. It demonstrates the potential of k-means clustering with Gaussian quantization for achieving better performance than linear quantizers in Imagenet, with significant savings in weights' storage and run-time memory accesses and minimal drop in top5 accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QUENN%3A_QUantization_engine_for_low-power_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QUENN%3A_QUantization_engine_for_low-power_neural_networks/#content","title":"Content","text":"<p> Deep Learning is becoming increasingly popular on edge devices opening new frontiers for distributed Artificial Intelligence (AI). However, deep neural networks demand significant computational resources that can be reduced by methods such as reduced-precision arithmetic and coarsely quantized numerical representations. LPDNN is a framework that enables the optimized deployment of Deep Neural Networks on heterogeneous embedded devices, and QUENN is a quantization engine that is integrated in LPDNN. The quantization engine depends on a fine-grained workflow that enables a Neural Network Design Exploration and a sensitivity analysis of each layer for quantization. The paper demonstrates the engine with a case study on Alexnet and VGG16 for three different techniques for direct quantization: standard fixed-point, dynamic fixed-point and k-means clustering, and demonstrates the potential of the latter. Gaussian quantizer with k-means clustering can achieve better performance than linear quantizers. The potential of the technique is demonstrated with Imagenet, where over 55.64% saving for weights\u2019 storage and 69.17% for run-time memory accesses with less than 1% drop in top5 accuracy is achieved without retraining."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QVIP%3A_an_ILP-based_formal_verification_approach_for_quantized_neural_networks/","title":"QVIP: an ILP-based formal verification approach for quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QVIP%3A_an_ILP-based_formal_verification_approach_for_quantized_neural_networks/#summary","title":"Summary","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QVIP%3A_an_ILP-based_formal_verification_approach_for_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/QVIP%3A_an_ILP-based_formal_verification_approach_for_quantized_neural_networks/#content","title":"Content","text":"<p> In this research paper, a formal verification approach for quantized neural networks (QNNs) is proposed. The verification problem of QNNs is encoded into the solving of integer linear constraints, which can be solved using off-the-shelf solvers. Local robustness verification and maximum robustness radius computation are demonstrated as applications of this approach. The effectiveness and efficiency of this approach are confirmed through experimental results on QNNs with different quantization bits, where it was shown to be two orders of magnitude faster and able to solve more verification tasks in the same time limit than the state-of-the-art methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qimera%3A_Data-free_quantization_with_synthetic_boundary_supporting_samples/","title":"Qimera: Data-free quantization with synthetic boundary supporting samples","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qimera%3A_Data-free_quantization_with_synthetic_boundary_supporting_samples/#summary","title":"Summary","text":"<p> Qimera is a method proposed by the authors for data-free neural network quantization. It generates synthetic boundary supporting samples using superposed latent embeddings and disentanglement mapping layer. The method improves the accuracy of model quantization in situations where the original training data is not available due to privacy or security concerns. Experimental results show that Qimera is state-of-the-art for data-free quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qimera%3A_Data-free_quantization_with_synthetic_boundary_supporting_samples/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qimera%3A_Data-free_quantization_with_synthetic_boundary_supporting_samples/#content","title":"Content","text":"<p> Model quantization is a powerful compression method for deep neural networks that is particularly useful for edge and mobile applications. However, model quantization typically requires the original training data to maintain accuracy, which is often impossible due to privacy or security concerns. Generative methods that use synthetic samples have shown promise, but they typically rely on random noise input, which may be insufficient to capture the distribution of the original data, especially around the decision boundaries. In this paper, the authors propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples. They also propose using an additional disentanglement mapping layer and extracting information from the full-precision model to improve the performance of these samples. Experimental results show that Qimera is state-of-the-art for data-free quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qkd%3A_Quantization-aware_knowledge_distillation/","title":"Qkd: Quantization-aware knowledge distillation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qkd%3A_Quantization-aware_knowledge_distillation/#summary","title":"Summary","text":"<p> The paper proposes a new method called Quantization-aware Knowledge Distillation (QKD) that coordinates quantization and knowledge distillation in three phases to reduce memory and power consumption of deep neural networks on edge devices. The proposed method outperformed existing state-of-the-art methods and achieved full-precision accuracy at low quantization levels on ResNet and MobilenetV2."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qkd%3A_Quantization-aware_knowledge_distillation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qkd%3A_Quantization-aware_knowledge_distillation/#content","title":"Content","text":"<p> Quantization and Knowledge distillation (KD) methods are used to reduce memory and power consumption of deep neural networks (DNNs) for resource-constrained edge devices. We propose Quantization-aware Knowledge Distillation (QKD) wherein quantization and KD are carefully coordinated in three phases. QKD outperformed existing state-of-the-art methods and could recover the full-precision accuracy at as low as W3A3 quantization on ResNet and W6A6 quantization on MobilenetV2."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qnet%3A_an_adaptive_quantization_table_generator_based_on_convolutional_neural_network/","title":"Qnet: an adaptive quantization table generator based on convolutional neural network","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qnet%3A_an_adaptive_quantization_table_generator_based_on_convolutional_neural_network/#summary","title":"Summary","text":"<p>Summary: The paper proposes a CNN-based optimal quantization table generator called QNet, which directly generates an adaptive quantization table by fusing frequency and spatial domain information of more than 10,000 images. It also presents a classification network to train to indicate the optimal quantization table for each image, improving compression performance and computational efficiency. The proposed method outperforms state-of-the-art methods and is computationally efficient, taking only a few milliseconds to process an image on a single CPU core.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qnet%3A_an_adaptive_quantization_table_generator_based_on_convolutional_neural_network/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qnet%3A_an_adaptive_quantization_table_generator_based_on_convolutional_neural_network/#content","title":"Content","text":"<p> The paper proposes a Convolutional Neural Network (CNN) based optimal quantization table generator called QNet, to obtain an image-adaptive quantization table for a JPEG image in a standard-compliant way. The proposed method extracts and fuses the frequency and spatial domain information of more than 10,000 images to directly generate an adaptive quantization table. Additionally, the paper presents a method to train a classification network to indicate the optimal quantization table for each image, further improving the compression performance and computational efficiency. Experimental results show that the proposed method outperforms state-of-the-art methods in terms of Peak Signal-to-Noise Ratio (PSNR) gains of nearly 1.2 and 1.4 dB, and improvements of 0.4% and 0.54% in Structural Similarity Index Measurement (SSIM), respectively. Moreover, the proposed method is computationally efficient, taking only 15 and 6.25 milliseconds, respectively, to process a 768 \u00d7 512 image on a single CPU core at 3.20 GHz."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qpp%3A_Real-time_quantization_parameter_prediction_for_deep_neural_networks/","title":"Qpp: Real-time quantization parameter prediction for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qpp%3A_Real-time_quantization_parameter_prediction_for_deep_neural_networks/#summary","title":"Summary","text":"<p> This paper discusses quantization of weights and feature maps, a popular technique to reduce computational complexity, memory and power consumption in deep neural networks, for mobile and embedded devices. Training-aware quantization requires a full dataset and post-training methods have limitations. The paper proposes a new quantization approach called QPP that accurately estimates quantization parameters without complex calculations, improving the efficiency of two dynamic approaches. The method is tested on various tasks like classification, segmentation, super-resolution and facial landmark."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qpp%3A_Real-time_quantization_parameter_prediction_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qpp%3A_Real-time_quantization_parameter_prediction_for_deep_neural_networks/#content","title":"Content","text":"<p>Modern deep neural networks (DNNs) cannot be effectively used in mobile and embedded devices due to strict requirements for computational complexity, memory, and power consumption. The quantization of weights and feature maps (activations) is a popular approach to solve this problem. Training-aware quantization often shows excellent results but requires a full dataset, which is not always available. Post-training quantization methods, in turn, are applied without fine-tuning but still work well for many classes of tasks like classification, segmentation, and so on. However, they either imply a big overhead for quantization parameters (QPs) calculation at runtime (dynamic methods) or lead to an accuracy drop if pre-computed static QPs are used (static methods). Moreover, most inference frameworks don't support dynamic quantization. Thus we propose a novel quantization approach called QPP: quantization parameter prediction. With a small subset of a training dataset or unlabeled data from the same domain, we find the predictor that can accurately estimate QPs of activations given only the NN's input data. Such a predictor allows us to avoid complex calculation of precise values of QPs while maintaining the quality of the model. To illustrate our method's efficiency, we added QPP into two dynamic approaches: 1) Dense+Sparse quantization, where the predetermined percentage of activations are not quantized, 2) standard quantization with equal quantization steps. We provide experiments on a wide set of tasks including super-resolution, facial landmark, segmentation, and classification."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantface%3A_Towards_lightweight_face_recognition_by_synthetic_data_low-bit_quantization/","title":"Quantface: Towards lightweight face recognition by synthetic data low-bit quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantface%3A_Towards_lightweight_face_recognition_by_synthetic_data_low-bit_quantization/#summary","title":"Summary","text":"<p> The paper proposes a solution, QuantFace, based on low-bit precision format model quantization, to reduce computational costs of existing over-parameterized deep learning face recognition models. The solution reduces the model size up to 5x without designing a specific architecture or accessing real training datasets. Privacy-friendly synthetic face data is used to the quantization process to avoid privacy concerns and make the training codes publicly available. The evaluation experiment shows the reduction in computational costs while maintaining the verification performance of full-precision models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantface%3A_Towards_lightweight_face_recognition_by_synthetic_data_low-bit_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantface%3A_Towards_lightweight_face_recognition_by_synthetic_data_low-bit_quantization/#content","title":"Content","text":"<p>Deep learning-based face recognition models rely on over-parameterized DNN with high computational costs. However, deploying such large models on devices constrained by computational requirements is challenging. Previous solutions proposed to design special compact architectures and train them from scratch using real training data. In this paper, we present the QuantFace solution based on low-bit precision format model quantization. QuantFace reduces the required computational cost of existing face recognition models without the need for designing a particular architecture or accessing real training data. Through evaluation experiments on seven benchmarks, we demonstrate that QuantFace can successfully reduce the model size up to 5x while maintaining, to a large degree, the verification performance of the full-precision model without accessing real training datasets. The solution introduces privacy-friendly synthetic face data to the quantization process to mitigate potential privacy concerns and issues related to the accessibility to real training data. The training codes are publicly available."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization%2C_training%2C_parasitic_resistance_correction%2C_and_programming_techniques_of_memristor-crossbar_neural_networks_for_edge_intelligence/","title":"Quantization, training, parasitic resistance correction, and programming techniques of memristor-crossbar neural networks for edge intelligence","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization%2C_training%2C_parasitic_resistance_correction%2C_and_programming_techniques_of_memristor-crossbar_neural_networks_for_edge_intelligence/#summary","title":"Summary","text":"<p>Summary: This paper reviews the quantization, training, parasitic resistance correction, programming techniques, challenges, and solutions of memristor-crossbar neural networks for edge intelligence applications. It also compares and analyzes various programming approaches and summarizes recent progress in applying these networks to fields like image and speech recognition and natural language processing.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization%2C_training%2C_parasitic_resistance_correction%2C_and_programming_techniques_of_memristor-crossbar_neural_networks_for_edge_intelligence/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization%2C_training%2C_parasitic_resistance_correction%2C_and_programming_techniques_of_memristor-crossbar_neural_networks_for_edge_intelligence/#content","title":"Content","text":"<p>Quantization, training, parasitic resistance correction and programming techniques of memristor-crossbar neural networks for edge intelligence are reviewed in this paper. Memristor-crossbar neural networks have shown remarkable potential in achieving high energy efficiency, high accuracy and scalability for edge intelligence applications. Challenges and solutions in each stage of the crossbar-based neural network design flow, from device level to circuit/module level, are discussed in detail. Moreover, various programming approaches of crossbar-based neural networks are compared and analyzed. Finally, recent progresses in applying memristor-crossbar neural networks to various fields, such as image recognition, object detection, speech recognition and natural language processing, are summarized."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization-guided_training_for_compact_TinyML_models/","title":"Quantization-guided training for compact TinyML models","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization-guided_training_for_compact_TinyML_models/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method called Quantization Guided Training (QGT) to guide DNN training towards optimized low-bit-precision targets, and reach extreme compression levels below 8-bit precision. QGT uses customized regularization to encourage weight values towards a distribution that maximizes accuracy while reducing quantization errors. The effectiveness of QGT is demonstrated using state-of-the-art model architectures (MobileNet, ResNet) on vision datasets and an 81KB tiny model for person detection down to 2-bit precision (representing 17.7x size reduction) while maintaining an accuracy drop of only 3% compared to a floating-point baseline.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization-guided_training_for_compact_TinyML_models/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization-guided_training_for_compact_TinyML_models/#content","title":"Content","text":"<p> We address the methodology to train and quantize deep neural networks (DNNs) in order to produce compact models while maintaining algorithmic accuracy. In this paper, we propose a Quantization Guided Training (QGT) method to guide DNN training towards optimized low-bit-precision targets, and reach extreme compression levels below 8-bit precision. Unlike standard quantization-aware training (QAT) approaches, QGT uses customized regularization to encourage weight values towards a distribution that maximizes accuracy while reducing quantization errors. We validate QGT using state-of-the-art model architectures (MobileNet, ResNet) on vision datasets. We also demonstrate the effectiveness with an 81KB tiny model for person detection down to 2-bit precision (representing 17.7x size reduction), while maintaining an accuracy drop of only 3% compared to a floating-point baseline."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization-guided_training_for_compact_tinyML_models/","title":"Quantization-guided training for compact tinyML models","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization-guided_training_for_compact_tinyML_models/#summary","title":"Summary","text":"<p>Summary: The paper proposes a method called Quantization Guided Training (QGT) to optimize deep neural network (DNN) training for low-bit-precision targets. The method uses customized regularization to encourage weight values towards a distribution that maximizes accuracy while reducing quantization errors. The proposed method is demonstrated with an 81KB tiny model for person detection down to 2-bit precision, representing a 17.7x size reduction, while maintaining an accuracy drop of only 3% compared to a floating-point baseline.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization-guided_training_for_compact_tinyML_models/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization-guided_training_for_compact_tinyML_models/#content","title":"Content","text":"<p> We propose a Quantization Guided Training (QGT) method to optimize DNN training for low-bit-precision targets and achieve compression levels below 8-bit precision. This method uses customized regularization to encourage weight values towards a distribution that maximizes accuracy while reducing quantization errors. QGT is able to identify compression bottlenecks and can be used for state-of-the-art model architectures on vision datasets. The proposed method is demonstrated with an 81KB tiny model for person detection down to 2-bit precision, representing a 17.7x size reduction, while maintaining an accuracy drop of only 3% compared to a floating-point baseline."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_and_training_of_low_bit-width_convolutional_neural_networks_for_object_detection/","title":"Quantization and training of low bit-width convolutional neural networks for object detection","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_and_training_of_low_bit-width_convolutional_neural_networks_for_object_detection/#summary","title":"Summary","text":"<p>The paper presents LBW-Net, an optimization-based method for training low bit-width CNNs, that quantizes weights to zero or powers of two by minimizing the Euclidean distance between full-precision and quantized weights during backpropagation. Compared to full-precision CNNs, LBW-Net offers advantages such as memory savings, energy efficiency, and faster deployment, with nearly lossless performance in object detection tasks and better performance in some real-world visual scenarios. Results from experiments on the PASCAL VOC dataset show more than 4 times faster deployment with 6-bit LBW-Net compared to its 32-bit floating-point counterpart."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_and_training_of_low_bit-width_convolutional_neural_networks_for_object_detection/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_and_training_of_low_bit-width_convolutional_neural_networks_for_object_detection/#content","title":"Content","text":"<p>We present LBW-Net, an e\ufb03cient optimization based method for quantization and training of the low bit-width convolutional neural networks (CNNs). Speci\ufb01cally, we quantize the weights to zero or powers of two by minimizing the Euclidean distance between full-precision weights and quantized weights during backpropagation. LBW-Net has several desirable advantages over full-precision CNNs, including considerable memory savings, energy e\ufb03ciency, and faster deployment. Our experiments on PASCAL VOC dataset show that compared with its 32-bit \ufb02oating-point counterpart, the performance of the 6-bit LBW-Net is nearly lossless in the object detection tasks, and can even do better in some real world visual scenes, while empirically enjoying more than 4 faster deployment."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_and_training_of_neural_networks_for_efficient_integer-arithmetic-only_inference/","title":"Quantization and training of neural networks for efficient integer-arithmetic-only inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_and_training_of_neural_networks_for_efficient_integer-arithmetic-only_inference/#summary","title":"Summary","text":"<p> The paper proposes a training procedure designed to maintain end-to-end model accuracy while using an integer-only quantization scheme. This scheme can increase on-device inference performance significantly while running on commonly available integer-only hardware. This approach effectively improves the tradeoff between model accuracy and on-device latency, and the results are demonstrated in ImageNet classification and COCO detection on CPUs, even on run-time efficient models like MobileNets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_and_training_of_neural_networks_for_efficient_integer-arithmetic-only_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_and_training_of_neural_networks_for_efficient_integer-arithmetic-only_inference/#content","title":"Content","text":"<p> We propose a quantization scheme to allow on-device inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. The proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_error-based_regularization_for_hardware-aware_neural_network_training/","title":"Quantization error-based regularization for hardware-aware neural network training","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_error-based_regularization_for_hardware-aware_neural_network_training/#summary","title":"Summary","text":"<p> The paper proposes a regularization strategy called QER, which aims to improve the accuracy of quantized neural networks. The regularization term based on quantization errors of weights is added to the loss function to reduce the quantization errors along with the original loss, thereby improving the accuracy. The proposed approach is evaluated on MNIST using a simple neural network model, and the results show that it achieves higher accuracy than the standard training approach with quantized forward propagation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_error-based_regularization_for_hardware-aware_neural_network_training/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_error-based_regularization_for_hardware-aware_neural_network_training/#content","title":"Content","text":"<p>Abstract: We propose \u201cQER\u201d, a novel regularization strategy for hardware-aware neural network training. Although quantized neural networks reduce computation power and resource consumption, it also degrades the accuracy due to quantization errors of the numerical representation, which are defined as differences between original numbers and quantized numbers. The QER solves such the problem by appending an additional regularization term based on quantization errors of weights to the loss function. The regularization term forces the quantization errors of weights to be reduced as well as the original loss. We evaluate our method by using MNIST on a simple neural network model. The evaluation results show that the proposed approach achieves higher accuracy than the standard training approach with quantized forward propagation.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training/","title":"Quantization error as a metric for dynamic precision scaling in neural net training","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training/#summary","title":"Summary","text":"<p>Summary: The paper introduces a dynamic precision scaling scheme that uses reduced numerical precision during training to reduce the computational cost of training. The proposed scheme achieves 98.8% test accuracy on the MNIST dataset using an average bit-width of just 16 bits for weights and 14 bits for activations, compared to the standard 32-bit values.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_error_as_a_metric_for_dynamic_precision_scaling_in_neural_net_training/#content","title":"Content","text":"<p> Recent work has explored reduced numerical precision for parameters, activations, and gradients during neural network training as a way to reduce the computational cost of training. We present a novel dynamic precision scaling (DPS) scheme. Using stochastic fixed-point rounding, a quantization-error based scaling scheme, and dynamic bit-widths during training, we achieve 98.8% test accuracy on the MNIST dataset using an average bit-width of just 16 bits for weights and 14 bits for activations, compared to the standard 32-bit floating point values used in deep learning frameworks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_for_rapid_deployment_of_deep_neural_networks/","title":"Quantization for rapid deployment of deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_for_rapid_deployment_of_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper introduces a method for deploying deep neural networks to energy-efficient accelerators without time-consuming fine-tuning or full datasets. The method involves converting DNNs to limited precision with channel-level distribution recognition to reduce quantization-induced accuracy loss and minimize required image samples. The results demonstrated that the proposed method can quantize 11 networks into 8-bit integer precision without fine-tuning.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_for_rapid_deployment_of_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_for_rapid_deployment_of_deep_neural_networks/#content","title":"Content","text":"<p> This paper presents a new method for rapidly deploying state-of-the-art deep neural networks (DNNs) to energy-efficient accelerators without the need for time-consuming fine tuning or the availability of full datasets. The proposed method involves converting DNNs in full precision to limited precision, which is essential in taking advantage of accelerators with reduced memory footprint and computation power. The method recognizes channel-level distribution to reduce quantization-induced accuracy loss and minimize the required image samples for profiling. The method was evaluated on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark, and the results showed that the networks can be quantized into 8-bit integer precision without fine tuning."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_guided_jpeg_artifact_correction/","title":"Quantization guided jpeg artifact correction","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_guided_jpeg_artifact_correction/#summary","title":"Summary","text":"<p> The paper discusses the limitations of the JPEG image compression algorithm due to information loss resulting in reduced image quality. It proposes a novel architecture allowing a single model to achieve state-of-the-art performance for different quality settings by parameterizing the model with the JPEG file's quantization matrix. This eliminates the need to train a different model for each quality setting."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_guided_jpeg_artifact_correction/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_guided_jpeg_artifact_correction/#content","title":"Content","text":"<p>The JPEG image compression algorithm is the most popular method of image compression because of it's ability for large compression ratios. However, to achieve such high compression, information is lost. For aggressive quantization settings, this leads to a noticeable reduction in image quality. Artifact correction has been studied in the context of deep neural networks for some time, but the current methods delivering state-of-the-art results require a different model to be trained for each quality setting, greatly limiting their practical application. We solve this problem by creating a novel architecture which is parameterized by the JPEG file's quantization matrix. This allows our single model to achieve state-of-the-art performance over models trained for specific quality settings."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_mimic%3A_Towards_very_tiny_cnn_for_object_detection/","title":"Quantization mimic: Towards very tiny cnn for object detection","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_mimic%3A_Towards_very_tiny_cnn_for_object_detection/#summary","title":"Summary","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_mimic%3A_Towards_very_tiny_cnn_for_object_detection/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_mimic%3A_Towards_very_tiny_cnn_for_object_detection/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_Deep_Neural_Networks_for_Accurate_Edge_Computing/","title":"Quantization of Deep Neural Networks for Accurate Edge Computing","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_Deep_Neural_Networks_for_Accurate_Edge_Computing/#summary","title":"Summary","text":"<p>This paper discusses how weight quantization and pruning is applied to Deep Neural Networks to ready them for use in edge devices. While it is believed that quantization leads to performance degradation, the paper argues that regularization on weight representations can actually improve accuracy, and their experiments on three widely used applications show that quantization can improve accuracy by 1-4% with a significant memory reduction."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_Deep_Neural_Networks_for_Accurate_Edge_Computing/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_Deep_Neural_Networks_for_Accurate_Edge_Computing/#content","title":"Content","text":"<p>Deep neural networks (DNNs) have demonstrated their great potential in recent years, exceeding the performance of human experts in a wide range of applications. Due to their large sizes, however, compression techniques such as weight quantization and pruning are usually applied before they can be accommodated on the edge. It is generally believed that quantization leads to performance degradation, and plenty of existing works have explored quantization strategies aiming at minimum accuracy loss. In this paper, we argue that quantization, which essentially imposes regularization on weight representations, can sometimes help to improve accuracy. We conduct comprehensive experiments on three widely used applications: fully connected network (FCN) for biomedical image segmentation, convolutional neural network (CNN) for image classification on ImageNet, and recurrent neural network (RNN) for automatic speech recognition, and experimental results show that quantization can improve the accuracy by 1%, 1.95%, 4.23% on the three applications respectively with 3.5x-6.4x memory reduction."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_deep_neural_networks_for_accumulator-constrained_processors/","title":"Quantization of deep neural networks for accumulator-constrained processors","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_deep_neural_networks_for_accumulator-constrained_processors/#summary","title":"Summary","text":"<p>The paper introduces a quantization methodology to enable fixed-point deployment of ANNs without wide accumulation registers. The quantization problem is aimed at maximizing model accuracy by maximizing input data and weights bit widths. The paper suggests 16-bit accumulators can achieve classification accuracy within 1% of floating-point baselines on image classification benchmarks, leading to a near-optimal 2-speedup for image classification."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_deep_neural_networks_for_accumulator-constrained_processors/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_deep_neural_networks_for_accumulator-constrained_processors/#content","title":"Content","text":"<p> We introduce a quantization methodology for Artificial Neural Networks (ANNs), aimed at fixed-point model deployment on embedded compute platforms without wide accumulation registers, i.e. accumulator-constrained processors. The quantization problem is formulated as a function of accumulator size, aiming to maximize model accuracy by maximizing bit widths of input data and weights. Solutions that fully utilize the available accumulator bits are being tested to reduce the number of configurations considered. Performance tests demonstrate that 16-bit accumulators can achieve classification accuracy within 1% of floating-point baselines on image classification benchmarks. Additionally, a near-optimal 2-speedup is obtained by exploiting 16-bit accumulators for image classification on the All-CNN-C and AlexNet networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_generative_adversarial_networks_for_efficient_inference%3A_a_methodological_study/","title":"Quantization of generative adversarial networks for efficient inference: a methodological study","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_generative_adversarial_networks_for_efficient_inference%3A_a_methodological_study/#summary","title":"Summary","text":"<p> The paper discusses the challenges of deploying Generative Adversarial Networks (GANs) on edge devices due to their resource-intensive nature during inference. It proposes Quantization, a neural network compression technique that replaces floating-point computations with low-bit integer ones, which can facilitate hardware-friendly inference. Through an experimental study, the authors have discovered practical recipes that successfully quantized three diverse GAN architectures without compromising the quality of the original full-precision models, and made them easy to deploy on edge devices."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_generative_adversarial_networks_for_efficient_inference%3A_a_methodological_study/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantization_of_generative_adversarial_networks_for_efficient_inference%3A_a_methodological_study/#content","title":"Content","text":"<p> Generative adversarial networks (GANs) are highly resource-intensive during inference, making deployment on edge devices challenging. Quantization, a neural network compression technique that replaces floating-point computations with low-bit integer ones, can facilitate hardware-friendly inference. However, the performance of quantization techniques in application to GANs remains unclear. To address this challenge, the authors of this paper conducted an experimental study on state-of-the-art quantization techniques on three diverse GAN architectures: StyleGAN, Self-Attention GAN, and CycleGAN. The authors discovered practical recipes that successfully quantized these models for inference with 4/8-bit weights and 8-bit activations while preserving the quality of the original full-precision models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_convolutional_neural_networks_for_mobile_devices/","title":"Quantized convolutional neural networks for mobile devices","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_convolutional_neural_networks_for_mobile_devices/#summary","title":"Summary","text":"<p>Summary: The paper proposes a framework called Quantized CNN to speed up computation and reduce storage and memory overhead of CNN models. They quantize filter kernels in convolutional layers and weighting matrices in fully-connected layers to minimize estimation error of each layer's response. Experiments on ILSVRC-12 show 4-6x speed-up with 15-20x compression and a one percentage loss of classification accuracy, enabling accurate image classification within one second, even on mobile devices.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_convolutional_neural_networks_for_mobile_devices/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_convolutional_neural_networks_for_mobile_devices/#content","title":"Content","text":"<p> In this paper, the authors propose a framework called Quantized CNN, which aims to speed up the computation and reduce the storage and memory overhead of CNN models. They quantize both the filter kernels in convolutional layers and weighting matrices in fully-connected layers to minimize the estimation error of each layer's response. The experiments conducted on the ILSVRC-12 benchmark demonstrate 4-6x speed-up with 15-20x compression and only a one percentage loss of classification accuracy. This method allows for accurate image classification within one second, even on mobile devices."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_deep_neural_networks_for_energy_efficient_hardware-based_inference/","title":"Quantized deep neural networks for energy efficient hardware-based inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_deep_neural_networks_for_energy_efficient_hardware-based_inference/#summary","title":"Summary","text":"<p> The paper proposes a novel approach to reduce energy consumption of a DNN by quantizing the weights, activations, and gradients of the neural network. They demonstrate the effectiveness of the proposed quantization method on several benchmark datasets by achieving up to 4x reduction in energy consumption with only minor retraining loss. A hardware architecture is also proposed, which can achieve up to 7x higher energy efficiency compared to the state-of-the-art FPGA-based accelerator for quantized DNNs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_deep_neural_networks_for_energy_efficient_hardware-based_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_deep_neural_networks_for_energy_efficient_hardware-based_inference/#content","title":"Content","text":"<p>In this paper, we propose a novel approach to reduce the energy consumption of deep neural network (DNN) inference by quantizing the weights, activations, and gradients of the neural network. Most previous approaches in the literature focused only on the quantization of weights and activations. Our approach includes quantization of back-propagation gradients which allows for training quantized models in an end-to-end manner. We demonstrate the effectiveness of our proposed quantization method on several benchmark datasets by achieving up to 4x reduction in energy consumption with only minor retraining loss. We also propose a hardware architecture with a datapath tailored for our quantized neural network that can be efficiently implemented on an FPGA platform. Our hardware design can achieve up to 7x higher energy efficiency compared to the state-of-the-art FPGA-based accelerator for quantized DNNs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_guided_pruning_for_efficient_hardware_implementations_of_convolutional_neural_networks/","title":"Quantized guided pruning for efficient hardware implementations of convolutional neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_guided_pruning_for_efficient_hardware_implementations_of_convolutional_neural_networks/#summary","title":"Summary","text":"<p>This paper proposes a combination of pruning and quantization techniques that effectively reduce the complexity and memory usage of convolutional layers in CNNs, allowing for a low-cost multiplexer to replace the complex convolutional operation. Experiments on CIFAR10, CIFAR100, and SVHN show that this method achieves almost state-of-the-art accuracy while dramatically reducing computational and memory footprints. The paper also proposes a hardware architecture to accelerate CNN operations, which is a pipeline that accommodates multiple layers simultaneously to speed up the inference process."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_guided_pruning_for_efficient_hardware_implementations_of_convolutional_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_guided_pruning_for_efficient_hardware_implementations_of_convolutional_neural_networks/#content","title":"Content","text":"<p>Convolutional Neural Networks (CNNs) are state-of-the-art in numerous computer vision tasks such as object classification and detection. However, the large amount of parameters they contain leads to a high computational complexity and strongly limits their usability in budget-constrained devices such as embedded devices. In this paper, we propose a combination of a new pruning technique and a quantization scheme that effectively reduce the complexity and memory usage of convolutional layers of CNNs, and replace the complex convolutional operation by a low-cost multiplexer. We perform experiments on the CIFAR10, CIFAR100 and SVHN and show that the proposed method achieves almost state-of-the-art accuracy, while drastically reducing the computational and memory footprints. We also propose an efficient hardware architecture to accelerate CNN operations. The proposed hardware architecture is a pipeline and accommodates multiple layers working at the same time to speed up the inference process."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_memory-augmented_neural_networks/","title":"Quantized memory-augmented neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_memory-augmented_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper focuses on quantizing Memory-augmented Neural Networks (MANNs) and proposes a quantization method to address the performance degradation caused mainly due to memory addressing. The proposed method achieves a computation-energy gain of 22 \u00d7 with 8-bit fixed-point and binary quantization compared to floating-point implementation, and results in a model named Quantized MANN (Q-MANN) that improves error rates by 46% and 30% with 8-bit fixed-point and binary quantization, respectively, compared to the MANN quantized using conventional techniques on the bAbI dataset.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_memory-augmented_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_memory-augmented_neural_networks/#content","title":"Content","text":"<p>Memory-augmented neural networks (MANNs) refer to a class of neural network models equipped with external memory. These neural networks outperform conventional recurrent neural networks (RNNs) in terms of learning long-term dependency, allowing them to solve intriguing AI tasks that would otherwise be hard to address. This paper concerns the problem of quantizing MANNs. In this paper, we identify memory addressing as the main reason for the performance degradation and propose a robust quantization method for MANNs to address the challenge. In our experiments, we achieved a computation-energy gain of 22 \u00d7 with 8-bit fixed-point and binary quantization compared to the floating-point implementation. Measured on the bAbI dataset, the resulting model, named the quantized MANN (Q-MANN), improved the error rate by 46% and 30% with 8-bit fixed-point and binary quantization, respectively, compared to the MANN quantized using conventional techniques."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_network_design_under_weight_capacity_constraint/","title":"Quantized neural network design under weight capacity constraint","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_network_design_under_weight_capacity_constraint/#summary","title":"Summary","text":"<p>The paper evaluates the effect of changing the network complexity and word-length of weights on the performance of fully-connected deep neural networks and convolutional neural networks. The authors propose a metric called effective compression ratio (ECR) to compare the complexity of floating-point and fixed-point networks with the same performance. The goal is to provide a guide for determining network size and word-length for efficient hardware implementation of DNNs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_network_design_under_weight_capacity_constraint/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_network_design_under_weight_capacity_constraint/#content","title":"Content","text":"<p>The complexity of deep neural network algorithms can be lowered either by scaling the number of units or reducing the word-length of weights. In this paper, the performances of fully-connected deep neural networks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while changing the network complexity and the word-length of weights. Based on the experiments, the authors propose a metric called the effective compression ratio (ECR) that compares the complexity of floating-point and fixed-point networks showing the same performance. This analysis intends to provide a guideline to network size and word-length determination for efficient hardware implementation of deep neural networks (DNN)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_network_inference_with_precision_batching/","title":"Quantized neural network inference with precision batching","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_network_inference_with_precision_batching/#summary","title":"Summary","text":"<p>The paper presents PrecisionBatching, a quantized inference algorithm that speeds up neural network execution on traditional hardware platforms at low bitwidths without retraining or recalibration. It decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching facilitates quantized inference at low bitwidths (&lt;8bits) and enables traditional hardware platforms to realize inference speedups at finer granularity of quantization. It also allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. The algorithm yields end-to-end speedups of over 8x on a GPU within a &lt;1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5-2x at the same error tolerance across several applications and neural network architectures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_network_inference_with_precision_batching/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_network_inference_with_precision_batching/#content","title":"Content","text":"<p>We present PrecisionBatching, a quantized inference algorithm for speeding up neural network execution on traditional hardware platforms at low bitwidths without the need for retraining or recalibration. PrecisionBatching decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching not only facilitates quantized inference at low bitwidths (&lt;8bits) without the need for retraining/recalibration, but also 1) enables traditional hardware platforms the ability to realize inference speedups at a finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. Across a variety of applications (MNIST, language modeling, natural language inference) and neural network architectures (fully connected, RNN, LSTM), Precision-Batching yields end-to-end speedups of over 8x on a GPU within a &lt;1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5-2x at the same error tolerance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_networks%3A_Characterization_and_holistic_optimization/","title":"Quantized neural networks: Characterization and holistic optimization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_networks%3A_Characterization_and_holistic_optimization/#summary","title":"Summary","text":"<p>Summary: This paper proposes a holistic approach for the optimization of quantized deep neural networks (QDNNs) that includes training methods and quantization-friendly architecture design. Results suggest that wider models are better suited for weight and activation quantization while deeper models are more susceptible to activation quantization. This study can aid in the advancement of QDNN optimization techniques.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_networks%3A_Characterization_and_holistic_optimization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_networks%3A_Characterization_and_holistic_optimization/#content","title":"Content","text":"<p>Abstract: Quantized deep neural networks (QDNNs) are necessary for low-power, high throughput, and embedded applications. This study proposes a holistic approach for the optimization of QDNNs, which contains QDNN training methods as well as quantization-friendly architecture design. The results indicate that deeper models are more prone to activation quantization, while wider models improve the resiliency to both weight and activation quantization. This study can provide insight into better optimization of QDNNs.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_networks_with_new_stochastic_multipliers/","title":"Quantized neural networks with new stochastic multipliers","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_networks_with_new_stochastic_multipliers/#summary","title":"Summary","text":"<p> The authors introduce a new stochastic multiplier with shifted unary code adders (SUC-Adder) for quantized neural networks. They use quantization and stochastic computing to reduce hardware cost. They retrain fully trained neural networks to obtain quantized weights and implement a stochastic matrix multiplication using the SUC-Adder component. The stochastic design reduces energy consumption by about 10x compared to binary implementation while maintaining slightly higher recognition error rates."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_networks_with_new_stochastic_multipliers/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantized_neural_networks_with_new_stochastic_multipliers/#content","title":"Content","text":"<p> In this paper, the authors propose a new stochastic multiplier with shifted unary code adders (SUC-Adder) for quantized neural networks. They combine the technologies of quantization and stochastic computing in neural networks to reduce hardware cost further. They first retrain a fully trained neural network with the back-propagation algorithm in order to obtain neural networks with quantized weights. Then, according to the quantized weights, a stochastic matrix multiplication is implemented with the SUC-Adder component. Experimental results indicate that their stochastic design achieves about 10x energy reduction compared to its counterpart binary implementation while maintaining slightly higher recognition error rates than the binary implementation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantizing_deep_convolutional_networks_for_efficient_inference%3A_A_whitepaper/","title":"Quantizing deep convolutional networks for efficient inference: A whitepaper","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantizing_deep_convolutional_networks_for_efficient_inference%3A_A_whitepaper/#summary","title":"Summary","text":"<p>Summary: The paper presents a study on quantization methods for convolutional neural networks (CNNs), including their trade-offs and limitations. It describes different techniques to train CNNs for quantization and run-time constraints, provides guidelines on how to design and train efficient quantized CNNs with minimal loss in accuracy, and explains how to deploy them on neural network accelerators.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantizing_deep_convolutional_networks_for_efficient_inference%3A_A_whitepaper/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Quantizing_deep_convolutional_networks_for_efficient_inference%3A_A_whitepaper/#content","title":"Content","text":"<p>Quantization of weights and activations in deep neural networks can lead to reduced memory bandwidth requirements and lower power consumption during inference on hardware platforms. We present a study of quantization methods for convolutional neural networks (CNNs) which helps to understand their trade-offs and limitations. We describe these quantization methods for CNNs and quantify their effects on accuracy. We explore different techniques to train CNNs to accommodate quantization and run-time constraints. We also provide guidelines on how to design and train efficient quantized CNNs, with minimal loss in accuracy, and how to deploy them on neural network accelerators."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qusecnets%3A_Quantization-based_defense_mechanism_for_securing_deep_neural_network_against_adversarial_attacks/","title":"Qusecnets: Quantization-based defense mechanism for securing deep neural network against adversarial attacks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qusecnets%3A_Quantization-based_defense_mechanism_for_securing_deep_neural_network_against_adversarial_attacks/#summary","title":"Summary","text":"<p> The paper proposes Constant Quantization (CQ) and Trainable Quantization (TQ), which are two quantization-based defense mechanisms to strengthen the convolutional neural networks' robustness against adversarial attacks. CQ uses a fixed number of quantization levels, while TQ's quantization levels are iteratively learned during the training phase. These techniques were applied to various adversarial attacks from the Cleverhans library, and the result showed a significant increase in classification accuracy for perturbed images generated from the MNIST and CIFAR-10 datasets on commonly used CNN."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qusecnets%3A_Quantization-based_defense_mechanism_for_securing_deep_neural_network_against_adversarial_attacks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Qusecnets%3A_Quantization-based_defense_mechanism_for_securing_deep_neural_network_against_adversarial_attacks/#content","title":"Content","text":"<p>Adversarial examples are a significant threat to convolutional neural networks (CNNs). In this paper, we propose two quantization-based defense mechanisms - Constant Quantization (CQ) and Trainable Quantization (TQ) - to increase CNNs' robustness against adversarial attacks. CQ quantizes input pixel intensities based on a fixed number of quantization levels, while TQ quantization levels are iteratively learned during the training phase, providing a stronger defense mechanism. We apply the proposed techniques on different state-of-the-art adversarial attacks from the open-source Cleverhans library. The experimental results demonstrate a 50%-96% and 10%-50% increase in the classification accuracy of the perturbed images generated from the MNIST and the CIFAR-10 datasets, respectively, on commonly used CNN available in Cleverhans."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/REQ-YOLO%3A_A_resource-aware%2C_efficient_quantization_framework_for_object_detection_on_FPGAs/","title":"REQ-YOLO: A resource-aware, efficient quantization framework for object detection on FPGAs","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/REQ-YOLO%3A_A_resource-aware%2C_efficient_quantization_framework_for_object_detection_on_FPGAs/#summary","title":"Summary","text":"<p>Summary: REQ-YOLO is a resource-aware, systematic weight quantization framework for object detection that significantly compresses the YOLO model while introducing small accuracy degradation, aiming to achieve real-time and highly-efficient implementations on FPGA. The paper presents detailed hardware implementation of block circulant matrices on CONV layers, an efficient processing element (PE) supporting weight quantization, CONV dataflow and pipelining techniques, design optimization, and a template-based automatic synthesis framework to optimally exploit hardware resource. It also touches upon the requirements of real-time and energy-efficient implementations of DNNs on a power-constrained system and the two research thrusts dedicated to performance and energy efficiency of DNNs.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/REQ-YOLO%3A_A_resource-aware%2C_efficient_quantization_framework_for_object_detection_on_FPGAs/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/REQ-YOLO%3A_A_resource-aware%2C_efficient_quantization_framework_for_object_detection_on_FPGAs/#content","title":"Content","text":"<p> This paper proposes REQ-YOLO, a resource-aware, systematic weight quantization framework for object detection, considering both algorithm and hardware resource aspects in object detection. The proposed framework significantly compresses the YOLO model while introducing very small accuracy degradation. The aim is to achieve real-time and highly-efficient implementations on FPGA, and the paper presents the detailed hardware implementation of block circulant matrices on CONV layers, as well as an efficient processing element (PE) structure supporting the heterogeneous weight quantization, CONV dataflow and pipelining techniques, design optimization, and a template-based automatic synthesis framework to optimally exploit hardware resource. The paper also touches upon the requirements of real-time and energy-efficient implementations of DNNs on a power-constrained system and the two research thrusts dedicated to performance and energy efficiency enhancement of the inference phase of DNNs \u2013 model compression techniques and efficient hardware implementations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/REx%3A_Data-Free_Residual_Quantization_Error_Expansion/","title":"REx: Data-Free Residual Quantization Error Expansion","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/REx%3A_Data-Free_Residual_Quantization_Error_Expansion/#summary","title":"Summary","text":"<p>REx is a flexible quantization method that uses residual error expansion, group sparsity, and ensemble approximation to reduce the high inference cost of deep neural networks. It achieves superior performance on various applications, architectures, and bit-width. Unlike traditional quantization methods, REx provides multiple accuracy vs. speed trade-off points for each bit width. It outperforms data-free quantization algorithms and offers an interesting alternative to data-driven techniques in terms of accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/REx%3A_Data-Free_Residual_Quantization_Error_Expansion/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/REx%3A_Data-Free_Residual_Quantization_Error_Expansion/#content","title":"Content","text":"<p>Deep neural networks suffer from high inference cost, which can be addressed by quantization. We propose REx, a flexible quantization method that leverages residual error expansion, group sparsity and ensemble approximation for better parallelization. REx achieves superior performance on every benchmarked application, architecture and bit-width. Traditional quantization methods offer limited options for a specific target device, but REx provides multiple accuracy vs. speed trade-off points for each bit width. Data-free quantization algorithms were also published in recent years, but they still struggle to offer an interesting alternative to data-driven techniques in terms of accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/RMSMP%3A_A_Novel_Deep_Neural_Network_Quantization_Framework_with_Row-wise_Mixed_Schemes_and_Multiple_Precisions/","title":"RMSMP: A Novel Deep Neural Network Quantization Framework with Row-wise Mixed Schemes and Multiple Precisions","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/RMSMP%3A_A_Novel_Deep_Neural_Network_Quantization_Framework_with_Row-wise_Mixed_Schemes_and_Multiple_Precisions/#summary","title":"Summary","text":"<p> The paper proposes a new method for quantizing deep neural networks called RMSMP, which assigns mixed quantization schemes and multiple precisions within layers for simplified operations in hardware inference, while preserving accuracy. The quantization error can be mitigated as long as a certain portion of the weights in every layer are in higher precisions, enabling layer-wise uniformity in the hardware implementation towards guaranteed inference acceleration, while still enjoying row-wise flexibility of mixed schemes and multiple precisions to boost accuracy. The candidates of schemes and precisions are derived practically and effectively with a highly hardware-informative strategy to reduce the problem search space."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/RMSMP%3A_A_Novel_Deep_Neural_Network_Quantization_Framework_with_Row-wise_Mixed_Schemes_and_Multiple_Precisions/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/RMSMP%3A_A_Novel_Deep_Neural_Network_Quantization_Framework_with_Row-wise_Mixed_Schemes_and_Multiple_Precisions/#content","title":"Content","text":"<p> This work proposes a novel Deep Neural Network (DNN) quantization framework, namely RMSMP , with a Row-wise Mixed-Scheme and Multi-Precision approach. Specifically, this is the first effort to assign mixed quantization schemes and multiple precisions within layers \u2013 among rows of the DNN weight matrix, for simplified operations in hardware inference, while preserving accuracy. Furthermore, this paper makes a different observation from the prior work that the quantization error does not necessarily exhibit the layer-wise sensitivity, and actually can be mitigated as long as a certain portion of the weights in every layer are in higher precisions. This observation enables layer-wise uniformity in the hardware implementation towards guaranteed inference acceleration, while still enjoying row-wise flexibility of mixed schemes and multiple precisions to boost accuracy. The candidates of schemes and precisions are derived practically and effectively with a highly hardware-informative strategy to reduce the problem search space."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rapq%3A_Rescuing_accuracy_for_power-of-two_low-bit_post-training_quantization/","title":"Rapq: Rescuing accuracy for power-of-two low-bit post-training quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rapq%3A_Rescuing_accuracy_for_power-of-two_low-bit_post-training_quantization/#summary","title":"Summary","text":"<p>Authors propose a Power-of-Two low-bit post-training quantization method, called RAPQ, which dynamically adjusts the Power-of-Two scales of the whole network instead of statically determining them layer by layer. The proposed method outperforms SOTA PTQ methods with nearly the same accuracy, reaching an accuracy of 65% and 48% on ResNet-18 and MobileNetV2 respectively with weight INT2 activation INT4 on ImageNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rapq%3A_Rescuing_accuracy_for_power-of-two_low-bit_post-training_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rapq%3A_Rescuing_accuracy_for_power-of-two_low-bit_post-training_quantization/#content","title":"Content","text":"<p>We introduce a Power-of-Two low-bit post-training quantization(PTQ) method for deep neural network that meets hardware requirements and does not call for long-time retraining. We propose a novel Power-of-Two PTQ framework, dubbed RAPQ, which dynamically adjusts the Power-of-Two scales of the whole network instead of statically determining them layer by layer. Extensive experiments on ImageNet prove the excellent performance of our proposed method. Without bells and whistles, RAPQ can reach accuracy of 65% and 48% on ResNet-18 and MobileNetV2 respectively with weight INT2 activation INT4. We are the first to propose the more constrained but hardware-friendly Power-of-Two quantization scheme for low-bit PTQ specially and prove that it can achieve nearly the same accuracy as SOTA PTQ method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Real-time_quantized_image_super-resolution_on_mobile_npus%2C_mobile_ai_2021_challenge%3A_Report/","title":"Real-time quantized image super-resolution on mobile npus, mobile ai 2021 challenge: Report","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Real-time_quantized_image_super-resolution_on_mobile_npus%2C_mobile_ai_2021_challenge%3A_Report/#summary","title":"Summary","text":"<p>Summary: The paper presents the results of the first Mobile AI challenge where participants trained quantized models for 3X image upscaling. The proposed solutions achieved real-time performance on mobile or edge NPUs and can reconstruct Full HD images under 40-60 ms while providing high fidelity results. The paper provides a detailed description of all the models developed in the challenge.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Real-time_quantized_image_super-resolution_on_mobile_npus%2C_mobile_ai_2021_challenge%3A_Report/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Real-time_quantized_image_super-resolution_on_mobile_npus%2C_mobile_ai_2021_challenge%3A_Report/#content","title":"Content","text":"<p> Image super-resolution is an important computer vision problem with applications for mobile devices. To optimize this task for mobile and edge NPUs, the authors introduced the first Mobile AI challenge, where participants trained quantized models to perform 3X image upscaling, using the DIV2K dataset. The proposed solutions achieved real-time performance on mobile or edge NPUs and are compatible with major mobile AI accelerators. The proposed solutions can reconstruct Full HD images under 40-60 ms while providing high fidelity results. The authors provided a detailed description of all the models developed in the challenge."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Reducing_the_model_order_of_deep_neural_networks_using_information_theory/","title":"Reducing the model order of deep neural networks using information theory","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Reducing_the_model_order_of_deep_neural_networks_using_information_theory/#summary","title":"Summary","text":"<p>The paper proposes a method to compress deep neural networks by estimating the Fisher Information metric through a stochastic optimization method. The method involves removing unimportant parameters and using non-uniform fixed point quantization to assign more bits to parameters with higher Fisher Information estimates. The proposed method outperforms existing methods for network pruning and quantization and was evaluated on a classification task with a convolutional neural network trained on the MNIST dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Reducing_the_model_order_of_deep_neural_networks_using_information_theory/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Reducing_the_model_order_of_deep_neural_networks_using_information_theory/#content","title":"Content","text":"<p> In this research paper, a method to compress deep neural networks by reducing the complexity of DNNs has been proposed using the Fisher Information metric by estimating it through a stochastic optimization method. The authors first remove unimportant parameters and then use non-uniform fixed point quantization to assign more bits to parameters with higher Fisher Information estimates. The performance of the proposed method was evaluated on a classification task with a convolutional neural network trained on the MNIST dataset, and experimental results showed that this method outperforms existing methods for both network pruning and quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Region_graph_embedding_network_for_zero-shot_learning/","title":"Region graph embedding network for zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Region_graph_embedding_network_for_zero-shot_learning/#summary","title":"Summary","text":"<p>Summary: This paper proposes a region-based relation reasoning method called Region Graph Embedding Network (RGEN) for Zero-Shot Learning. The proposed method incorporates region-based relation reasoning in ZSL by representing the local image regions as a graph, on which the parts relation reasoning is performed with graph convolutions. The proposed method is validated through experiments conducted on four datasets under both ZSL and generalized ZSL settings.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Region_graph_embedding_network_for_zero-shot_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Region_graph_embedding_network_for_zero-shot_learning/#content","title":"Content","text":"<p>Most of the existing Zero-Shot Learning (ZSL) approaches learn direct embeddings from global features or image parts (regions) to the semantic space, which, however, fail to capture the appearance relationships between different local regions within a single image. In this paper, to model the relations among local image regions, we incorporate the region-based relation reasoning into ZSL. Our method, termed as Region Graph Embedding Network (RGEN), is trained end-to-end from raw image data. Specifically, RGEN consists of two branches: the Constrained Part Attention (CPA) branch and the Parts Relation Reasoning (PRR) branch. CPA branch is built upon attention and produces the image regions. To exploit the progressive interactions among these regions, we represent them as a region graph, on which the parts relation reasoning is performed with graph convolutions, thus leading to our PRR branch. To train our model, we introduce both a transfer loss and a balance loss to contrast class similarities and pursue the maximum response consistency among seen and unseen outputs, respectively. Extensive experiments on four datasets well validate the effectiveness of the proposed method under both ZSL and generalized ZSL settings."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relation-based_discriminative_cooperation_network_for_zero-shot_classification/","title":"Relation-based discriminative cooperation network for zero-shot classification","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relation-based_discriminative_cooperation_network_for_zero-shot_classification/#summary","title":"Summary","text":"<p> The paper proposes a Relation-based Discriminative Cooperation Network for Zero-Shot Classification that leverages compatibility relation and significant differences among classes through cooperative learning. The proposed method demonstrates better performance than state-of-the-art methods on four widely-used benchmarks in practical zero-shot scenarios."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relation-based_discriminative_cooperation_network_for_zero-shot_classification/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relation-based_discriminative_cooperation_network_for_zero-shot_classification/#content","title":"Content","text":"<p> Relation-based Discriminative Cooperation Network for Zero-Shot Classification is proposed to tackle the task of zero-shot classification (ZSC) by fully leveraging the compatibility relation between class embeddings and the significant differences among classes via cooperative learning. Specifically, we introduce two types of relational interaction mechanisms to support constructive reasoning over the target-class and the prototype-class embeddings, respectively. Based on the discriminatively learned embeddings and the refined compatibility relation, ZSC can be conducted by searching for the closest matching class on the Semantic manifold. Extensive experiments on four widely-used benchmarks demonstrate the proposed method outperforms state-of-the-art methods by a large margin, and sheds new light on practical zero-shot scenarios."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relative_representations_enable_zero-shot_latent_space_communication/","title":"Relative representations enable zero-shot latent space communication","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relative_representations_enable_zero-shot_latent_space_communication/#summary","title":"Summary","text":"<p>The authors propose the use of latent similarity between each sample and a fixed set of anchors as an alternative data representation. They demonstrate that it enforces the desired invariances without additional training, allowing for latent space communication and effective model stitching. They validate the capability of this approach on various datasets, modalities, tasks, and architectures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relative_representations_enable_zero-shot_latent_space_communication/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relative_representations_enable_zero-shot_latent_space_communication/#content","title":"Content","text":"<p>Neural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-speci\ufb01c constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, the angles between the encodings within distinct latent spaces do not change. In this work, we propose the latent similarity between each sample and a \ufb01xed set of anchors as an alternative data representation, demonstrating that it can enforce the desired invariances without any additional training. We show how neural architectures can leverage these relative representations to guarantee, in practice, invariance to latent isometries and rescalings, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings. We extensively validate the generalization capability of our approach on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classi\ufb01cation, reconstruction) and architectures (e.g., CNNs, GCNs, transformers)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relative_robustness_of_quantized_neural_networks_against_adversarial_attacks/","title":"Relative robustness of quantized neural networks against adversarial attacks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relative_robustness_of_quantized_neural_networks_against_adversarial_attacks/#summary","title":"Summary","text":"<p>Summary: The paper studies the robustness of quantized neural networks (NNs) against adversarial attacks. The authors compare the relative robustness of various quantized NNs to their floating-point counterparts and find that some quantization methods improve robustness while others do not. Additionally, they propose a new method for quantizing NNs that improves robustness while maintaining accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relative_robustness_of_quantized_neural_networks_against_adversarial_attacks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relative_robustness_of_quantized_neural_networks_against_adversarial_attacks/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relaxed_quantization_for_discretized_neural_networks/","title":"Relaxed quantization for discretized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relaxed_quantization_for_discretized_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper introduces Relaxed Quantization (RQ), a differentiable quantization method that converts continuous distributions over network weights and activations into categorical distributions over the quantization grid and then relaxes them into continuous surrogates for efficient gradient-based optimization. The authors validate the performance of their method experimentally for classifying images using several datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relaxed_quantization_for_discretized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Relaxed_quantization_for_discretized_neural_networks/#content","title":"Content","text":"<p> Neural network quantization is a research field with significant impact on the deployment of models on resource-limited devices. The goal of this paper is to introduce Relaxed Quantization (RQ), a differentiable quantization method that transforms continuous distributions over network weights and activations into categorical distributions over the quantization grid, and subsequently relaxes them to continuous surrogates for efficient gradient-based optimization. Stochastic rounding can be seen as a special case of this approach, and the quantization grid can also be optimized using gradient descent. The authors validate the performance of their method experimentally for classifying images using the MNIST, CIFAR 10, and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Releq%3A_A_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/","title":"Releq: A reinforcement learning approach for deep quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Releq%3A_A_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>This paper proposes a systematic approach to automating the process of deep quantization for neural networks to minimize computational and storage costs while preserving accuracy. The approach uses a deep reinforcement learning framework called RELEQ. The proposed approach provides a general solution for quantization of a variety of deep networks with the potential to achieve a speedup of up to 2.2x and reduce energy consumption for hardware and custom DNN accelerators compared to 8-bit runs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Releq%3A_A_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Releq%3A_A_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#content","title":"Content","text":"<p> Deep Neural Networks (DNNs) require massive amounts of computation resources in inference tasks for computer vision applications. Quantization can significantly reduce DNN computation and storage by decreasing the bitwidth of network encodings. Recent research affirms that carefully selecting the quantization levels for each layer can preserve the accuracy while pushing the bitwidth below eight bits. However, without manual effort, this deep quantization can lead to a significant loss in accuracy. We proposed a systematic approach to tackle this problem, by automating the process of discovering the quantization levels through an end-to-end deep reinforcement learning framework (RELEQ). We show how RELEQ can balance speed and quality, and provide an asymmetric general solution for quantization of a large variety of deep networks that virtually preserve the accuracy while minimizing the computation and storage cost. With these DNNs, RELEQ enables conventional hardware to achieve a 2.2x speedup over 8-bit execution, and a custom DNN accelerator achieves a 2.0x speedup and energy reduction compared to 8-bit runs. These encouraging results mark RELEQ as the initial step towards automating the deep quantization of neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Resource-aware_object_classification_and_segmentation_for_semi-autonomous_grasping_with_prosthetic_hands/","title":"Resource-aware object classification and segmentation for semi-autonomous grasping with prosthetic hands","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Resource-aware_object_classification_and_segmentation_for_semi-autonomous_grasping_with_prosthetic_hands/#summary","title":"Summary","text":"<p>Summary: The paper presents a visual perception system that extracts scene information for semi-autonomous hand control using electromyographic (EMG) signals captured by two surface electrodes attached to the human body. The system allows for intuitive and effortless control with few simple user commands, reduces the complexity of manual control, and increases classification and segmentation accuracy up to 96.5% and 89.5% respectively. The system runs on an in-hand Arm Cortex-H7 microcontroller at only 400 MHz and captures visual environmental information for real-time evaluation, enabling offline design with online parametrization.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Resource-aware_object_classification_and_segmentation_for_semi-autonomous_grasping_with_prosthetic_hands/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Resource-aware_object_classification_and_segmentation_for_semi-autonomous_grasping_with_prosthetic_hands/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rethinking_Data-Free_Quantization_as_a_Zero-Sum_Game/","title":"Rethinking Data-Free Quantization as a Zero-Sum Game","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rethinking_Data-Free_Quantization_as_a_Zero-Sum_Game/#summary","title":"Summary","text":"<p>This paper proposes a method called Adaptability-aware Sample Generation (AdaSG) for data-free quantization (DFQ) that reformulates DFQ as a zero-sum game between two players: a generator and a quantized network. AdaSG aims to generate a sample with desirable adaptability to maximally recover the performance of the quantized network with varied bit widths."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rethinking_Data-Free_Quantization_as_a_Zero-Sum_Game/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rethinking_Data-Free_Quantization_as_a_Zero-Sum_Game/#content","title":"Content","text":"<p>Data-free quantization (DFQ) is a potential method to quantize models without accessing the original data by synthesizing meaningful fake samples instead, which improves the quantized network (Q) by knowledge distillation against the pretrained full-precision model (P). The generative model is introduced as a generator (G) to capture the distribution of the original data from P for better fake samples. However, there still remains a non-ignorable performance loss when encountering various bit-width settings. In this paper, we revisit DFQ and specialize it as a zero-sum game between two players - a generator and a quantized network, and further propose an Adaptability-aware Sample Generation (AdaSG) method. AdaSG reformulates DFQ as a dynamic maximization-vs-minimization game process anchored on the sample adaptability, and aims to generate the sample with desirable adaptability to maximally recover the performance of Q with varied bit widths."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rethinking_differentiable_search_for_mixed-precision_neural_networks/","title":"Rethinking differentiable search for mixed-precision neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rethinking_differentiable_search_for_mixed-precision_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes a new approach for searching the optimal mixed-precision network search (MPS) problem, which is used to tune the bit-width to individual filter requirements. The proposed Efficient differentiable MIxed-Precision network Search (EdMIPS) method is effective in finding the optimal bit allocation for multiple popular deep neural networks, and can search a large model, e.g. Inception-V3, directly on ImageNet without proxy task in a reasonable amount of time. The learned mixed-precision networks outperform their uniform counterparts by accounting for different sensitivities of different filters."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rethinking_differentiable_search_for_mixed-precision_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rethinking_differentiable_search_for_mixed-precision_neural_networks/#content","title":"Content","text":"<p>Low-precision networks, with weights and activations quantized to low bit-width, are widely used to accelerate inference on edge devices. However, current solutions are uniform, using identical bit-width for all filters. This fails to account for the different sensitivities of different filters and is suboptimal. Mixed-precision networks address this problem, by tuning the bit-width to individual filter requirements. In this work, the problem of optimal mixed-precision network search (MPS) is considered. To circumvent its difficulties of discrete search space and combinatorial optimization, a new differentiable search architecture is proposed, with several novel contributions to advance the efficiency by leveraging the unique properties of the MPS problem. The resulting Efficient differentiable MIxed-Precision network Search (EdMIPS) method is effective at finding the optimal bit allocation for multiple popular networks, and can search a large model, e.g. Inception-V3, directly on ImageNet without proxy task in a reasonable amount of time. The learned mixed-precision networks significantly outperform their uniform counterparts."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Retracted_article%3A_Quantized_majorana_conductance/","title":"Retracted article: Quantized majorana conductance","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Retracted_article%3A_Quantized_majorana_conductance/#summary","title":"Summary","text":"<p> The paper reports the observation of conductance quantization in nanowires coupled to arrays of InSb semiconductor nanowires with epitaxial aluminum shells, which is consistent with the predicted Majorana-induced quantization. The findings provide strong evidence for Majorana zero modes in such devices and demonstrate a platform for topologically protected quantum computation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Retracted_article%3A_Quantized_majorana_conductance/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Retracted_article%3A_Quantized_majorana_conductance/#content","title":"Content","text":"<p>Abstract: We report the observation of conductance quantization in nanowires coupled to arrays of InSb semiconductor nanowires with epitaxial aluminum shells. The resulting devices exhibit a series of conductance plateaus at discrete values, which are consistent with the predicted Majorana-induced quantization. Our observations provide strong evidence for Majorana zero modes in such devices and demonstrate a platform for topologically protected quantum computation.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Retraining-based_iterative_weight_quantization_for_deep_neural_networks/","title":"Retraining-based iterative weight quantization for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Retraining-based_iterative_weight_quantization_for_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper presents an iterative technique for weight quantization, which involves weight quantization and retraining the model with full precision weights. This approach achieves high model compression ratios without altering the training algorithm. The results show that an LSTM model using 1-bit quantized weights suffices for a PTB dataset without accuracy degradation, which outperforms previous methods that required 2-4 bits for quantized weights.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Retraining-based_iterative_weight_quantization_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Retraining-based_iterative_weight_quantization_for_deep_neural_networks/#content","title":"Content","text":"<p> Model compression has gained attention for reducing hardware resource requirements while maintaining accuracy of deep neural networks. Quantization is an effective method for model compression, but minimizing the number of bits to represent parameters has been a challenge. We present an iterative technique for weight quantization that achieves a high compression ratio without modifications to the training algorithm. Our proposed technique involves weight quantization followed by retraining the model with full precision weights, generating new sets of weights that can be quantized with decreasing loss at each iteration. We also show that quantization can efficiently leverage pruning. Our experimental results demonstrate that an LSTM model using 1-bit quantized weights suffices for a PTB dataset without accuracy degradation, while previous methods required at least 2-4 bits for quantized weights."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Revisiting_the_calibration_of_modern_neural_networks/","title":"Revisiting the calibration of modern neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Revisiting_the_calibration_of_modern_neural_networks/#summary","title":"Summary","text":"<p> The paper focuses on the importance of accurate estimation of predictive uncertainty in neural networks for safe application. The authors revisit the question of model calibration and accuracy in recent state-of-the-art image classification models. They find that newer models not using convolutions are well-calibrated, and the trends observed in prior models such as decay of calibration with distribution shift or model size are less pronounced in recent architectures. The study suggests that architecture plays a major role in determining calibration properties."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Revisiting_the_calibration_of_modern_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Revisiting_the_calibration_of_modern_neural_networks/#content","title":"Content","text":"<p>Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Riptide%3A_Fast_end-to-end_binarized_neural_networks/","title":"Riptide: Fast end-to-end binarized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Riptide%3A_Fast_end-to-end_binarized_neural_networks/#summary","title":"Summary","text":"<p>This paper explores the challenges in achieving high performance using binarized neural networks and proposes solutions to overcome them. The proposed techniques show speedups ranging from 4-12 times and demonstrate a 6.3x speedup over a standard VGGNet variant with state-of-the-art accuracy. The authors highlight the importance of missing implementations for certain operations and carefully scheduled library support for binarized linear algebra operations to achieve such performance gains."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Riptide%3A_Fast_end-to-end_binarized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Riptide%3A_Fast_end-to-end_binarized_neural_networks/#content","title":"Content","text":"<p>Binarized neural networks have attracted much recent attention due to their promise of making convolutional neural networks fast and compact. However, these benefits have proven hard to realize in practice. In this paper, we identify the underlying barriers to high performance and propose solutions ranging from missing implementations for certain operations to carefully scheduled library support for binarized linear algebra operations. The combination of these innovations allows us to report the first measured end-to-end speedups for binarized networks. For instance, we show a 6.3x speedup over a standard VGGNet variant at state-of-the-art (64.2% for top-1 binarized classification of ImageNet) accuracy. More broadly, speedups range from 4-12x and the techniques we propose are crucial to achieving them."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ristretto%3A_A_framework_for_empirical_study_of_resource-efficient_inference_in_convolutional_neural_networks/","title":"Ristretto: A framework for empirical study of resource-efficient inference in convolutional neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ristretto%3A_A_framework_for_empirical_study_of_resource-efficient_inference_in_convolutional_neural_networks/#summary","title":"Summary","text":"<p>This paper describes Ristretto, a CNN approximation framework that allows for investigating the trade-off between various number representation and word width choices and the classification accuracy of the model. Ristretto analyzes a given CNN with respect to numerical range required to represent weights, activations, and intermediate results of convolutional and fully connected layers, and subsequently simulates the effect of reduced word width or lower precision arithmetic operators on the model accuracy. Additionally, Ristretto can fine-tune a quantized network to enhance its classification accuracy under a given number representation and word width configuration. The aim of the framework is to enable efficient implementation of CNNs on resource-constrained embedded systems."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ristretto%3A_A_framework_for_empirical_study_of_resource-efficient_inference_in_convolutional_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ristretto%3A_A_framework_for_empirical_study_of_resource-efficient_inference_in_convolutional_neural_networks/#content","title":"Content","text":"<p>Convolutional neural networks (CNN) have led to remarkable progress in key pattern recognition tasks, but inference using pre-trained modern deep CNNs requires significant system resources. To enable efficient implementation on resource-constrained embedded systems, we present Ristretto, a CNN approximation framework that enables empirical investigation of the trade-off between various number representation and word width choices, and the classification accuracy of the model. Ristretto analyzes a given CNN with respect to numerical range required to represent weights, activations and intermediate results of convolutional and fully connected layers, and subsequently, it simulates the impact of reduced word width or lower precision arithmetic operators on the model accuracy. Moreover, Ristretto can fine-tune a quantized network to further improve its classification accuracy under a given number representation and word width configuration."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rna%3A_An_accurate_residual_network_accelerator_for_quantized_and_reconstructed_deep_neural_networks/","title":"Rna: An accurate residual network accelerator for quantized and reconstructed deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rna%3A_An_accurate_residual_network_accelerator_for_quantized_and_reconstructed_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper introduces a quantized and reconstructed deep neural network technique called QR-DNN, which adds batch normalization (BN) layers during training and later removes them for efficient hardware implementation. It also presents a residual network accelerator (RNA) that utilizes batch-normalization-free structures and logarithmic number system for weights. RNA uses a systolic array architecture for shift-and-accumulate operations instead of multiplication operations, and is reported to have the best fixed-point accelerators. The proposed techniques result in improved accuracy and an FPGA implementation of ResNet-50 achieves state-of-the-art results.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rna%3A_An_accurate_residual_network_accelerator_for_quantized_and_reconstructed_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rna%3A_An_accurate_residual_network_accelerator_for_quantized_and_reconstructed_deep_neural_networks/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robust_image_hashing_using_non-uniform_sampling_in_discrete_Fourier_domain/","title":"Robust image hashing using non-uniform sampling in discrete Fourier domain","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robust_image_hashing_using_non-uniform_sampling_in_discrete_Fourier_domain/#summary","title":"Summary","text":"<p>Summary: The paper proposes a robust image hashing method in discrete Fourier domain for image authentication and retrieval. The method involves using image resizing, total variation based filtering, rotation projection, and non-uniform sampling to extract the robust frequency feature from the secondary image after discrete Fourier transform. The intermediate sampling feature vectors are then scrambled and quantized to produce the resulting binary hash securely. The method is shown to have satisfactory robustness against perceptual content-preserving manipulations and low probability for collision of the hashes of distinct images.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robust_image_hashing_using_non-uniform_sampling_in_discrete_Fourier_domain/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robust_image_hashing_using_non-uniform_sampling_in_discrete_Fourier_domain/#content","title":"Content","text":"<p>This paper proposes a robust image hashing method in discrete Fourier domain that can be applied in such fields as image authentication and retrieval. In the pre-processing stage, image resizing and total variation based filtering are first used to regularize the input image. Then the secondary image is obtained by the rotation projection, and the robust frequency feature is extracted from the secondary image after discrete Fourier transform. More sampling points are chosen from the low- and middle-frequency component to represent the salient content of the image effectively, which is achieved by the non-uniform sampling. Finally, the intermediate sampling feature vectors are scrambled and quantized to produce the resulting binary hash securely. The security of the method depends entirely on the secret key. Experiments are conducted to show that the present method has satisfactory robustness against perceptual content-preserving manipulations and has also very low probability for collision of the hashes of distinct images."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robust_quantization%3A_One_model_to_rule_them_all/","title":"Robust quantization: One model to rule them all","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robust_quantization%3A_One_model_to_rule_them_all/#summary","title":"Summary","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robust_quantization%3A_One_model_to_rule_them_all/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robust_quantization%3A_One_model_to_rule_them_all/#content","title":"Content","text":"<p> Neural network quantization methods often involve simulating the quantization process during training, making the trained model highly dependent on the target bit-width and precise way quantization is performed. Robust quantization offers an alternative approach with improved tolerance to different classes of data-types and quantization policies. It opens up new exciting applications where the quantization process is not static and can vary to meet different circumstances and implementations. To address this issue, we propose a method that provides intrinsic robustness to the model against a broad range of quantization processes. Our method is motivated by theoretical arguments and enables us to store a single generic model capable of operating at various bit-widths and quantization policies. We validate our method\u2019s effectiveness on different ImageNet models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robustness-aware_2-bit_quantization_with_real-time_performance_for_neural_network/","title":"Robustness-aware 2-bit quantization with real-time performance for neural network","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robustness-aware_2-bit_quantization_with_real-time_performance_for_neural_network/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new 2-bit quantization scheme for neural networks based on binary neural networks and generative adversarial network (GANs) to improve performance and consider the robustness of the quantized neural network. The proposed method uses shift addition operation instead of multiply-accumulate to speed up the network and a structural loss to preserve structural information after quantization. Furthermore, the method introduces a non-sensitive perturbation loss function to consider the robustness of the quantized neural network. Experimental results demonstrate that the proposed scheme is competitive and robust under adversarial attacks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robustness-aware_2-bit_quantization_with_real-time_performance_for_neural_network/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Robustness-aware_2-bit_quantization_with_real-time_performance_for_neural_network/#content","title":"Content","text":"<p>Quantized neural network with a reduced bit precision is an effective solution to reduce the computational and memory resource requirements and plays a vital role in machine learning. However, it is still challenging to avoid the significant accuracy degradation due to its numerical approximation and lower redundancy. In this paper, a novel robustness-aware 2-bit quantization scheme is proposed for NN based on binary NN and generative adversarial network (GAN), which improves the performance by enriching the information of binary NN, efficiently extract the structural information and considering the robustness of the quantized NN. Specifically, using shift addition operation to replace the multiply-accumulate in the quantization process which can effectively speed the NN. Meanwhile, a structural loss between the original NN and quantized NN is proposed to such that the structural information of data is preserved after quantization. The structural information learned from NN not only plays an important role in improving the performance but also allows for further finetuning of the quantization network by applying the Lipschitz constraint to the structural loss. In addition, we also for the first time take the robustness of the quantized NN into consideration and propose a non-sensitive perturbation loss function by introducing an extra-neous term of spectral norm. The experiments are conducted on CIFAR-10 and ImageNet datasets with popular NN (such as MoblieNetV2, SqueezeNet, ResNet20, etc.). The experimental results show that the proposed algorithm is more competitive under 2-bit-precision than the state-of-the-art quantization methods. Meanwhile, the experimental results also demonstrate that the proposed method is robust under the FGSM adversarial samples attack."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rotated_binary_neural_network/","title":"Rotated binary neural network","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rotated_binary_neural_network/#summary","title":"Summary","text":"<p>This paper provides a solution to the major impediment that the Binary Neural Network (BNN) faces when reducing deep neural network complexity, which is severe performance degradation due to the large quantization error between the full-precision weight vector and its binary vector. The authors introduce the Rotated Binary Neural Network (RBNN), which considers the angle alignment between the full-precision weight vector and its binarized version. The RBNN uses rotating techniques at the beginning of each training epoch to reduce the angular bias, leading to about a 50% weight flip maximizing information gain. Additionally, the paper proposes a training-aware approximation of the sign function for the gradient backward.  Experiments on CIFAR-10 and ImageNet demonstrate that RBNN performs better than many state-of-the-art methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rotated_binary_neural_network/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rotated_binary_neural_network/#content","title":"Content","text":"<p>Binary Neural Network (BNN) shows its predominance in reducing the complexity of deep neural networks. However, it suffers severe performance degradation. One of the major impediments is the large quantization error between the full-precision weight vector and its binary vector. Previous works focus on compensating for the norm gap while leaving the angular bias hardly touched. In this paper, for the \ufb01rst time, we explore the in\ufb02uence of angular bias on the quantization error and then introduce a Rotated Binary Neural Network (RBNN), which considers the angle alignment between the full-precision weight vector and its binarized version. At the beginning of each training epoch, we propose to rotate the full- precision weight vector to its binary vector to reduce the angular bias. To avoid the high complexity of learning a large rotation matrix, we further introduce a bi-rotation formulation that learns two smaller rotation matrices. In the training stage, we devise an adjustable rotated weight vector for binarization to escape the potential local optimum. Our rotation leads to around 50% weight \ufb02ips which maximize the information gain. Finally, we propose a training-aware approximation of the sign function for the gradient backward. Experiments on CIFAR-10 and ImageNet demonstrate the superiorities of RBNN over many state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at https://github.com/lmbxmu/RBNN."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rotation_consistent_margin_loss_for_efficient_low-bit_face_recognition/","title":"Rotation consistent margin loss for efficient low-bit face recognition","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rotation_consistent_margin_loss_for_efficient_low-bit_face_recognition/#summary","title":"Summary","text":"<p>The paper proposes a rotation consistent margin loss to improve low-bit quantization in face recognition through the disentanglement of quantization errors in angular space into individual and class errors. Instead of minimizing the entire quantization error, only the individual error is minimized to increase feature discriminative power. The proposed loss is found to be superior in low-bit quantization on benchmark datasets such as MegaFace Challenge, Youtube Faces (YTF), Labeled Face in the Wild (LFW) and IJB-C."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rotation_consistent_margin_loss_for_efficient_low-bit_face_recognition/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Rotation_consistent_margin_loss_for_efficient_low-bit_face_recognition/#content","title":"Content","text":"<p> In this paper, we propose a rotation consistent margin (RCM) loss to address the low-bit quantization problem in face recognition (FR) under the open-set protocol. We redefine quantization errors (QEs) in angular space and disentangle them into class error and individual error, which correspond to inter-class separability and intra-class compactness, respectively. Instead of eliminating the entire QEs, we minimize the individual error with RCM loss, which improves feature discriminative power. Extensive experiments demonstrate the superiority of our proposed loss in low-bit FR quantization tasks on popular benchmark datasets such as MegaFace Challenge, Youtube Faces (YTF), Labeled Face in the Wild (LFW) and IJB-C."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SDQ%3A_Stochastic_Differentiable_Quantization_with_Mixed_Precision/","title":"SDQ: Stochastic Differentiable Quantization with Mixed Precision","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SDQ%3A_Stochastic_Differentiable_Quantization_with_Mixed_Precision/#summary","title":"Summary","text":"<p>The paper discusses a Stochastic Differentiable Quantization method (SDQ) that can automatically learn the Mixed Precision Quantization (MPQ) strategy in a more flexible and globally optimized space with a smoother gradient approximation. The approach outperforms all state-of-the-art mixed or single precision quantization with a lower bitwidth, even better than the full-precision counterparts across various ResNet and MobileNet families."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SDQ%3A_Stochastic_Differentiable_Quantization_with_Mixed_Precision/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SDQ%3A_Stochastic_Differentiable_Quantization_with_Mixed_Precision/#content","title":"Content","text":"<p>In order to efficiently deploy deep models, model quantization approaches have been used. Mixed precision quantization (MPQ) has recently been researched, but previous studies mainly search the MPQ strategy in a costly scheme, or utilize partial prior knowledge for bitwidth assignment. This work presents a Stochastic Differentiable Quantization (SDQ) method that automatically learns the MPQ strategy in a more flexible and globally-optimized space with smoother gradient approximation. Extensive evaluations have shown that SDQ outperforms all state-of-the-art mixed or single precision quantization with a lower bitwidth and is even better than the full-precision counterparts across various ResNet and MobileNet families, demonstrating its effectiveness and superiority."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SPIQ%3A_Data-Free_Per-Channel_Static_Input_Quantization/","title":"SPIQ: Data-Free Per-Channel Static Input Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SPIQ%3A_Data-Free_Per-Channel_Static_Input_Quantization/#summary","title":"Summary","text":"<p> The paper proposes a per-channel input quantization scheme named SPIQ that allows for preserving cross-channel dynamics in static input quantization, leading to accuracy levels of dynamic methods. SPIQ achieves accuracies rivaling dynamic approaches with static-level inference speed and significantly outperforms state-of-the-art quantization methods on multiple computer vision problems."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SPIQ%3A_Data-Free_Per-Channel_Static_Input_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SPIQ%3A_Data-Free_Per-Channel_Static_Input_Quantization/#content","title":"Content","text":"<p> Computationally expensive neural networks are ubiquitous and solutions for efficient inference have drawn a growing attention in the machine learning community. In this work, we propose a per-channel input quantization scheme that allows for more finely preserving cross-channel dynamics in static input quantization, which can reach the accuracy levels of dynamic methods. We show through a thorough empirical evaluation on multiple computer vision problems that the proposed method, dubbed SPIQ, achieves accuracies rivaling dynamic approaches with static-level inference speed, significantly outperforming state-of-the-art quantization methods on every benchmark."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SQuant%3A_On-the-fly_data-free_quantization_via_diagonal_hessian_approximation/","title":"SQuant: On-the-fly data-free quantization via diagonal hessian approximation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SQuant%3A_On-the-fly_data-free_quantization_via_diagonal_hessian_approximation/#summary","title":"Summary","text":"<p>Summary: The paper introduces SQuant, an on-the-fly data-free quantization framework for deep neural networks (DNN). It employs Constrained Absolute Sum of Error (CASE) to minimize the data-free optimization objective in the discrete domain without the need for any datasets and the awareness of the network architecture. SQuant is an efficient algorithm with no backpropagation, which decreases the computation complexity of the objective solver. It achieves sub-second quantization time and over 30% accuracy improvement compared to existing data-free post-training quantization works, without synthetic datasets or fine-tuning. The framework is open-sourced at https://github.com/clevercool/SQuant.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SQuant%3A_On-the-fly_data-free_quantization_via_diagonal_hessian_approximation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SQuant%3A_On-the-fly_data-free_quantization_via_diagonal_hessian_approximation/#content","title":"Content","text":"<p> Quantization of deep neural networks (DNN) is a suitable technique for compressing and accelerating DNN models. Data-free quantization (DFQ) is preferred when privacy and confidentiality scenarios prohibit the use of original datasets. However, DFQ solutions currently available suffer from lower accuracy, require synthetic data to calibration networks and are expensive in terms of time and cost. This research introduces SQuant, an on-the-fly DFQ framework that quantizes networks on inference-only devices with low computation and memory needs. Through the analysis of the second-order information of DNN task loss, the paper decomposes and approximates the Hessian-based optimization objective into three diagonal sub-items. These sub-items correspond to the three dimensions of weight tensor, such as element-wise, kernel-wise, and output channel-wise. The proposed method, named Constrained Absolute Sum of Error (CASE), minimizes the data-free optimization objective in the discrete domain without the need for any dataset and the awareness of network architecture. The research proposes an efficient algorithm with no backpropagation that decreases the computation complexity of the objective solver. With no fine-tuning and synthetic datasets, the proposed framework accelerates data-free quantization by achieving sub-second levels of quantization time and over 30% accuracy improvement compared to existing data-free post-training quantization works. SQuant framework is open-sourced at https://github.com/clevercool/SQuant."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SWIS--Shared_Weight_bIt_Sparsity_for_Efficient_Neural_Network_Acceleration/","title":"SWIS--Shared Weight bIt Sparsity for Efficient Neural Network Acceleration","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SWIS--Shared_Weight_bIt_Sparsity_for_Efficient_Neural_Network_Acceleration/#summary","title":"Summary","text":"<p>Summary: The paper proposes a quantization framework called SWIS which achieves up to 54.3% point accuracy improvement compared to weight truncation. It achieves improved performance and storage compression through an offline weight decomposition and scheduling algorithm. It uses configurable, non-consecutive shift values on small groups of weights to significantly reduce the effective required bitwidth, achieving up to 3.7\u00d7 neural network weight compression compared to conventional quantization approaches. The proposed SWIS architecture achieves up to 6\u00d7 improvement in inference latency (energy) compared to state-of-the-art bit-serial accelerators of same size. The SWIS architecture reduces the number of memory weight to activation accesses by up to 10x in some convolutional layers by using different precision weights and activations and reducing precision more on the weight side.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SWIS--Shared_Weight_bIt_Sparsity_for_Efficient_Neural_Network_Acceleration/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SWIS--Shared_Weight_bIt_Sparsity_for_Efficient_Neural_Network_Acceleration/#content","title":"Content","text":"<p> Quantization is important for efficient neural network computing systems, but only a subset of applications can take advantage of such aggressive precision reduction. SWIS - Shared Weight bIt Sparsity is a quantization framework that achieves up to 54.3% point accuracy improvement compared to weight truncation. It delivers improved performance and storage compression via an offline weight decomposition and scheduling algorithm. With configurable, non-consecutive shift values on small groups of weights, SWIS can significantly reduce the effective required bitwidth. It achieves up to 3.7\u00d7neural network weight compression compared to conventional quantization approaches at similar inference accuracy. Additionally, the proposed SWIS architecture achieves up to 6\u00d7(1.8\u00d7) improvement in inference latency (energy) compared to state-of-the-art bit-serial accelerators of same size. Finally, SWIS architecture uses different precision weights and activations and reduces precision more on the weight side, reducing the number of memory weight to activation accesses by up to 10x in some convolutional layers."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SYMOG%3A_Learning_symmetric_mixture_of_Gaussian_modes_for_improved_fixed-point_quantization/","title":"SYMOG: Learning symmetric mixture of Gaussian modes for improved fixed-point quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SYMOG%3A_Learning_symmetric_mixture_of_Gaussian_modes_for_improved_fixed-point_quantization/#summary","title":"Summary","text":"<p>The paper proposes a new method called SYMOG for soft quantization to reduce the complexity of deep neural networks (DNNs) through low-bit fixed-point quantization. The proposed method involves changing the weight distribution from a unimodal Gaussian distribution to a symmetric mixture of Gaussians where each mean value belongs to a fixed-point mode. The approach was evaluated with various architectures on benchmark datasets and achieved better results than state-of-the-art quantization approaches. On CIFAR-10, it achieved an error rate of only 5.71%, and on CIFAR-100, it achieved an error rate of 27.65%."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SYMOG%3A_Learning_symmetric_mixture_of_Gaussian_modes_for_improved_fixed-point_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/SYMOG%3A_Learning_symmetric_mixture_of_Gaussian_modes_for_improved_fixed-point_quantization/#content","title":"Content","text":"<p>We propose SYMOG (symmetric mixture of Gaussian modes) as a novel soft quantization method for decreasing the complexity of deep neural networks (DNNs) through low-bit fixed-point quantization. During training, the weight distribution changes from an unimodal Gaussian distribution to a symmetric mixture of Gaussians where each mean value belongs to a particular fixed-point mode. We evaluate our approach with different architectures on benchmark datasets and achieve excellent results, outperforming state-of-the-art quantization approaches. We achieve an error rate of only 5.71% on CIFAR-10 and 27.65% on CIFAR-100."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Same%2C_same_but_different%3A_Recovering_neural_network_quantization_error_through_weight_factorization/","title":"Same, same but different: Recovering neural network quantization error through weight factorization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Same%2C_same_but_different%3A_Recovering_neural_network_quantization_error_through_weight_factorization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new method for improving the performance of quantized neural networks by utilizing a degree of freedom of scaling output channels. The approach significantly decreases the degradation caused by quantization and achieves state-of-the-art results for MobileNets. The authors also explore equivalent weight arrangements to improve network interpretability, network-pruning, and neural nets regularization.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Same%2C_same_but_different%3A_Recovering_neural_network_quantization_error_through_weight_factorization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Same%2C_same_but_different%3A_Recovering_neural_network_quantization_error_through_weight_factorization/#content","title":"Content","text":"<p> Quantization of neural networks is essential for implementing deep neural networks on embedded devices. In this paper, the authors propose a novel approach for improving the performance of a quantized network while utilizing an overlooked degree of freedom of scaling output channels. They present a simple and easy-to-implement method that significantly decreases the degradation caused by quantization and achieve state-of-the-art degradation results for MobileNets. Furthermore, the authors explore equivalent weight arrangements that make the network less sensitive to quantization to improve network interpretability, network-pruning, neural nets regularization, and other domains."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sampling-free_learning_of_Bayesian_quantized_neural_networks/","title":"Sampling-free learning of Bayesian quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sampling-free_learning_of_Bayesian_quantized_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes Bayesian Quantized Networks (BQNs) which are quantized neural networks with a posterior distribution over their discrete parameters. Algorithms for efficient learning and prediction in BQNs are provided which allows for differentiable learning in QNNs and reduces the variance in gradients. Evaluation on image classification datasets shows that BQNs achieve lower predictive errors and better-calibrated uncertainties compared to ensemble of QNNs.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sampling-free_learning_of_Bayesian_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sampling-free_learning_of_Bayesian_quantized_neural_networks/#content","title":"Content","text":"<p> Bayesian learning of model parameters in neural networks is important in scenarios where estimates with well-calibrated uncertainty are important. In this paper, we propose Bayesian quantized networks (BQNs), quantized neural networks (QNNs) for which we learn a posterior distribution over their discrete parameters. We provide a set of efficient algorithms for learning and prediction in BQNs without the need to sample from their parameters or activations, which not only allows for differentiable learning in QNNs, but also reduces the variance in gradients. We evaluate BQNs on MNIST, Fashion-MNIST, KMNIST and CIFAR10 image classification datasets. compared against bootstrap ensemble of QNNs (E-QNN). We demonstrate BQNs achieve both lower predictive errors and better-calibrated uncertainties than E-QNN (with less than 20% of the negative log-likelihood)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_image_retrieval_by_sparse_product_quantization/","title":"Scalable image retrieval by sparse product quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_image_retrieval_by_sparse_product_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new approach called Sparse Product Quantization (SPQ) to encode high-dimensional feature vectors into sparse representation for Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional feature indexing and retrieval. The proposed SPQ technique is able to compress data and obtain state-of-the-art results for ANN search on four public image datasets, validating the efficacy of the proposed method.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_image_retrieval_by_sparse_product_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_image_retrieval_by_sparse_product_quantization/#content","title":"Content","text":"<p> Fast Approximate Nearest Neighbor (ANN) search technique for high-dimensional feature indexing and retrieval is the crux of large-scale image retrieval. A recent promising technique is Product Quantization, which attempts to index high-dimensional image features by decomposing the feature space into a Cartesian product of low dimensional subspaces and quantizing each of them separately. In this paper, we propose a novel approach called Sparse Product Quantization (SPQ) to encoding the high-dimensional feature vectors into sparse representation. We optimize the sparse representations of the feature vectors by minimizing their quantization errors, making the resulting representation essentially close to the original data in practice. Experiments show that the proposed SPQ technique is not only able to compress data but also an effective encoding technique. We obtain state-of-the-art results for ANN search on four public image datasets, and the promising results of content-based image retrieval further validate the efficacy of our proposed method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_methods_for_8-bit_training_of_neural_networks/","title":"Scalable methods for 8-bit training of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_methods_for_8-bit_training_of_neural_networks/#summary","title":"Summary","text":"<p>The paper discusses quantized neural networks (QNNs) and their effectiveness in improving network efficiency during the inference phase. The authors use theoretical analysis to show that most of the training process is robust to substantial precision reduction, with only a few specific operations requiring higher precision. They then quantize the model parameters, activations, and layer gradients to 8-bit, leaving at a higher precision only the final step in the computation of the weight gradients. Additionally, they introduce Range Batch-Normalization (BN), which is more tolerant of quantization noise and has improved computational complexity. The authors demonstrate through simulations that Range BN is equivalent to traditional batch norm with precise scale adjustments, which can be approximated analytically. This work is the first to quantize weights, activations, and a significant volume of gradients stream in all layers, including batch normalization, to 8-bit while achieving state-of-the-art results on the ImageNet-1K dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_methods_for_8-bit_training_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_methods_for_8-bit_training_of_neural_networks/#content","title":"Content","text":"<p>Quantized Neural Networks (QNNs) are often used to improve network efficiency during the inference phase. Our theoretical analysis suggests that most of the training process is robust to substantial precision reduction, and points to only a few specific operations that require higher precision. Armed with this knowledge, we quantize the model parameters, activations, and layer gradients to 8-bit, leaving at a higher precision only the final step in the computation of the weight gradients. Additionally, we introduce Range Batch-Normalization (BN) which has significantly higher tolerance to quantization noise and improved computational complexity. Our simulations show that Range BN is equivalent to the traditional batch norm if a precise scale adjustment, which can be approximated analytically, is applied. To the best of the authors\u2019 knowledge, this work is the first to quantize the weights, activations, as well as a substantial volume of the gradients stream, in all layers (including batch normalization) to 8-bit while showing state-of-the-art results over the ImageNet-1K dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_verification_of_quantized_neural_networks_%28technical_report%29/","title":"Scalable verification of quantized neural networks (technical report)","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_verification_of_quantized_neural_networks_%28technical_report%29/#summary","title":"Summary","text":"<p>Summary: The article studies formal verification of neural networks, which is often done for idealized models over real arithmetic and fails to account for rounding imprecisions. The study explores verifying bit-exact implementation of quantized neural networks with bit vector specifications, which is a technique that trades numerical precision for computational efficiency, and shows that it is PSPACE-hard. The authors propose three techniques for making SMT-based verification of quantized neural networks more scalable, which can provide a speedup of up to three orders of magnitude over existing approaches.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_verification_of_quantized_neural_networks_%28technical_report%29/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scalable_verification_of_quantized_neural_networks_%28technical_report%29/#content","title":"Content","text":"<p>Formal verification of neural networks is an active topic of research, and recent advances have significantly increased the size of the networks that verification tools can handle. However, most methods are designed for verification of an idealized model of the actual network which works over real arithmetic and ignores rounding imprecisions. This idealization is in stark contrast to network quantization, which is a technique that trades numerical precision for computational efficiency and is, therefore, often applied in practice. Neglecting rounding errors of such low-bit quantized neural networks has been shown to lead to wrong conclusions about the network\u2019s correctness. Thus, the desired approach for verifying quantized neural networks would be one that takes these rounding errors into account. In this paper, we show that verifying the bit-exact implementation of quantized neural networks with bit-vector specifications is PSPACE-hard, even though verifying idealized real-valued networks and satisfiability of bit-vector specifications alone are each in NP. Furthermore, we explore several practical heuristics toward closing the complexity gap between idealized and bit-exact verification. In particular, we propose three techniques for making SMT-based verification of quantized neural networks more scalable. Our experiments demonstrate that our proposed methods allow a speedup of up to three orders of magnitude over existing approaches."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scientific_image_restoration_anywhere/","title":"Scientific image restoration anywhere","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scientific_image_restoration_anywhere/#summary","title":"Summary","text":"<p>Summary: The paper explores the performance and accuracy of a scientific image restoration model called TomoGAN on edge computing devices such as Google Edge TPU and NVIDIA Jetson. The results show that these devices can provide comparable accuracy to a full-fledged CPU or GPU model with faster inference response, making them suitable for low-latency inference in scientific experimental facilities.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scientific_image_restoration_anywhere/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Scientific_image_restoration_anywhere/#content","title":"Content","text":"<p>The use of deep learning models within scientific experimental facilities frequently requires low-latency inference. Edge computing devices can be useful in this context. We explore the performance and accuracy of a scientific image restoration model on edge computing devices by evaluating the deployments of TomoGAN on the Google Edge TPU and NVIDIA Jetson. We show that these edge computing devices can deliver accuracy comparable to that of a full-fledged CPU or GPU model, at speeds that are more than adequate for use in the intended deployments. Our experiments also show that the Edge TPU models can provide 3 times faster inference response than a CPU-based model and 1.5 times faster than an edge GPU-based model."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Search_what_you_want%3A_Barrier_panelty_nas_for_mixed_precision_quantization/","title":"Search what you want: Barrier panelty nas for mixed precision quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Search_what_you_want%3A_Barrier_panelty_nas_for_mixed_precision_quantization/#summary","title":"Summary","text":"<p>Summary: The paper presents a new method called Barrier Penalty based NAS (BP-NAS) for mixed precision quantization that uses Barrier Penalty and Prob-1 regularizer to ensure the models are within valid domain and training is reasonable. The proposed method outperforms other mixed precision methods in terms of detection and classification in achieving higher mAP with lower bit computation cost. The proposed method is highly useful in terms of Mixed Precision Quantization, NAS, Optimization Problem with Constraint and Soft Barrier Penalty.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Search_what_you_want%3A_Barrier_panelty_nas_for_mixed_precision_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Search_what_you_want%3A_Barrier_panelty_nas_for_mixed_precision_quantization/#content","title":"Content","text":"<p> In this paper, the authors propose a new method called Barrier Penalty based NAS (BP-NAS) for mixed precision quantization, which can find an optimal mixed precision model that meets specific constraints on model size and computation. The proposed method uses a soft Barrier Penalty that ensures all the models searched are within the valid domain and a differentiable Prob-1 regularizer to ensure reasonable learning. The authors also employ a distribution reshaping training strategy to make training more stable. The proposed BP-NAS outperforms other mixed precision methods in terms of detection and classification, achieving higher mAP and lower bit computation cost. Keywords include Mixed Precision Quantization, NAS, Optimization Problem with Constraint, Soft Barrier Penalty."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Searching_for_low-bit_weights_in_quantized_neural_networks/","title":"Searching for low-bit weights in quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Searching_for_low-bit_weights_in_quantized_neural_networks/#summary","title":"Summary","text":"<p>This paper presents a novel method for searching low-bit weights in quantized neural networks using a differential method to accurately search the discrete weights in an arbitrary quantized neural network. The proposed method represents each weight as a probability distribution optimized during training, resulting in better performance than state-of-the-art methods. The PyTorch code and MindSpore code are available online."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Searching_for_low-bit_weights_in_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Searching_for_low-bit_weights_in_quantized_neural_networks/#content","title":"Content","text":"<p>Quantized neural networks with low-bit weights and activations are attractive for developing AI accelerators. However, the quantization functions used in most conventional quantization methods are non-differentiable, which increases the optimization difficulty of quantized networks. This paper presents a novel method of searching for low-bit weights in quantized neural networks by regarding the discrete weights in an arbitrary quantized neural network as searchable variables and utilizing a differential method to search them accurately. Each weight is represented as a probability distribution over the discrete value set, and the probabilities are optimized during training. The proposed method is demonstrated to produce quantized neural networks with higher performance than state-of-the-art methods on both image classification and super-resolution tasks. The PyTorch code and MindSpore code will be made available at https://github.com/huawei-noah/Binary-Neural-Networks/tree/main/SLB and https://www.mindspore.cn/resources/hub, respectively."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Secure_quantized_training_for_deep_learning/","title":"Secure quantized training for deep learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Secure_quantized_training_for_deep_learning/#summary","title":"Summary","text":"<p> A secure multi-party computation (MPC) training of neural networks is implemented using quantization. The article reports the first successful MPC trained MNIST classifier to reach an accuracy of 99.2% in 3.5 hours with a 0.2% accuracy difference from the same network trained via plaintext computation. Additionally, the article presents novel protocols for exponentiation and inverse square root and tests the implementation in various MPC security models for up to ten parties, both with honest and dishonest majority as well as semi-honest and malicious security."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Secure_quantized_training_for_deep_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Secure_quantized_training_for_deep_learning/#content","title":"Content","text":"<p>We implement training of neural networks in secure multi-party computation (MPC) using quantization commonly used in said setting. We are the first to present an MNIST classifier purely trained in MPC that comes within 0.2 percent of the accuracy of the same convolutional neural network trained via plaintext computation. More concretely, we have trained a network with two convolutional and two dense layers to 99.2% accuracy in 3.5 hours (under one hour for 99% accuracy). We have also implemented AlexNet for CIFAR-10, which converges in a few hours. We develop novel protocols for exponentiation and inverse square root. Finally, we present experiments in a range of MPC security models for up to ten parties, both with honest and dishonest majority as well as semi-honest and malicious security."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Seernet%3A_Predicting_convolutional_neural_network_feature-map_sparsity_through_low-bit_quantization/","title":"Seernet: Predicting convolutional neural network feature-map sparsity through low-bit quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Seernet%3A_Predicting_convolutional_neural_network_feature-map_sparsity_through_low-bit_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposed a method to accelerate CNN inference by utilizing the sparsity of feature maps. The method involves obtaining a binary sparsity mask of the output feature maps through a highly quantized version of the original network, followed by a full-precision sparse convolution. The approach is applicable to general CNN networks without needing to train additional auxiliary networks, and incurs negligible accuracy drop compared to the original network.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Seernet%3A_Predicting_convolutional_neural_network_feature-map_sparsity_through_low-bit_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Seernet%3A_Predicting_convolutional_neural_network_feature-map_sparsity_through_low-bit_quantization/#content","title":"Content","text":"<p> In this paper, we present a novel and general method to accelerate convolutional neural network (CNN) inference by taking advantage of feature map sparsity. We experimentally demonstrate that a highly quantized version of the original network is sufficient in predicting the output sparsity accurately, and verify that leveraging such sparsity in inference incurs negligible accuracy drop compared with the original network. To accelerate inference, for each convolution layer, our approach first obtains a binary sparsity mask of the output feature maps by running inference on a quantized version of the original network layer and then conducts a full-precision sparse convolution to find out the precise values of the non-zero outputs. Compared with existing work, our approach avoids the overhead of training additional auxiliary networks, while is still applicable to general CNN networks without being limited to certain application domains."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-Supervised_Consistent_Quantization_for_Fully_Unsupervised_Image_Retrieval/","title":"Self-Supervised Consistent Quantization for Fully Unsupervised Image Retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-Supervised_Consistent_Quantization_for_Fully_Unsupervised_Image_Retrieval/#summary","title":"Summary","text":"<p> This paper proposes a self-supervised consistent quantization approach for deep fully unsupervised image retrieval that improves semantic structure information and codeword diversity regularization. The approach uses contrastive learning for embedding and quantized representations and fuses them for consistent regularization between instances, leading to superior results on three benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-Supervised_Consistent_Quantization_for_Fully_Unsupervised_Image_Retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-Supervised_Consistent_Quantization_for_Fully_Unsupervised_Image_Retrieval/#content","title":"Content","text":"<p> The paper proposes a novel self-supervised consistent quantization approach for deep fully unsupervised image retrieval. The approach consists of part consistent quantization and global consistent quantization, which focuses on semantic structure information and codeword diversity regularization. The authors introduce contrastive learning for both embedding and quantized representations and fuse them for consistent contrastive regularization between instances. With a uni\ufb01ed learning objective, the approach exploits richer self-supervision cues to facilitate model learning. The experiments on three benchmark datasets show the superiority of the approach over state-of-the-art methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-improving_Models_for_the_Intelligent_Digital_Twin%3A_Towards_Closing_the_Reality-to-Simulation_Gap/","title":"Self-improving Models for the Intelligent Digital Twin: Towards Closing the Reality-to-Simulation Gap","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-improving_Models_for_the_Intelligent_Digital_Twin%3A_Towards_Closing_the_Reality-to-Simulation_Gap/#summary","title":"Summary","text":"<p>The paper proposes a novel approach using reinforcement learning to improve Digital Twin models of autonomous mobile robots for modern Cyber-Physical Manufacturing Systems (CPMS). The approach closes the simulation gap through a three-step mechanism and uses reinforcement learning to find patterns in deviations between simulated and real data to learn compensation, resulting in promising results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-improving_Models_for_the_Intelligent_Digital_Twin%3A_Towards_Closing_the_Reality-to-Simulation_Gap/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-improving_Models_for_the_Intelligent_Digital_Twin%3A_Towards_Closing_the_Reality-to-Simulation_Gap/#content","title":"Content","text":"<p>This paper presents a novel approach to ensure the quality of the Digital Twin models that modern Cyber-Physical Manufacturing Systems (CPMS) rely on. Based on a reinforcement learning-based methodology, autonomous mobile robots are used as an example to show how the Digital Twin automatically improves models that do not perfectly represent the physical asset, making it an intelligent Digital Twin. The presented approach closes the simulation gap through a three-step mechanism that makes the simulated data and the real data comparable and synchronizes it, applies reinforcement learning to find patterns in the deviations between the simulated and real data, and learns to compensate for them. The evaluation of this example shows promising results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-supervised_product_quantization_for_deep_unsupervised_image_retrieval/","title":"Self-supervised product quantization for deep unsupervised image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-supervised_product_quantization_for_deep_unsupervised_image_retrieval/#summary","title":"Summary","text":"<p>Summary: The paper introduces Self-supervised Product Quantization (SPQ), a deep unsupervised image retrieval method that does not rely on precise and error-free label annotations for large training datasets. SPQ uses a Cross Quantized Contrastive learning strategy to learn codewords and deep visual descriptors, leading to accurate retrieval without supervised pre-training. The experimental results show that SPQ achieves state-of-the-art performance.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-supervised_product_quantization_for_deep_unsupervised_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Self-supervised_product_quantization_for_deep_unsupervised_image_retrieval/#content","title":"Content","text":"<p>  Supervised deep learning-based hash and vector quantization methods for image retrieval require precise and error-free label annotations for large training datasets. To overcome this challenge, we propose Self-supervised Product Quantization (SPQ), the first deep unsupervised image retrieval method. The SPQ network is trained in a self-supervised manner and enables us to analyze image contents to achieve accurate retrieval. We use a Cross Quantized Contrastive learning strategy to learn codewords and deep visual descriptors, which enhances retrieval performance without supervised pre-training. Extensive experiments reveal that SPQ can deliver state-of-the-art results without supervised pre-training."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semantic-Guided_Hashing_for_Cross-Modal_Retrieval/","title":"Semantic-Guided Hashing for Cross-Modal Retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semantic-Guided_Hashing_for_Cross-Modal_Retrieval/#summary","title":"Summary","text":"<p>The paper proposes a two-step supervised cross-modal hashing approach named Semantic-Guided Hashing (SeGH) which addresses the problem of neglecting valuable semantic correlation among different classes in most existing supervised cross-modal hashing approaches. It takes the encoder-decoder paradigm based on label semantics obtained by the word vector of class names to learn the discriminative projection from original feature space to common semantic space in Step 1, and in Step 2, semantic representations of different modalities in the common space are projected into a Hamming space while preserving the intra-modality and inter-modality similarity. Results from experiments on two datasets show its superiority for cross-modal retrieval and effectiveness for zero-shot cross-modal retrieval against several state-of-the-art baselines."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semantic-Guided_Hashing_for_Cross-Modal_Retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semantic-Guided_Hashing_for_Cross-Modal_Retrieval/#content","title":"Content","text":"<p>In the Big Data era, cross-modal retrieval is a significant issue for retrieving heterogeneous or multimodal data. Cross-modal hashing has received considerable attention for effectively solving this issue by encoding high-dimensional data into compact binary codes. However, most existing supervised cross-modal hashing approaches only consider capturing the semantic information from original features to latent common semantic space, resulting in neglect of valuable semantic correlation among different classes. To address this problem, we propose a novel two-step supervised cross-modal hashing approach, termed Semantic-Guided Hashing (SeGH), which takes the encoder-decoder paradigm based on label semantics obtained by the word vector of class names to learn the discriminative projection from original feature space to common semantic space. In Step 2, semantic representations of different modalities in the common space are projected into a Hamming space while preserving the intra-modality and inter-modality similarity. The proposed SeGH has been compared against several state-of-the-art baselines on two datasets, and it showcases its superiority for cross-modal retrieval, as well as the effectiveness for zero-shot cross-modal retrieval with extensive experiments."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semantically_tied_paired_cycle_consistency_for_zero-shot_sketch-based_image_retrieval/","title":"Semantically tied paired cycle consistency for zero-shot sketch-based image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semantically_tied_paired_cycle_consistency_for_zero-shot_sketch-based_image_retrieval/#summary","title":"Summary","text":"<p> The paper proposes a SEM-PCYC model for zero-shot SBIR, which trains two branches to map visual information to a common semantic space. The model achieves state-of-the-art results on the Sketchy and TU-Berlin datasets, without requiring highly-priced aligned sketch-image pairs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semantically_tied_paired_cycle_consistency_for_zero-shot_sketch-based_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semantically_tied_paired_cycle_consistency_for_zero-shot_sketch-based_image_retrieval/#content","title":"Content","text":"<p>Zero-shot sketch-based image retrieval (SBIR) is an emerging task in computer vision, allowing to retrieve natural images relevant to sketch queries that might not been seen in the training phase. In this work, we propose a semantically aligned paired cycle-consistent generative (SEM-PCYC) model for zero-shot SBIR, where each branch maps the visual information to a common semantic space via an adversarial training. Each of these branches maintains a cycle consistency that only requires supervision at category levels, and avoids the need of highly-priced aligned sketch-image pairs. Our results demonstrate a significant boost in zero-shot SBIR performance over the state-of-the-art on the challenging Sketchy and TU-Berlin datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semi-Parametric_Neural_Image_Synthesis/","title":"Semi-Parametric Neural Image Synthesis","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semi-Parametric_Neural_Image_Synthesis/#summary","title":"Summary","text":"<p>Summary: The paper proposes a semi-parametric approach to generative image synthesis by combining diffusion or autoregressive models with a separate image database and retrieval strategy. The model is conditioned on nearest neighbors retrieved from the external database during training, providing content information, and focused on learning scene composition. The approach transfers a trained model to a novel domain by replacing the database for one with different contents without requiring paired text-image data. The method reduces the parameter count of the generative model and outperforms the state-of-the-art with negligible memory and computational overhead for the external database and retrieval.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semi-Parametric_Neural_Image_Synthesis/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Semi-Parametric_Neural_Image_Synthesis/#content","title":"Content","text":"<p>Our work proposes a semi-parametric approach to generative image synthesis, complementing diffusion or autoregressive models with a separate image database and retrieval strategy. During training, the generative model is conditioned on nearest neighbors retrieved from the external database, providing content information, while focusing on learning scene composition. Our approach transfers a trained model to a novel domain by swapping the database for one with different contents, performing competitively on tasks that the model was not trained on, such as class-conditional synthesis, zero-shot stylization or text-to-image synthesis, without requiring paired text-image data. Our method significantly reduces the parameter count of the generative model and outperforms the state-of-the-art, with negligible memory and computational overhead for the external database and retrieval."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sgquant%3A_Squeezing_the_last_bit_on_graph_neural_networks_with_specialized_quantization/","title":"Sgquant: Squeezing the last bit on graph neural networks with specialized quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sgquant%3A_Squeezing_the_last_bit_on_graph_neural_networks_with_specialized_quantization/#summary","title":"Summary","text":"<p> The paper proposes a specialized Graph Neural Network (GNN) quantization scheme called SGQuant to systematically reduce the GNN memory consumption. SGQuant can effectively reduce the memory footprint while limiting the accuracy drop to 0.4% on average."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sgquant%3A_Squeezing_the_last_bit_on_graph_neural_networks_with_specialized_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sgquant%3A_Squeezing_the_last_bit_on_graph_neural_networks_with_specialized_quantization/#content","title":"Content","text":"<p>With the increasing popularity of graph-based learning, Graph Neural Networks (GNNs) win lots of attention from research and industry field because of their high accuracy. However, existing GNNs suffer from high memory footprints (e.g., node embedding features). To this end, we propose a specialized GNN quantization scheme, SGQuant, to systematically reduce the GNN memory consumption. Intensive experiments show that SGQuant can effectively reduce the memory footprint from 4.25 to 31.9 compared with the original full-precision GNNs while limiting the accuracy drop to 0.4% on average."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Shared_predictive_cross-modal_deep_quantization/","title":"Shared predictive cross-modal deep quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Shared_predictive_cross-modal_deep_quantization/#summary","title":"Summary","text":"<p> The paper presents a deep quantization approach called shared predictive deep quantization (SPDQ) to improve cross-modal similarity search. The SPDQ method learns shared subspace and private subspaces for individual modalities, representations in the shared subspace and the private subspaces by embedding them to a reproducing kernel Hilbert space, and a quantizer in the shared subspace to produce compact codes with the help of label alignment. Experiment results on two popular benchmarks show that SPDQ outperforms existing methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Shared_predictive_cross-modal_deep_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Shared_predictive_cross-modal_deep_quantization/#content","title":"Content","text":"<p> This paper presents a solution for cross-modal similarity search in the form of a deep compact code learning approach. The proposed method, called shared predictive deep quantization (SPDQ), is a deep quantization approach that leverages deep neural networks into quantization based cross-modal similarity search. The SPDQ method simultaneously learns shared subspace across different modalities and two private subspaces for individual modalities, representations in the shared subspace and the private subspaces by embedding them to a reproducing kernel Hilbert space, which allows for explicit comparison of the mean embedding of different modality distributions. A quantizer is also learned in the shared subspace to produce semantics preserving compact codes with the help of label alignment. Experiments on two popular benchmarks show that the approach outperforms state-of-the-art methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sharpness-aware_Quantization_for_Deep_Neural_Networks/","title":"Sharpness-aware Quantization for Deep Neural Networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sharpness-aware_Quantization_for_Deep_Neural_Networks/#summary","title":"Summary","text":"<p>Summary: The authors propose a new method called Sharpness-Aware Quantization (SAQ) that combines Sharpness-Aware Minimization (SAM) and quantization for model compression. SAQ shows state-of-the-art results in uniform quantization for convolutional neural networks and Transformers on various datasets, outperforming previous methods, including AdamW.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sharpness-aware_Quantization_for_Deep_Neural_Networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sharpness-aware_Quantization_for_Deep_Neural_Networks/#content","title":"Content","text":"<p> In this paper, the authors propose a novel method called Sharpness-Aware Quantization (SAQ) to explore the effect of Sharpness-Aware Minimization (SAM) in model compression, particularly quantization for the first time. SAQ formulates a unified view of quantization and SAM by treating them as introducing quantization noises and adversarial perturbations to the model weights, respectively. Using an efficient training strategy, SAQ incurs little additional training overhead compared with the default optimizer. Extensive experiments on both convolutional neural networks and Transformers across various datasets show that SAQ improves the generalization performance of the quantized models, yielding state-of-the-art results in uniform quantization. The authors show that on ImageNet, SAQ outperforms AdamW by 1.2% on the Top-1 accuracy for a 4-bit ViT-B/16, and their 4-bit ResNet-50 surpasses the previous state-of-the-art method by 0.9% on the Top-1 accuracy."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Shifting_capsule_networks_from_the_cloud_to_the_deep_edge/","title":"Shifting capsule networks from the cloud to the deep edge","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Shifting_capsule_networks_from_the_cloud_to_the_deep_edge/#summary","title":"Summary","text":"<p>The paper presents an API and a framework to execute quantized Capsule Networks in resource-constrained devices, such as microcontroller units. CapsNets are complex to deploy as they require large memory footprints, thus leading to challenges in adoption at the edge. Results showed a reduction in memory footprint of almost 75%, with accuracy loss ranging from 0.07% to 0.18%. The Arm Cortex-M API enables primary capsule and capsule layers with medium-sized kernels to run in just 119.94 and 90.60 milliseconds, respectively, and for the GAP-8 SoC (RISC-V RV32IMCXpulp @ 170 MHz), latency drops to 7.02 and 38.03 ms, respectively."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Shifting_capsule_networks_from_the_cloud_to_the_deep_edge/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Shifting_capsule_networks_from_the_cloud_to_the_deep_edge/#content","title":"Content","text":"<p> Capsule networks (CapsNets) are an emerging trend in image processing. In contrast to a convolutional neural network, CapsNets are not vulnerable to object deformation, as the relative spatial information of the objects is preserved across the network. However, their complexity is mainly related to the capsule structure and the dynamic routing mechanism, which makes it almost unreasonable to deploy a CapsNet, in its original form, in a resource-constrained device powered by a small microcontroller (MCU). In an era where intelligence is rapidly shifting from the cloud to the edge, this high complexity imposes serious challenges to the adoption of CapsNets at the very edge. To tackle this issue, we present an API for the execution of quantized CapsNets in Arm Cortex-M and RISC-V MCUs. Our software kernels extend the Arm CMSIS-NN and RISC-V PULP-NN to support capsule operations with 8-bit integers as operands. Along with it, we propose a framework to perform post-training quantization of a CapsNet. Results show a reduction in memory footprint of almost 75%, with accuracy loss ranging from 0.07% to 0.18%. In terms of throughput, our Arm Cortex-M API enables the execution of primary capsule and capsule layers with medium-sized kernels in just 119.94 and 90.60 milliseconds (ms), respectively (STM32H755ZIT6U, Cortex-M7 @ 480 MHz). For the GAP-8 SoC (RISC-V RV32IMCXpulp @ 170 MHz), the latency drops to 7.02 and 38.03 ms, respectively. Keywords capsule networks, capsule network quantization, edge, cloud, CMSIS-NN, PULP-NN."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sigma_delta_quantized_networks/","title":"Sigma delta quantized networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sigma_delta_quantized_networks/#summary","title":"Summary","text":"<p> The paper introduces Sigma-Delta networks as a solution to the fixed computation waste in video processing by sending only discretized changes in activation from one layer to the next. An optimization approach is presented to convert pre-trained deep networks into Sigma-Delta networks for optimal efficiency, reducing computational costs by an order of magnitude when processing video data."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sigma_delta_quantized_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sigma_delta_quantized_networks/#content","title":"Content","text":"<p>Deep neural networks can be obscenely wasteful. When processing video, a convolutional network expends a fixed amount of computation for each frame with no regard to the similarity between neighbouring frames. To put an end to such waste, we introduce Sigma- Delta networks. With each new input, each layer in this network sends a discretized form of its change in activation to the next layer. Thus the amount of computation that the network does scales with the amount of change in the input and layer activations, rather than the size of the network. We introduce an optimization method for converting any pre-trained deep network into an optimally efficient Sigma-Delta network, and show that our algorithm, if run on the appropriate hardware, could cut at least an order of magnitude from the computational cost of processing video data."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Similarity_preserving_deep_asymmetric_quantization_for_image_retrieval/","title":"Similarity preserving deep asymmetric quantization for image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Similarity_preserving_deep_asymmetric_quantization_for_image_retrieval/#summary","title":"Summary","text":"<p> The paper proposes a deep learning model called SPDAQ for quantization that can learn compact binary codes and quantization codebooks for all items in a database in a time-efficient manner. SPDAQ uses an image subset and label information to preserve label similarity and an efficient optimization algorithm for the learning process. The model is experimentally shown to be superior to existing models on four benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Similarity_preserving_deep_asymmetric_quantization_for_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Similarity_preserving_deep_asymmetric_quantization_for_image_retrieval/#content","title":"Content","text":"<p> Quantization is widely used for large-scale multimedia retrieval due to its effectiveness in coding high-dimensional data. However, training deep quantization models on a large-scale database is highly time-consuming, and existing models often sample only a subset for training, leading to unsatisfactory performance. To address this problem, we propose a novel model called Similarity Preserving Deep Asymmetric Quantization (SPDAQ) which can learn compact binary codes and quantization codebooks for all items in the database. SPDAQ uses an image subset and the label information of all database items to map them to different but correlated distributions, preserving label similarity. An efficient optimization algorithm is proposed for the learning process. Extensive experiments demonstrate the superiority of our proposed model on four widely-used benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simulated_Quantization%2C_Real_Power_Savings/","title":"Simulated Quantization, Real Power Savings","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simulated_Quantization%2C_Real_Power_Savings/#summary","title":"Summary","text":"<p>This paper proposes reducing power consumption in neural network inference by using consecutive 0 bits from the multipliers to simulate low bit-width quantization on higher bit-width hardware. They show that simulating 4-bit quantization on 8-bit hardware can result in up to 17% relative reduction in power consumption. The use of bit operations (BOPs) as a proxy for power efficiency is also discussed, and learning mixed-precision configurations targeting lower BOPs can achieve better trade-offs between accuracy and power efficiency."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simulated_Quantization%2C_Real_Power_Savings/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simulated_Quantization%2C_Real_Power_Savings/#content","title":"Content","text":"<p>Reduced precision hardware-based matrix multiplication accelerators have been widely used to reduce power consumption in neural network inference. In this paper, we show that the effect of consecutive 0 bits from the multipliers can be used to further reduce power consumption by simulating low bit-width quantization on higher bit-width hardware. We demonstrate that simulating 4-bit quantization on 8-bit hardware can result in up to 17% relative reduction in power consumption on commonly used networks. We also show that bit operations (BOPs) can be used as a proxy for power efficiency, and learning mixed-precision configurations targeting lower BOPs can achieve better trade-offs between accuracy and power efficiency."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simultaneous_similarity-based_self-distillation_for_deep_metric_learning/","title":"Simultaneous similarity-based self-distillation for deep metric learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simultaneous_similarity-based_self-distillation_for_deep_metric_learning/#summary","title":"Summary","text":"<p> The paper proposes Simultaneous Similarity-based Self-Distillation (S2SD) as a method to improve deep metric learning (DML) for visual similarity and zero-shot applications. S2SD extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining test-time cost and with negligible changes to the training time. Experiments and ablations across different objectives and standard benchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while also setting a new state-of-the-art."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simultaneous_similarity-based_self-distillation_for_deep_metric_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simultaneous_similarity-based_self-distillation_for_deep_metric_learning/#content","title":"Content","text":"<p> This research paper proposes Simultaneous Similarity-based Self-Distillation (S2SD) as a method to improve deep metric learning (DML) for visual similarity and zero-shot applications. S2SD extends DML with knowledge distillation from auxiliary, high-dimensional embedding and feature spaces to leverage complementary context during training while retaining test-time cost and with negligible changes to the training time. Experiments and ablations across different objectives and standard benchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while also setting a new state-of-the-art."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simultaneously_optimizing_weight_and_quantizer_of_ternary_neural_network_using_truncated_gaussian_approximation/","title":"Simultaneously optimizing weight and quantizer of ternary neural network using truncated gaussian approximation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simultaneously_optimizing_weight_and_quantizer_of_ternary_neural_network_using_truncated_gaussian_approximation/#summary","title":"Summary","text":"<p>Summary: The paper introduces a ternarized neural network training method to optimize both weights and quantizer during training, thereby reducing model size and computational cost. The authors incorporate the threshold of weight ternarization using truncated Gaussian approximation and optimize it through back-propagation, achieving a 3.9/2.52/2.16% accuracy degradation on the ImageNet classification task using ternarized ResNet-18/34/50 with the first and last layer ternarized.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simultaneously_optimizing_weight_and_quantizer_of_ternary_neural_network_using_truncated_gaussian_approximation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Simultaneously_optimizing_weight_and_quantizer_of_ternary_neural_network_using_truncated_gaussian_approximation/#content","title":"Content","text":"<p>In this paper, the authors propose a novel ternarized neural network training method that simultaneously optimizes both weights and quantizer during training, enabling a significant reduction in model size and computational cost. They are the first to incorporate the thresholds of weight ternarization into a closed-form expression using truncated Gaussian approximation, which can be optimized through back-propagation together with network's other parameters through the end-to-end training. With both the first and last layer ternarized, the experiments on the ImageNet classification task show that their ternarized ResNet-18/34/50 only has \u223c3.9/2.52/2.16% accuracy degradation in comparison to the full-precision counterparts."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sketchmate%3A_Deep_hashing_for_million-scale_human_sketch_retrieval/","title":"Sketchmate: Deep hashing for million-scale human sketch retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sketchmate%3A_Deep_hashing_for_million-scale_human_sketch_retrieval/#summary","title":"Summary","text":"<p>Summary: The paper proposes a deep hashing framework for sketch retrieval that utilizes a multi-million scale human sketch dataset. It introduces the problem of sketch hashing retrieval which requires fine-grained sketch feature learning and a compact binary code for efficient retrieval. The proposed two-branch CNN-RNN architecture explores the temporal ordering of strokes and a novel hashing loss is designed to accommodate both temporal and abstract traits of sketches. Their network offers the best retrieval and generalization performance, demonstrating the benefit of their sketch-specific design.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sketchmate%3A_Deep_hashing_for_million-scale_human_sketch_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sketchmate%3A_Deep_hashing_for_million-scale_human_sketch_retrieval/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Soft-to-hard_vector_quantization_for_end-to-end_learning_compressible_representations/","title":"Soft-to-hard vector quantization for end-to-end learning compressible representations","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Soft-to-hard_vector_quantization_for_end-to-end_learning_compressible_representations/#summary","title":"Summary","text":"<p> The paper presents a new approach to deep learning model compression using a soft-to-hard quantization method that shows competitive results for both image and neural network compression tasks. The approach involves continuous relaxation of quantization and entropy, which gradually transitions to their discrete counterparts throughout the training process."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Soft-to-hard_vector_quantization_for_end-to-end_learning_compressible_representations/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Soft-to-hard_vector_quantization_for_end-to-end_learning_compressible_representations/#content","title":"Content","text":"<p>We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Spatio-temporal_pruning_and_quantization_for_low-latency_spiking_neural_networks/","title":"Spatio-temporal pruning and quantization for low-latency spiking neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Spatio-temporal_pruning_and_quantization_for_low-latency_spiking_neural_networks/#summary","title":"Summary","text":"<p>The paper introduces spatio-temporal pruning and quantization as techniques for enhancing the efficiency of Spiking Neural Networks (SNNs). The proposed method uses structured spatial pruning and gradual reduction of timesteps in temporal pruning. The method achieves up to 10-14X model size compression and 3-30X reduced latency compared to other SNNs while maintaining a high level of accuracy. The paper also finds that post-training weight quantization with up to 5-bit quantization does not significantly affect network performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Spatio-temporal_pruning_and_quantization_for_low-latency_spiking_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Spatio-temporal_pruning_and_quantization_for_low-latency_spiking_neural_networks/#content","title":"Content","text":"<p>Spiking Neural Networks (SNNs) present a promising alternative to traditional deep learning methods due to their computationally efficient sparse event-driven information processing. However, SNNs suffer from high inference latency, especially for real-time applications. This paper proposes spatio-temporal pruning and quantization for SNNs to enhance their efficiency. Structured spatial pruning is performed by identifying layer-wise significant dimensions using principal component analysis, which leads to 10-14X model size compression. Additionally, gradual reduction of timesteps in temporal pruning reduces the latency of inference. The proposed spatio-temporally pruned SNNs perform with 89.04% and 66.4% accuracy on CIFAR10 and CIFAR100, respectively, while achieving 3-30X reduced latency compared to other state-of-the-art SNNs. Furthermore, post-training weight quantization does not significantly affect network performance for up to 5-bit quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sqwa%3A_Stochastic_quantized_weight_averaging_for_improving_the_generalization_capability_of_low-precision_deep_neural_networks/","title":"Sqwa: Stochastic quantized weight averaging for improving the generalization capability of low-precision deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sqwa%3A_Stochastic_quantized_weight_averaging_for_improving_the_generalization_capability_of_low-precision_deep_neural_networks/#summary","title":"Summary","text":"<p>The paper introduces a new approach called SQWA for optimizing low-precision DNNs using model averaging to achieve good generalization capability. The approach includes floating-point model training, direct quantization of weights, capturing multiple low-precision models during retraining, averaging the captured models, and re-quantizing the averaged model and fine-tuning it. Results demonstrate state-of-the-art performance for 2-bit QDNNs on CIFAR-100 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sqwa%3A_Stochastic_quantized_weight_averaging_for_improving_the_generalization_capability_of_low-precision_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Sqwa%3A_Stochastic_quantized_weight_averaging_for_improving_the_generalization_capability_of_low-precision_deep_neural_networks/#content","title":"Content","text":"<p>We present SQWA, a new approach to optimize low-precision DNNs using model averaging to achieve good generalization capability. It includes (1) floating-point model training, (2) direct quantization of weights, (3) capturing multiple low-precision models during retraining, (4) averaging the captured models, and (5) re-quantizing the averaged model and fine-tuning it. Visualization results show that a quantized DNN optimized with SQWA is located near the center of the flat minimum in the loss surface, and state-of-the-art results were achieved for 2-bit QDNNs on CIFAR-100 and ImageNet datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Standard_Deviation-Based_Quantization_for_Deep_Neural_Networks/","title":"Standard Deviation-Based Quantization for Deep Neural Networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Standard_Deviation-Based_Quantization_for_Deep_Neural_Networks/#summary","title":"Summary","text":"<p> The paper proposes a novel framework to quantize deep neural networks using the standard deviation of the network's weight and activation distributions. A logarithmic quantization scheme is also proposed to quantize weights to power-of-two discrete values. The method outperforms existing work on CIFAR10 and ImageNet datasets and achieves better accuracy performance with 3-bit weights and activations when compared to full-precision models. The proposed scheme simultaneously prunes the network's parameters and allows for flexible adjustment of the pruning ratio during the quantization process."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Standard_Deviation-Based_Quantization_for_Deep_Neural_Networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Standard_Deviation-Based_Quantization_for_Deep_Neural_Networks/#content","title":"Content","text":"<p>Quantization of deep neural networks is a promising approach to reduce the inference cost for resource-restricted devices. In this paper, the authors propose a new framework for quantizing intervals (discrete values) using the standard deviation of the network's weight and activation distributions. They also propose a novel base-2 logarithmic quantization scheme to quantize weights to power-of-two discrete values. According to their evaluations, their method outperforms existing work on CIFAR10 and ImageNet datasets and even achieves better accuracy performance with 3-bit weights and activations when compared to full-precision models. Their proposed scheme simultaneously prunes the network's parameters and allows for flexible adjustment of the pruning ratio during the quantization process."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Stochastic-shield%3A_A_probabilistic_approach_towards_training-free_adversarial_defense_in_quantized_cnns/","title":"Stochastic-shield: A probabilistic approach towards training-free adversarial defense in quantized cnns","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Stochastic-shield%3A_A_probabilistic_approach_towards_training-free_adversarial_defense_in_quantized_cnns/#summary","title":"Summary","text":"<p> The paper investigates how quantized neural networks are vulnerable to adversarial attacks and proposes a modular defense mechanism called Stochastic-Shield, which includes an input filtering layer and a Monte Carlo dropout approach, that can improve the efficiency and robustness of these models. The authors suggest that Stochastic-Shield can provide a feasible solution to real-world scenarios against attacks of varying strengths without retraining or fine-tuning."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Stochastic-shield%3A_A_probabilistic_approach_towards_training-free_adversarial_defense_in_quantized_cnns/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Stochastic-shield%3A_A_probabilistic_approach_towards_training-free_adversarial_defense_in_quantized_cnns/#content","title":"Content","text":"<p> In this research paper, the authors investigate the vulnerability of quantized neural networks to adversarial attacks and propose a probabilistic framework called Stochastic-Shield as a defense mechanism. The framework includes an input filtering layer and a Monte Carlo dropout approach. The authors show that this modular defense mechanism can improve the efficiency and robustness of quantized deep learning models without the need for retraining or fine-tuning. They also highlight the importance of providing robustness and adversarial defense to real-world scenarios against attacks of varying strengths. The authors propose Stochastic-Shield as a feasible solution to this challenge."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Stochastic_precision_ensemble%3A_self-knowledge_distillation_for_quantized_deep_neural_networks/","title":"Stochastic precision ensemble: self-knowledge distillation for quantized deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Stochastic_precision_ensemble%3A_self-knowledge_distillation_for_quantized_deep_neural_networks/#summary","title":"Summary","text":"<p> <p>The paper introduces a new training scheme for quantized deep neural networks (QDNNs) called Stochastic Precision Ensemble Training (SPEQ). The proposed scheme uses knowledge distillation (KD) with a teacher network formed by sharing model parameters of the student network. The soft labels of the teacher network are obtained through random bit-precision assignments made stochastically at each layer's forward-pass computation. The student model then learns with these soft labels to reduce activation quantization noise, and cosine similarity loss is employed instead of KL-divergence during KD training. The proposed method outperforms existing quantization training approaches for various tasks such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Stochastic_precision_ensemble%3A_self-knowledge_distillation_for_quantized_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Stochastic_precision_ensemble%3A_self-knowledge_distillation_for_quantized_deep_neural_networks/#content","title":"Content","text":"<p>Abstract: The quantization of deep neural networks (QDNNs) has been actively studied for deployment in edge devices. Recent studies employ the knowledge distillation (KD) method to improve the performance of quantized networks. In this study, we propose stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is a knowledge distillation training scheme; however, the teacher is formed by sharing the model parameters of the student network. We obtain the soft labels of the teacher by randomly changing the bit precision of the activation stochastically at each layer of the forward-pass computation. The student model is trained with these soft labels to reduce the activation quantization noise. The cosine similarity loss is employed, instead of the KL-divergence, for KD training. As the teacher model changes continuously by random bit-precision assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms the existing quantization training methods in various tasks, such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Streaming_architecture_for_large-scale_quantized_neural_networks_on_an_FPGA-based_dataflow_platform/","title":"Streaming architecture for large-scale quantized neural networks on an FPGA-based dataflow platform","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Streaming_architecture_for_large-scale_quantized_neural_networks_on_an_FPGA-based_dataflow_platform/#summary","title":"Summary","text":"<p> This paper introduces a new streaming architecture for running quantized NNs on FPGAs that supports skip connections used in state-of-the-art NNs. This architecture allows for the implementation of an 18-layer ResNet for 224x224 image classification achieving 57.5% top-1 accuracy and a full-sized quantized AlexNet with 2-bit activations instead of 1-bit, improving accuracy from 41.8% to 51.03% for ImageNet classification. The ResNet-18 implementation consumes 5 times less power and is 4 times slower for ImageNet than the same NN on the latest Nvidia GPUs. Smaller Nets that fit a single FPGA run faster than GPUs on inputs of up to 32x32 while consuming up to 20 times less energy and power."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Streaming_architecture_for_large-scale_quantized_neural_networks_on_an_FPGA-based_dataflow_platform/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Streaming_architecture_for_large-scale_quantized_neural_networks_on_an_FPGA-based_dataflow_platform/#content","title":"Content","text":"<p>Deep neural networks (DNNs) are widely used in various applications, but their huge footprint and high computational and communication needs put a strain on resources. Low-precision representations of weights and other parameters have been found to achieve similar accuracy while requiring fewer resources. Quantized values enable the use of field-programmable gate arrays (FPGAs) to run neural networks (NNs), as they are well-suited to bitwise operations and arbitrary-precision representation of numbers. This paper presents a new streaming architecture for running quantized NNs on FPGAs, with support for skip connections used in state-of-the-art NNs. The architecture allows for the implementation of an 18-layer ResNet for 224x224 image classification achieving 57.5% top-1 accuracy and a full-sized quantized AlexNet with 2-bit activations instead of 1-bit, improving accuracy from 41.8% to 51.03% for ImageNet classification. The ResNet-18 implementation consumes 5 times less power and is 4 times slower for ImageNet than the same NN on the latest Nvidia GPUs. Smaller Nets that fit a single FPGA run faster than GPUs on inputs of up to 32x32 while consuming up to 20 times less energy and power."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Streamlined_deployment_for_quantized_neural_networks/","title":"Streamlined deployment for quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Streamlined_deployment_for_quantized_neural_networks/#summary","title":"Summary","text":"<p>  This paper discusses the challenges of running Deep Neural Network (DNN) models on devices with limited computational capability and how Quantized Neural Networks (QNNs) can solve these challenges. The paper provides a streamlining flow to convert all QNN inference operations to integer ones and techniques based on processing one bit position at a time to show how QNNs can be efficiently deployed using common bitwise operations. The paper demonstrates the potential of QNNs on mobile CPUs with microbenchmarks and on a quantized AlexNet, which is 3.5\u00d7 faster than an optimized 8-bit baseline."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Streamlined_deployment_for_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Streamlined_deployment_for_quantized_neural_networks/#content","title":"Content","text":"<p>Running Deep Neural Network (DNN) models on devices with limited computational capability is a challenge due to large compute and memory requirements. Quantized Neural Networks (QNNs) have emerged as a potential solution to this problem, promising to offer most of the DNN accuracy benefits with much lower computational cost. In this work, we first describe a streamlining flow to convert all QNN inference operations to integer ones. Afterwards, we provide techniques based on processing one bit position at a time (bit-serial) to show how QNNs can be efficiently deployed using common bitwise operations. We demonstrate the potential of QNNs on mobile CPUs with microbenchmarks and on a quantized AlexNet, which is 3.5\u00d7faster than an optimized 8-bit baseline. Our bit-serial matrix multiplication library is available on GitHub."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Strict_deformation_quantization_of_the_state_space_of__with_applications_to_the_Curie%E2%80%93Weiss_model/","title":"Strict deformation quantization of the state space of  with applications to the Curie\u2013Weiss model","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Strict_deformation_quantization_of_the_state_space_of__with_applications_to_the_Curie%E2%80%93Weiss_model/#summary","title":"Summary","text":"<p>The text discusses the continuous bundle of C-algebras over a given set with fibers including N=Mk(C) N and C(Xk). It further proposes the idea of strict deformation quantization of a compact Poisson manifold, Xk."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Strict_deformation_quantization_of_the_state_space_of__with_applications_to_the_Curie%E2%80%93Weiss_model/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Strict_deformation_quantization_of_the_state_space_of__with_applications_to_the_Curie%E2%80%93Weiss_model/#content","title":"Content","text":"<p>Increasing tensor powers of the kkmatricesMk(C) are known to give rise to a continuous bundle of C-algebras over I=f0g[1=N\u001a[0;1] with \fbers A1=N=Mk(C) N andA0=C(Xk), whereXk=S(Mk(C)), the state space of Mk(C), which is canonically a compact Poisson manifold (with strati\fed boundary). Our \frst result is the existence of a strict deformation quantization of Xk\u0012 a la Rie\u000bel, de\fned by perfectly natural quantization mapsQ1=N:~A0!A1=N(where ~A0is an equally natural dense Poisson subalgebra of A0)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Structured_compression_by_weight_encryption_for_unstructured_pruning_and_quantization/","title":"Structured compression by weight encryption for unstructured pruning and quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Structured_compression_by_weight_encryption_for_unstructured_pruning_and_quantization/#summary","title":"Summary","text":"<p>The paper presents a new weight representation scheme for Sparse Quantized Neural Networks, which uses a fine-grained and unstructured pruning method for structured regular format during inference. The proposed format has high compression ratio without compromising the model accuracy and allows for performance enhancement on devices with irregular representations. The paper also discusses the challenges of achieving performance enhancement without an inherently parallel sparse-matrix decoding process during inference."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Structured_compression_by_weight_encryption_for_unstructured_pruning_and_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Structured_compression_by_weight_encryption_for_unstructured_pruning_and_quantization/#content","title":"Content","text":"<p>This paper proposes a new weight representation scheme for Sparse Quantized Neural Networks using a fine-grained and unstructured pruning method, which achieves a structured regular format for efficient decoding during inference through an XOR-gate network. The proposed format shows high compression ratio for various deep learning models without compromising on the model accuracy, enabling performance enhancement on devices with irregular representations of sparse matrix formats. This paper also discusses the challenges of achieving performance enhancement without an inherently parallel sparse-matrix decoding process during inference."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Style-woven_Attention_Network_for_Zero-shot_Ink_Wash_Painting_Style_Transfer/","title":"Style-woven Attention Network for Zero-shot Ink Wash Painting Style Transfer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Style-woven_Attention_Network_for_Zero-shot_Ink_Wash_Painting_Style_Transfer/#summary","title":"Summary","text":"<p>The paper proposes a style-woven attention network to achieve zero-shot ink wash painting style transfer, where traditional Chinese painting style - ink wash - is applied to real-world photos. The authors' model disentangles feature representations for styles and contents and captures semantic correlations through unsupervised learning and an added ink style loss. The experiments show that their method achieves state-of-the-art results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Style-woven_Attention_Network_for_Zero-shot_Ink_Wash_Painting_Style_Transfer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Style-woven_Attention_Network_for_Zero-shot_Ink_Wash_Painting_Style_Transfer/#content","title":"Content","text":"<p>Traditional Chinese painting places more emphasis on the verve in visual effect, especially ink painting, which utilizes lines to convey expression and pays little attention to texture. While some style transfer methods have recently applied traditional Chinese painting style (such as ink wash style) to photorealistic, there are limitations to applying ink stylization to different types of real-world photos in a dataset using these style transfer methods. In this paper, the authors propose a style-woven attention network to disentangle feature representations for styles and contents and achieve zero-shot ink wash painting style transfer. Their model captures the semantic correlations of content and style through unsupervised learning and an added ink style loss. Experiments on a publicly available dataset prove that their method achieves state-of-the-art results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Subtensor_quantization_for_mobilenets/","title":"Subtensor quantization for mobilenets","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Subtensor_quantization_for_mobilenets/#summary","title":"Summary","text":"<p> This paper presents a study on the challenges faced while quantizing Mobilenet architecture and proposes an alternative approach that uses asymmetric quantization to support depthwise convolutions, achieving near-floating point accuracy in 8-bit post-training quantized inference on the ImageNet dataset, without the need for per-channel or training-aware methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Subtensor_quantization_for_mobilenets/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Subtensor_quantization_for_mobilenets/#content","title":"Content","text":"<p> Quantization is an essential technique for optimizing deep neural network (DNN) inference, particularly for embedded systems with strict constraints on memory and power consumption. However, not all DNN designs are easily quantifiable, such as the widely-used Mobilenet architecture. In this paper, the authors present a case study on the challenges of quantizing the Mobilenet architecture and propose an alternative approach that enhances asymmetric quantization to support depthwise convolutions. They demonstrate that their approach achieves near-floating point accuracy in 8-bit post-training quantized inference on the ImageNet dataset, without the need for per-channel or training-aware methods."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Successive_log_quantization_for_cost-efficient_neural_networks_using_stochastic_computing/","title":"Successive log quantization for cost-efficient neural networks using stochastic computing","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Successive_log_quantization_for_cost-efficient_neural_networks_using_stochastic_computing/#summary","title":"Summary","text":"<p>Summary: This paper proposes successive log quantization (SLQ), which extends log quantization with significant improvements in precision and accuracy for state-of-the-art SC-DNNs. The experimental results demonstrate that SLQ can significantly extend both the accuracy and efficiency of SC-DNNs achieving less than 1-1.5% accuracy drop for AlexNet, SqueezeNet, and VGG-S at mere 4-5 bit weight resolution.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Successive_log_quantization_for_cost-efficient_neural_networks_using_stochastic_computing/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Successive_log_quantization_for_cost-efficient_neural_networks_using_stochastic_computing/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Switchable_precision_neural_networks/","title":"Switchable precision neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Switchable_precision_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes a flexible quantization strategy called Switchable Precision neural Networks (SP-Nets), which allows a shared network to operate at multiple quantization levels based on instant memory, latency, power consumption, and accuracy demands. The network can adjust its precision on the fly from BinaryConnect to Binarized Neural Network enabling dot-products using only summations or bit operations. The paper also proposes a self-distillation scheme to increase the performance of the quantized switches. The approach is tested with three different quantizers, and the performance of SP-Nets is demonstrated against independently trained quantized models for classification accuracy for Tiny ImageNet and ImageNet datasets using ResNet-18 and MobileNet architectures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Switchable_precision_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Switchable_precision_neural_networks/#content","title":"Content","text":"<p>Instantaneous and on demand accuracy-ef\ufb01ciency trade-off has been recently explored in the context of neural networks slimming. In this paper, we propose a \ufb02exible quantization strategy, termed Switchable Precision neural Networks (SP-Nets), to train a shared network capable of operating at multiple quantization levels. At runtime, the network can adjust its precision on the \ufb02y according to instant memory, latency, power consumption and accuracy demands. For example, by constraining the network weights to 1-bit with switchable precision activations, our shared network spans from BinaryConnect to Binarized Neural Network, allowing to perform dot-products using only summations or bit operations. In addition, a self-distillation scheme is proposed to increase the performance of the quantized switches. We tested our approach with three different quantizers and demonstrate the performance of SP-Nets against independently trained quantized models in classification accuracy for Tiny ImageNet and ImageNet datasets using ResNet-18 and MobileNet architectures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Syq%3A_Learning_symmetric_quantization_for_efficient_deep_neural_networks/","title":"Syq: Learning symmetric quantization for efficient deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Syq%3A_Learning_symmetric_quantization_for_efficient_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper introduces a quantization method that uses a symmetric codebook to reduce the loss resulting from low-precision quantization of weight parameters and/or activations in deep neural networks. The method is applied to weight subgroups determined based on their locality in the weight matrix, preserving the hardware simplicity of low-precision representations. Empirical results show that the method can improve accuracy for networks with extremely low-precision weights and activations and imposes minimal or no hardware implications to more coarse-grained approaches. The source code is available for use.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Syq%3A_Learning_symmetric_quantization_for_efficient_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Syq%3A_Learning_symmetric_quantization_for_efficient_deep_neural_networks/#content","title":"Content","text":"<p>Inference for state-of-the-art deep neural networks is computationally expensive, making them dif\ufb01cult to deploy on constrained hardware environments. An ef\ufb01cient way to reduce this complexity is to quantize the weight pa- rameters and/or activations during training by approximat- ing their distributions with a limited entry codebook. For very low-precisions, such as binary or ternary networks with 1-8-bit activations, the information loss from quan- tization leads to signi\ufb01cant accuracy degradation due to large gradient mismatches between the forward and back- ward functions. In this paper, we introduce a quantization method to reduce this loss by learning a symmetric code- book for particular weight subgroups. These subgroups are determined based on their locality in the weight ma- trix, such that the hardware simplicity of the low-precision representations is preserved. Empirically, we show that symmetric quantization can substantially improve accu- racy for networks with extremely low-precision weights and activations. We also demonstrate that this representa- tion imposes minimal or no hardware implications to more coarse-grained approaches. Source code is available at https://www.github.com/julianfaraone/SYQ."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/TVT%3A_Three-Way_Vision_Transformer_through_Multi-Modal_Hypersphere_Learning_for_Zero-Shot_Sketch-Based_Image_Retrieval/","title":"TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/TVT%3A_Three-Way_Vision_Transformer_through_Multi-Modal_Hypersphere_Learning_for_Zero-Shot_Sketch-Based_Image_Retrieval/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel token-based strategy called Three-Way Vision Transformer (TVT) to retrieve natural images related to sketch queries from unseen categories in zero-shot sketch-based image retrieval (ZS-SBIR) task. They integrate three pre-trained Vision Transformers (ViTs) into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. They also propose a distillation process to supervise fusion ViT with soft targets from modality-specific ViTs. The method learns a multi-modal hypersphere to bridge the modal gap between modalities of sketch and image without losing uniformity. The proposed TVT method outperforms other ZS-SBIR methods on three benchmark datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/TVT%3A_Three-Way_Vision_Transformer_through_Multi-Modal_Hypersphere_Learning_for_Zero-Shot_Sketch-Based_Image_Retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/TVT%3A_Three-Way_Vision_Transformer_through_Multi-Modal_Hypersphere_Learning_for_Zero-Shot_Sketch-Based_Image_Retrieval/#content","title":"Content","text":"<p>Abstract:  In this paper, the authors propose a novel approach for zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. They propose a token-based strategy called Three-Way Vision Transformer (TVT) that integrates three pre-trained Vision Transformers (ViTs) into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. They also propose a distillation process to supervise fusion ViT with soft targets from modality-specific ViTs to prevent catastrophic forgetting. Their method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments demonstrate the superiority of their TVT method over the state-of-the-art ZS-SBIR methods on three benchmark datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Task-aware_quantization_network_for_JPEG_image_compression/","title":"Task-aware quantization network for JPEG image compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Task-aware_quantization_network_for_JPEG_image_compression/#summary","title":"Summary","text":"<p>The paper proposes a deep neural network for JPEG image compression that can predict image-specific optimized quantization tables by adjusting the objective function of the network. They learn a differentiable loss function to address non-differentiable components in the encoder and evaluate the proposed algorithm using multiple task-specific losses. The effectiveness of the algorithm is demonstrated in different tasks including adaptive quantization and bitrate approximation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Task-aware_quantization_network_for_JPEG_image_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Task-aware_quantization_network_for_JPEG_image_compression/#content","title":"Content","text":"<p>We propose a deep neural network for JPEG image compression that predicts image-specific optimized quantization tables compatible with the standard JPEG encoder and decoder. We provide the capability to learn task-specific quantization tables in a principled way by adjusting the objective function of the network. We address the challenge of non-differentiable components in the encoder, such as run-length encoding and Huffman coding, by learning a differentiable loss function that approximates bitrates using simple network blocks \u2013 two MLPs and an LSTM. We evaluate the proposed algorithm using multiple task-specific losses, and demonstrate its effectiveness in different tasks. Keywords: JPEG image compression, adaptive quantization, bitrate approximation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tbn%3A_Convolutional_neural_network_with_ternary_inputs_and_binary_weights/","title":"Tbn: Convolutional neural network with ternary inputs and binary weights","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tbn%3A_Convolutional_neural_network_with_ternary_inputs_and_binary_weights/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Ternary-Binary Network (TBN) to provide an efficient approximation to standard CNNs. TBN replaces the arithmetical operations in standard CNNs with efficient XOR, AND and bitcount operations, and provides an optimal tradeoff between memory, efficiency and performance. TBN demonstrates its consistent effectiveness when applied to various CNN architectures on multiple datasets of different scales, and provides \u223c32\u00d7 memory savings and 40\u00d7 faster convolutional operations. Meanwhile, TBN can outperform XNOR-Network by up to 5.5% (top-1 accuracy) on the ImageNet classification task, and up to 4.4% (mAP score) on the PASCAL VOC object detection task.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tbn%3A_Convolutional_neural_network_with_ternary_inputs_and_binary_weights/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tbn%3A_Convolutional_neural_network_with_ternary_inputs_and_binary_weights/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tensorquant%3A_A_simulation_toolbox_for_deep_neural_network_quantization/","title":"Tensorquant: A simulation toolbox for deep neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tensorquant%3A_A_simulation_toolbox_for_deep_neural_network_quantization/#summary","title":"Summary","text":"<p> The paper discusses the advantages of using low precision numerical representations in deep neural networks (DNN), and how such compact representations can be implemented on hardware accelerators like FPGAs. The authors introduce TensorQuant, a quantization toolbox for the TensorFlow framework, which supports generic quantization methods and allows experimental evaluation of the impact of quantization on DNN topologies. They show an analysis of fix-point quantizations of popular CNN topologies using TensorQuant toolset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tensorquant%3A_A_simulation_toolbox_for_deep_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tensorquant%3A_A_simulation_toolbox_for_deep_neural_network_quantization/#content","title":"Content","text":"<p>Recent research shows that low precision numerical representations of the training and test data, weights, and gradients in deep neural networks (DNN) do not lead to a general loss in accuracy. Such compact representations lead to a significant reduction of communication bottleneck in distributed DNN training and faster neural network implementations on hardware accelerators like FPGAs. However, the optimal choice of the quantization method and number of coding bits is topology-dependent, and there is no general theory available to derive the optimal quantization during the design of a DNN topology. Therefore, in this paper, the authors present TensorQuant - a quantization toolbox for the TensorFlow framework, allowing a transparent quantization simulation of existing DNN topologies during training and inference. TensorQuant supports generic quantization methods and allows experimental evaluation of the impact of the quantization on single layers as well as the full topology. The authors show an analysis of fix-point quantizations of popular CNN topologies using TensorQuant toolset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tent%3A_Efficient_quantization_of_neural_networks_on_the_tiny_edge_with_tapered_fixed_point/","title":"Tent: Efficient quantization of neural networks on the tiny edge with tapered fixed point","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tent%3A_Efficient_quantization_of_neural_networks_on_the_tiny_edge_with_tapered_fixed_point/#summary","title":"Summary","text":"<p>Summary: The paper presents a new low-precision framework, TENT, for TinyML models. The authors propose a tapered fixed-point quantization algorithm that matches the numerical format\u2019s dynamic range and distribution to that of the deep neural network model at each layer. The accuracy on classification tasks improves up to \u224831%, with an energy overhead of \u224817-30% as compared to fixed-point, for ConvNet and ResNet-18 models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tent%3A_Efficient_quantization_of_neural_networks_on_the_tiny_edge_with_tapered_fixed_point/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Tent%3A_Efficient_quantization_of_neural_networks_on_the_tiny_edge_with_tapered_fixed_point/#content","title":"Content","text":"<p>In this research, we propose a new low-precision framework, TENT, to leverage the benefits of a tapered fixed-point numerical format in TinyML models. We introduce a tapered fixed-point quantization algorithm that matches the numerical format\u2019s dynamic range and distribution to that of the deep neural network model\u2019s parameter distribution at each layer. Results show that the accuracy on classification tasks improves up to \u224831% with an energy overhead of \u224817-30% as compared to fixed-point, for ConvNet and ResNet-18 models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Term_quantization%3A_furthering_quantization_at_run_time/","title":"Term quantization: furthering quantization at run time","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Term_quantization%3A_furthering_quantization_at_run_time/#summary","title":"Summary","text":"<p>This paper proposes a method called Term Quantization (TQ) to further quantization at run time in deep neural networks (DNNs) that have already been quantized. TQ works on power-of-two terms in values and can be used for synchronized processor arrays like systolic arrays to achieve efficient parallel processing. TQ has minimal impact on DNN model performance and significantly reduces inference computation costs compared to uniform quantization for the same level of model performance. The authors evaluated TQ on various DNN architectures."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Term_quantization%3A_furthering_quantization_at_run_time/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Term_quantization%3A_furthering_quantization_at_run_time/#content","title":"Content","text":"<p>We present a novel technique, called Term Quantization (TQ), for furthering quantization at run time for improved computational efficiency of deep neural networks (DNNs) already quantized with conventional quantization methods. TQ operates on power-of-two terms in expressions of values. We use TQ to facilitate tightly synchronized processor arrays, such as systolic arrays, for efficient parallel processing. TQ has a minimal impact on DNN model performance, and we evaluate TQ on various DNN architectures, demonstrating significant reductions in inference computation costs compared to conventional uniform quantization for the same level of model performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Term_revealing%3A_Furthering_quantization_at_run_time_on_quantized_dnns/","title":"Term revealing: Furthering quantization at run time on quantized dnns","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Term_revealing%3A_Furthering_quantization_at_run_time_on_quantized_dnns/#summary","title":"Summary","text":"<p>Summary: This paper presents a new technique called Term Revealing (TR) that further improves quantization in Deep Neural Networks (DNNs) by operating on power-of-two terms and selecting a fixed number of largest terms at runtime. It has minimal impact on DNN model performance and is used to facilitate tightly synchronized processor arrays for efficient parallel processing. An FPGA implementation is developed that can switch between conventional quantization and TR-enabled quantization with negligible delay. The paper uses a signed digit representation (SDR) to enhance TR's efficiency and shows significant reductions in inference computations compared to conventional quantization with the same level of model performance, evaluated on MLR for MNIST, multiple CNNs for Imagenet, and an LSTM for Wikitext-2.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Term_revealing%3A_Furthering_quantization_at_run_time_on_quantized_dnns/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Term_revealing%3A_Furthering_quantization_at_run_time_on_quantized_dnns/#content","title":"Content","text":"<p> We present a novel technique called Term Revealing (TR) for furthering quantization at run time in Deep Neural Networks (DNNs) already quantized with conventional quantization methods for improved performance. TR operates on power-of-two terms in binary expressions of values and selects a fixed number of largest terms at runtime in computing a dot-product computation. By exploiting normal-like weight and data distributions, TR has minimal impact on DNN model performance. We use TR to facilitate tightly synchronized processor arrays for efficient parallel processing. We show an FPGA implementation that can use a small number of control bits to switch between conventional quantization and TR-enabled quantization with negligible delay. To enhance TR's efficiency further, we use a signed digit representation (SDR), and we develop an efficient encoding method called Hybrid Encoding for Signed Expressions (HESE). We evaluate TR with HESE encoded values on an MLP for MNIST, multiple CNNs for ImageNet, and an LSTM for Wikitext-2 and show significant reductions in inference computations compared to conventional quantization for the same level of model performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ternarybert%3A_Distillation-aware_ultra-low_bit_bert/","title":"Ternarybert: Distillation-aware ultra-low bit bert","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ternarybert%3A_Distillation-aware_ultra-low_bit_bert/#summary","title":"Summary","text":"<p>Summary: The authors present TernaryBERT, a method that ternarizes the weights of pre-trained BERT models. They use approximation-based and loss-aware ternarization and investigate the ternarization granularity of different parts of BERT. The training process involves knowledge distillation to reduce accuracy degradation. TernaryBERT performs better than other BERT quantization methods while being 14.9x smaller. The authors also discuss other compression methods and their challenges in ultra-low bit quantization.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ternarybert%3A_Distillation-aware_ultra-low_bit_bert/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ternarybert%3A_Distillation-aware_ultra-low_bit_bert/#content","title":"Content","text":"<p> In this work, the authors propose TernaryBERT, a method to compress the pre-trained BERT model by ternarizing its weights. They use approximation-based and loss-aware ternarization methods and investigate the ternarization granularity of different parts of BERT. To reduce accuracy degradation, they leverage knowledge distillation in the training process. Experiments show that TernaryBERT outperforms other BERT quantization methods and achieves comparable performance as the full-precision model while being 14.9x smaller. The authors also discuss various other compression methods for neural networks, such as low-rank approximation, weight sharing, pruning, adaptive depth and/or width, and quantization, and their challenges in ultra-low bit quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/The_bach_doodle%3A_Approachable_music_composition_with_machine_learning_at_scale/","title":"The bach doodle: Approachable music composition with machine learning at scale","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/The_bach_doodle%3A_Approachable_music_composition_with_machine_learning_at_scale/#summary","title":"Summary","text":"<p>The paper describes the design and implementation of an AI-powered Google Doodle called the Bach Doodle, which allows users to create their own melody and have it harmonized in the style of Bach using a machine learning model called Coconet. The authors re-implemented Coconet using TensorFlow.js and reduced its runtime from 40s to 2s by adopting dilated depthwise separable convolutions and fusing operations. The Bach Doodle was used by many people in just three days and Coconet received more than 55 million queries. Users were able to rate their compositions and contribute them to a public dataset, which the authors are releasing with the paper, for applications like ethnomusicological studies, music education, and improving machine learning models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/The_bach_doodle%3A_Approachable_music_composition_with_machine_learning_at_scale/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/The_bach_doodle%3A_Approachable_music_composition_with_machine_learning_at_scale/#content","title":"Content","text":"<p>To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model in the style of Bach. We re-implemented Coconet in TensorFlow.js to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depthwise separable convolutions and fusing operations. In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries. Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper. We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_binary_convolutional_neural_network/","title":"Towards accurate binary convolutional neural network","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_binary_convolutional_neural_network/#summary","title":"Summary","text":"<p>Summary: The paper proposes a scheme called ABC-Net for training binary convolutional neural networks (CNNs) with weights and activations constrained to {-1, +1} at run-time. The proposed scheme addresses the challenge of severe prediction accuracy degradation in binarizing CNNs by approximating full-precision weights with the linear combination of multiple binary weight bases and employing multiple binary activations to reduce information loss. The resulting ABC-Net is shown to achieve performance closer to full-precision CNNs and even comparable prediction accuracy on the ImageNet and forest trail datasets, provided adequate binary weight bases and activations are used.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_binary_convolutional_neural_network/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_binary_convolutional_neural_network/#content","title":"Content","text":"<p> We introduce a novel scheme to train binary convolutional neural networks (CNNs) \u2013 CNNs with weights and activations constrained to {-1,+1} at run-time. In this paper, we address the severe prediction accuracy degradation issue in the previous works on binarizing CNNs with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_post-training_network_quantization_via_bit-split_and_stitching/","title":"Towards accurate post-training network quantization via bit-split and stitching","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_post-training_network_quantization_via_bit-split_and_stitching/#summary","title":"Summary","text":"<p>Summary: The paper presents a Bit-Split and Stitching framework for lower-bit post-training quantization, which is validated on various computer vision tasks and can achieve near-original model performance even when quantizing FP32 models to INT3 without fine-tuning. However, post-training quantization still suffers from the problem of significant accuracy degradation when both activations and weights are quantized into very low-bit integers.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_post-training_network_quantization_via_bit-split_and_stitching/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_post-training_network_quantization_via_bit-split_and_stitching/#content","title":"Content","text":"<p>In this paper, a Bit-Split and Stitching framework for lower-bit post-training quantization is proposed. The proposed framework is validated on various computer vision tasks, including image classification, object detection, and instance segmentation, with various network architectures. The framework can achieve near-original model performance even when quantizing FP32 models to INT3 without fine-tuning. Despite the advantages of post-training quantization, it has the problem of significant accuracy degradation, especially when both activations and weights are quantized into very low-bit integers."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_quantization_and_pruning_via_data-free_knowledge_transfer/","title":"Towards accurate quantization and pruning via data-free knowledge transfer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_quantization_and_pruning_via_data-free_knowledge_transfer/#summary","title":"Summary","text":"<p> The paper discusses the challenges of obtaining compact networks without access to training data and proposes data-free quantization and pruning methods by transferring knowledge from trained large networks, which leads to competitive and accurate compact networks. The proposed method facilitates the creation of more compact and lightweight networks without sacrificing performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_quantization_and_pruning_via_data-free_knowledge_transfer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_accurate_quantization_and_pruning_via_data-free_knowledge_transfer/#content","title":"Content","text":"<p>When large scale training data is available, one can obtain compact and accurate networks to be deployed in resource-constrained environments effectively through quantization and pruning. However, training data are often protected due to privacy concerns and it is challenging to obtain compact networks without data. We study data-free quantization and pruning by transferring knowledge from trained large networks to compact networks. Our data-free compact networks achieve competitive accuracy to networks trained and fine-tuned with training data. Our quantized and pruned networks achieve good performance while being more compact and lightweight."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_effective_low-bitwidth_convolutional_neural_networks/","title":"Towards effective low-bitwidth convolutional neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_effective_low-bitwidth_convolutional_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper proposes three methods to improve the training of a low-precision convolutional neural network using low-precision weights and low-bitwidth activations. The approaches include a two-stage optimization strategy, a progressive optimization approach, and a novel learning scheme. The methods demonstrated to be effective in different datasets, with a 4-bit precision network performing comparable to full-precision models using standard architectures like AlexNet and ResNet-50.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_effective_low-bitwidth_convolutional_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_effective_low-bitwidth_convolutional_neural_networks/#content","title":"Content","text":"<p> This paper proposes three approaches to improve the training of a low-precision convolutional neural network with both low-precision weights and low-bitwidth activations. First, a two-stage optimization strategy is proposed to optimize a net with quantized weights followed by quantized activations. Second, a progressive optimization approach is used to decrease bit-width from high-precision to low-precision during the training process. Finally, a novel learning scheme is adopted to jointly train a full-precision model alongside the low-precision one to provide guidance. The proposed methods are shown to be effective in experiments on various datasets, with a 4-bit precision network achieving performance comparable to its full-precision counterpart using standard network architectures (i.e., AlexNet and ResNet-50)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_efficient_training_for_neural_network_quantization/","title":"Towards efficient training for neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_efficient_training_for_neural_network_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a technique named scale-adjusted training (SAT) to address the accuracy drop resulting from performance degeneration in quantization, due to reduced capacity or inefficient training. The authors identified two critical rules for efficient training and recommend compliance with them while avoiding popular parameterized clipping activation (PACT) technique, which introduces quantization error while calculating gradient. SAT and gradient-calibrated PACT help achieve comparable or better performance on quantized models than full-precision ones, with consistent improvement over previous quantization methods across multiple models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_efficient_training_for_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_efficient_training_for_neural_network_quantization/#content","title":"Content","text":"<p> Quantization is a promising method to deploy computation-intensive deep models into resource-constrained platforms, but it suffers from performance degeneration. To investigate whether the accuracy drop is due to the reduced capacity or the inef\ufb01cient training during the quantization procedure, two critical rules for ef\ufb01cient training are discovered. Recent quantization approaches violate these rules and lead to degenerated convergence. To deal with this problem, the authors propose a simple yet effective technique, named scale-adjusted training (SAT), to comply with the discovered rules and facilitate ef\ufb01cient training. The authors also analyze the quantization error introduced in calculating the gradient in the popular parameterized clipping activation (PACT) technique. Through SAT together with gradient-calibrated PACT, quantized models obtain comparable or even better performance than their full-precision counterparts, achieving state-of-the-art accuracy with consistent improvement over previous quantization methods on a wide spectrum of models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_optimal_quantization_of_neural_networks/","title":"Towards optimal quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_optimal_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>The paper proposes an approach to quantizing deep networks for use in mobile and in-sensor applications using functional high-rate quantization theory, which leads to an optimal quantizer that is computed using backpropagation algorithm. The paper focuses on fixed-rate scalar quantization of edge weights that can be used directly for inferential computation. The proposed approach provides a theoretical foundation for quantization, and even in cases where technical conditions are not met, a heuristic quantizer can still be computed with certain regularization guarantees."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_optimal_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_optimal_quantization_of_neural_networks/#content","title":"Content","text":"<p>Due to the need for compressing deep networks for use in mobile and in-sensor applications, there has been a growing interest in quantizing synaptic weights. However, most prior work has been heuristic, lacking theoretical foundations. This study develops an approach to quantizing deep networks using functional high-rate quantization theory, which leads to an optimal quantizer that is computed using the backpropagation algorithm. In cases where certain technical conditions are not met, a heuristic quantizer can still be computed with certain regularization guarantees. The focus is on fixed-rate scalar quantization of edge weights that can be used directly for inferential computation. </p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_unified_int8_training_for_convolutional_neural_network/","title":"Towards unified int8 training for convolutional neural network","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_unified_int8_training_for_convolutional_neural_network/#summary","title":"Summary","text":"<p>Summary: This paper presents a unified 8-bit training framework for common CNNs that ensures stable training and high accuracy. The authors identified four distinctive characteristics of gradients and used them to guide gradient quantization. They also derived two principles for stable INT8 training and proposed two universal techniques to avoid illegal gradient update and reduce direction deviation. The framework promises accurate and efficient INT8 training for various networks and tasks, including object detection. Additionally, it reduces training time by 22% on a Pascal GPU without complex optimization effort.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_unified_int8_training_for_convolutional_neural_network/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Towards_unified_int8_training_for_convolutional_neural_network/#content","title":"Content","text":"<p> Recently, there has been extensive research on low-bit network quantization to accelerate the inference of convolutional neural networks (CNNs). However, low-bit training with quantized gradients for accelerating the backward process often leads to unstable training and even crashes. Therefore, a unified low-bit training framework that can support diverse networks on various tasks is lacking. In this paper, we present a  unified 8-bit (INT8) training framework for common CNNs that ensures stable training and high accuracy. We empirically identify the four distinctive characteristics of gradients and use them to guide gradient quantization. Moreover, we provide an in-depth analysis of the convergence bound and derive two principles for stable INT8 training. We also propose two universal techniques, Direction Sensitive Gradient Clipping and Deviation Counteractive Learning Rate Scaling, to further avoid illegal gradient update and reduce direction deviation. The experiments show that our framework promises accurate and efficient INT8 training for various networks and tasks, including object detection, which prior studies have not succeeded in. Additionally, our framework is flexible and reduces training time by 22% on a Pascal GPU without complex optimization effort."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trainable_Thresholds_for_Neural_Network_Quantization/","title":"Trainable Thresholds for Neural Network Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trainable_Thresholds_for_Neural_Network_Quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes two methods for optimizing the training with quantization procedure to maintain the accuracy of full-precision neural networks for modern mobile architectures like Mobilenet-v1, MobileNet-v2, and MNAS. By reducing the train dataset size and small number of trainable parameters, the models achieved high accuracy in less time and resources. The proposed techniques are available on Github.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trainable_Thresholds_for_Neural_Network_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trainable_Thresholds_for_Neural_Network_Quantization/#content","title":"Content","text":"<p> The neural network quantization is highly desired procedure to perform before running neural networks on mobile devices. Quantization without fine-tuning leads to accuracy drop of the model, whereas commonly used training with quantization is done on the full set of the labeled data and therefore is both time- and resource-consuming. Real-life applications require simplification and acceleration of quantization procedure that will maintain the accuracy of full-precision neural network, especially for modern mobile neural network architectures like Mobilenet-v1, MobileNet-v2 and MNAS. Here we present two methods to significantly optimize the training with quantization procedure. The first one is introducing the trained scale factors for the discretization thresholds that are separate for each filter. The second one is based on mutual rescaling of consequent depth-wise separable convolution and convolution layers. Using the proposed techniques, we quantize the modern mobile architectures of neural networks with the set of train data of only 10% of the total ImageNet 2012 sample. Such reduction of train dataset size and small number of trainable parameters allow fine-tuning the network for several hours while maintaining the high accuracy of quantized model (accuracy drop was less than 0.5%). Ready-for-use models and code are available at: https://github.com/agoncharenko1992/FAT-fast-adjustable-threshold. Keywords: Distillation, Machine Learning, Neural Networks, Quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trained_quantization_thresholds_for_accurate_and_efficient_fixed-point_inference_of_deep_neural_networks/","title":"Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trained_quantization_thresholds_for_accurate_and_efficient_fixed-point_inference_of_deep_neural_networks/#summary","title":"Summary","text":"<p> In this paper, a method is proposed for training quantization thresholds using backpropagation and gradient descent for uniform symmetric quantizers. The quantizers are constrained to per-tensor scaling of weights and activations to make it feasible for hardware implementations. The empirical validation shows that the proposed approach achieves near-floating-point accuracy with less than 5 epochs on various CNNs for ImageNet classification, including traditionally difficult networks like MobileNets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trained_quantization_thresholds_for_accurate_and_efficient_fixed-point_inference_of_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trained_quantization_thresholds_for_accurate_and_efficient_fixed-point_inference_of_deep_neural_networks/#content","title":"Content","text":"<p> We propose a method for training quantization thresholds using standard backpropagation and gradient descent for uniform symmetric quantizers. Our approach shows a range-precision trade-off leading to better optima. The quantizers are constrained to per-tensor scaling of weights and activations to make it amenable for hardware implementations. We empirically validate them on various CNNs for ImageNet classification, and achieve near-floating-point accuracy on traditionally difficult networks such as MobileNets with less than 5 epochs."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trained_ternary_quantization/","title":"Trained ternary quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trained_ternary_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes Trained Ternary Quantization (TTQ) to reduce the precision of weights in neural networks to ternary values for easy deployment on mobile devices with limited power budgets. The method has little accuracy degradation and can even improve the accuracy of some models, and the models are 16 times smaller than full-precision models. Experiments show that the TTQ outperforms previous ternary models and even full-precision models in some cases.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trained_ternary_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trained_ternary_quantization/#content","title":"Content","text":"<p>Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be dif\ufb01cult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16smaller than full-precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_and_inference_with_integers_in_deep_neural_networks/","title":"Training and inference with integers in deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_and_inference_with_integers_in_deep_neural_networks/#summary","title":"Summary","text":"<p> The paper proposes WAGE, a novel method to discretize both training and inference processes in deep neural networks, with low-bitwidth integers. The method includes linearly constraining weights, activations, gradients, and errors among layers, replacing batch normalization by a constant scaling layer, and simplifying other components for integer implementation in pure discrete data flow for fixed-point devices. The authors demonstrate improved accuracies on multiple datasets and potential deployment of training in hardware systems, such as integer-based deep learning accelerators and neuromorphic chips, with comparable accuracy and higher energy efficiency."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_and_inference_with_integers_in_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_and_inference_with_integers_in_deep_neural_networks/#content","title":"Content","text":"<p>In this work, the authors propose a novel method called WAGE to discretize both training and inference processes in deep neural networks with low-bitwidth integers. The method shifts and linearly constrains weights, activations, gradients and errors among layers. The authors also replace batch normalization by a constant scaling layer, and simplify other components for integer implementation in pure discrete dataflow for fixed-point devices. The authors demonstrate improved accuracies on multiple datasets and potential deployment of training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is important for future AI applications."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_for_multi-resolution_inference_using_reusable_quantization_terms/","title":"Training for multi-resolution inference using reusable quantization terms","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_for_multi-resolution_inference_using_reusable_quantization_terms/#summary","title":"Summary","text":"<p> The paper proposes a training approach that supports multi-resolution inference by reusing quantization terms and introduces a multi-resolution Multiply-Accumulator (mMAC) design. The approach enables the creation of Deep Neural Network models that can support up to 10 resolutions with a moderate performance reduction, allowing for a broader range of choices in trading off cost, efficiency, and latency across different computational budgets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_for_multi-resolution_inference_using_reusable_quantization_terms/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_for_multi-resolution_inference_using_reusable_quantization_terms/#content","title":"Content","text":"<p> Low-resolution uniform quantization, particularly 4-bit bitwidth, has significantly improved the efficiency of Deep Neural Network (DNN) inference. However, it presents a trade-off between performance and hardware cost. To address this concern, the authors present a novel training approach that supports multi-resolution inference by reusing quantization terms. They also introduce a multi-resolution Multiplier-Accumulator (mMAC) design that allows a hardware platform to support multiple resolutions at runtime, which takes fewer cycles to multiply lower-resolution numbers and more cycles to multiply higher-resolution numbers. The authors evaluate their approach on multiple applications and show that models resulting from their approach can support up to 10 resolutions with only a moderate performance reduction compared to individually trained models. They also compare their multi-resolution mMAC design with other conventional MAC designs and show that it broadens the choices for trading off cost, efficiency, and latency across a range of computational budgets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_high-performance_and_large-scale_deep_neural_networks_with_full_8-bit_integers/","title":"Training high-performance and large-scale deep neural networks with full 8-bit integers","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_high-performance_and_large-scale_deep_neural_networks_with_full_8-bit_integers/#summary","title":"Summary","text":"<p>Summary: This paper proposes a complete quantization framework called \"WAGEUBN\" to quantize deep neural networks (DNNs) for training and inference using only low bit-width integer data, including weights, activation, gradient, error, update, and batch normalization (BN), with the goal of achieving faster processing speed, decreased memory cost, and higher energy efficiency. The proposed framework has achieved competitive accuracy on ResNet18/34/50 models on the ImageNet dataset, and advances the study of quantization in large-scale DNNs to the full 8-bit INT level, with potential for future efficient portable devices with online learning ability.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_high-performance_and_large-scale_deep_neural_networks_with_full_8-bit_integers/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_high-performance_and_large-scale_deep_neural_networks_with_full_8-bit_integers/#content","title":"Content","text":"<p>Abstract:\u2014Deep neural network (DNN) quantization converting \ufb02oating-point (FP) data in the network to integers (INT) is an effective way to shrink the model size for memory saving and simplify the operations for compute acceleration. Recently, researches on DNN quantization develop from inference to training, laying a foundation for the online training on accelerators. However, existing schemes leaving batch normalization (BN) untouched during training are mostly incomplete quantization that still adopt high precision FP in some parts of the data paths. Currently, there is no solution that can use only low bit-width INT data during the whole training process of large-scale DNNs with acceptable accuracy. In this work, through decomposing all the computation steps in DNNs and fusing three special quantization functions to satisfy the different precision requirements, we propose a unified complete quantization framework termed as \u201cWAGEUBN\u201d to quantize DNNs involving all data paths including W (Weights), A (Activation), G (Gradient), E (Error), U (Update), and BN. Moreover, the Momentum optimizer is also quantized to realize a completely quantized framework. Experiments on ResNet18/34/50 models demonstrate that WAGEUBN can achieve competitive accuracy on the ImageNet dataset. For the first time, the study of quantization in large-scale DNNs is advanced to the full 8-bit INT level. In this way, all the operations in the training and inference can be bit-wise operations, pushing towards faster processing speed, decreased memory cost, and higher energy efficiency. Our throughout quantization framework has great potential for future efficient portable devices with online learning ability.  Keywords: Neural Network Quantization, 8-bit Training, Full Quantization, Online Learning Device</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_nets%3A_A_deeper_understanding/","title":"Training quantized nets: A deeper understanding","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_nets%3A_A_deeper_understanding/#summary","title":"Summary","text":"<p>Summary: This paper discusses the current method of deploying deep neural networks on low-power portable devices and suggests training models directly with coarsely quantized weights as a key step towards learning on embedded platforms. The authors investigate training methods for quantized neural networks from a theoretical viewpoint, exploring both accuracy guarantees for training methods under convexity assumptions and the behavior of these algorithms for non-convex problems. The authors show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_nets%3A_A_deeper_understanding/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_nets%3A_A_deeper_understanding/#content","title":"Content","text":"<p>Currently, deep neural networks are deployed on low-power portable devices by \ufb01rst training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for ef\ufb01cient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neu-ral networks from a theoretical viewpoint. We \ufb01rst explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision repre-sentations have an important greedy search phase that purely quantized training methods lack, which explains the dif\ufb01culty of training using low-precision arithmetic."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_neural_networks_to_global_optimality_via_semidefinite_programming/","title":"Training quantized neural networks to global optimality via semidefinite programming","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_neural_networks_to_global_optimality_via_semidefinite_programming/#summary","title":"Summary","text":"<p>This article discusses the importance of neural network weights quantization due to its impact on energy efficiency, inference time and hardware deployment. The paper introduces a convex optimization approach to train polynomial activation-based quantized neural networks, which can be solved to global optimality in polynomial time using semidefinite relaxations. The article also presents examples to illustrate the effectiveness of this method, and describes prior research on compression and quantization of neural networks for hardware implementation."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_neural_networks_to_global_optimality_via_semidefinite_programming/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_neural_networks_to_global_optimality_via_semidefinite_programming/#content","title":"Content","text":"<p>Neural networks have been successful in various areas of machine learning. The quantization of neural network weights has become important due to its impact on the energy efficiency, inference time and deployment on hardware. Although post-training quantization is well-studied, training optimal quantized neural networks involves combinatorial non-convex optimization problems which are usually intractable. This research introduces a convex optimization approach to train quantized neural networks with polynomial activations. By leveraging hidden convexity in two-layer neural networks from recent literature, semidefinite lifting, and Grothendieck's identity, the authors show that certain quantized neural network problems can be solved to global optimality in polynomial time using tight semidefinite relaxations. Numerical examples are presented to illustrate the effectiveness of their method. Prior work on compression and quantization of neural networks for hardware implementations is also discussed.&gt;"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_neural_networks_with_a_full-precision_auxiliary_module/","title":"Training quantized neural networks with a full-precision auxiliary module","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_neural_networks_with_a_full-precision_auxiliary_module/#summary","title":"Summary","text":"<p>Summary: The paper proposes a solution to the difficulty of propagating gradients through a low-precision network by suggesting training the network with a full-precision auxiliary module, which creates additional full-precision routes to update the parameters of the low-precision model, without adding computational complexity at the inference time. The authors evaluate the proposed method on image classification and object detection over various quantization approaches and report consistent performance improvement. They achieve near lossless performance to the full-precision model using a 4-bit detector.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_neural_networks_with_a_full-precision_auxiliary_module/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Training_quantized_neural_networks_with_a_full-precision_auxiliary_module/#content","title":"Content","text":"<p>In this paper, the authors propose a solution to the difficulty in propagating gradients through a low-precision network due to the non-differentiable quantization function. They suggest training the low-precision network with a full-precision auxiliary module, which creates additional full-precision routes to update the parameters of the low-precision model, thus making the gradient back-propagation more easily. They discard the auxiliary module at the inference time without introducing any computational complexity to the low-precision network. The proposed method has been evaluated on image classification and object detection over various quantization approaches, and consistent performance increase has been observed. The authors achieve near lossless performance to the full-precision model by using a 4-bit detector, which is of great practical value."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/TransZero%2B%2B%3A_Cross_Attribute-Guided_Transformer_for_Zero-Shot_Learning/","title":"TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/TransZero%2B%2B%3A_Cross_Attribute-Guided_Transformer_for_Zero-Shot_Learning/#summary","title":"Summary","text":"<p>Summary: The paper proposes a cross attribute-guided Transformer network named TransZero++ for accurate attribute localization in zero-shot learning tasks. The model uses feature-level and prediction-level losses to teach two attribute-guided transformers to learn visual embeddings and semantic knowledge representations collaboratively. These embeddings are fused with class semantic vectors for desirable visual-semantic interaction and ZSL classification.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/TransZero%2B%2B%3A_Cross_Attribute-Guided_Transformer_for_Zero-Shot_Learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/TransZero%2B%2B%3A_Cross_Attribute-Guided_Transformer_for_Zero-Shot_Learning/#content","title":"Content","text":"<p>Abstract: \u2014Zero-shot learning (ZSL) tackles the novel class recognition problem by transferring semantic knowledge from seen classes to unseen ones. Semantic knowledge is typically represented by attribute descriptions shared between different classes, which act as strong priors for localizing object attributes that represent discriminative region features, enabling significant and sufficient visual-semantic interaction for advancing ZSL. In this paper, we propose a cross attribute-guided Transformer network, termed TransZero++, to refine visual features and learn accurate attribute localization for key semantic knowledge representations in ZSL. By further introducing feature-level and prediction-level semantical collaborative losses, the two attribute-guided transformers teach each other to learn semantic-augmented visual embeddings for key semantic knowledge representations via semantical collaborative learning. Finally, the semantic-augmented visual embeddings learned by AVT and VAT are fused to conduct desirable visual-semantic interaction cooperated with class semantic vectors for ZSL classification.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transductive_zero-shot_hashing_via_coarse-to-fine_similarity_mining/","title":"Transductive zero-shot hashing via coarse-to-fine similarity mining","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transductive_zero-shot_hashing_via_coarse-to-fine_similarity_mining/#summary","title":"Summary","text":"<p>The paper presents a transductive Zero-shot Hashing (ZSH) method that uses coarse-to-fine similarity mining to transfer knowledge from source data to target data for learning hashing models for novel/target classes without training data. The method achieves significant improvement over state-of-the-art methods on several benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transductive_zero-shot_hashing_via_coarse-to-fine_similarity_mining/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transductive_zero-shot_hashing_via_coarse-to-fine_similarity_mining/#content","title":"Content","text":"<p>Zero-shot Hashing (ZSH) is the process of learning hashing models for novel/target classes without training data. Most existing ZSH approaches use an intermediate shared semantic representation between the seen/source classes and novel/target classes, but the hash functions learned from the source dataset are biased when applied directly to the target classes. This paper presents a method for transductive ZSH and focuses on unlabelled data for novel classes. The method uses coarse-to-fine similarity mining and transfers knowledge from source data to target data, achieving significant improvement over state-of-the-art methods on several benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transform-based_feature_map_compression_for_cnn_inference/","title":"Transform-based feature map compression for cnn inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transform-based_feature_map_compression_for_cnn_inference/#summary","title":"Summary","text":"<p> The paper introduces a novel hardware-friendly transform-based approach, named 1D-Discrete Cosine Transform on Channel dimension with Masks (DCT-CM), to compress activations in deep convolutional neural networks (CNNs) by combining DCT, masks and coding formats. The proposed algorithm achieves a high compression ratio of 2.9x on ResNet-50 with an 8-bit quantization scheme, which can reduce data transfer bandwidth and power consumption."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transform-based_feature_map_compression_for_cnn_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transform-based_feature_map_compression_for_cnn_inference/#content","title":"Content","text":"<p> To achieve higher accuracy in machine learning tasks, deep convolutional neural networks (CNNs) are designed, but their large memory access leads to high power consumption. Various hardware-friendly compression methods have been proposed to reduce data transfer bandwidth by exploiting the sparsity of feature maps, but most of them focus on designing specialized encoding formats to increase compression ratio. In this paper, a novel hardware-friendly transform-based method named 1D-Discrete Cosine Transform on Channel dimension with Masks (DCT-CM) is proposed, which intelligently combines DCT, masks, and a coding format to compress activations. The proposed algorithm achieves an average compression ratio of 2.9\u00d7 during inference on ResNet-50 with an 8-bit quantization scheme."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transform_quantization_for_cnn_compression/","title":"Transform quantization for cnn compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transform_quantization_for_cnn_compression/#summary","title":"Summary","text":"<p>Summary: The paper proposes a post-training compression method using transform quantization to optimize and quantize CNN weights for better compression at a given quantization bit-rate. The method aims to unify quantization and dimensionality reduction using a rate-distortion framework. The experiments demonstrate that transform quantization improves the state of the art in CNN compression for both retrained and non-retrained scenarios.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transform_quantization_for_cnn_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Transform_quantization_for_cnn_compression/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Translation-equivariant_image_quantizer_for_bi-directional_image-text_generation/","title":"Translation-equivariant image quantizer for bi-directional image-text generation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Translation-equivariant_image_quantizer_for_bi-directional_image-text_generation/#summary","title":"Summary","text":"<p>This paper proposes a solution to achieve translation-equivariant image quantization through enforcing orthogonality among codebook embeddings. Three proof-of-concept experiments were conducted to analyze sample efficiency and effectiveness of the solution, which showed that translation-equivariant image quantization improves both sample efficiency and accuracy over VQGAN. The paper also discusses limitations of representing images with quantized indices in downstream tasks and demonstrates that current image quantization methods fail to satisfy translation equivariance due to aliasing caused by downsampling operations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Translation-equivariant_image_quantizer_for_bi-directional_image-text_generation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Translation-equivariant_image_quantizer_for_bi-directional_image-text_generation/#content","title":"Content","text":"<p> This paper presents a study on image quantization and explores the issue of translation equivariance. The authors propose a solution to achieve translation-equivariant image quantization by enforcing orthogonality among the codebook embeddings. The paper presents three proof-of-concept experiments to analyze sample efficiency and test the proposed solution's effectiveness. The experiments show that translation-equivariant image quantization improves not only sample efficiency but also accuracy over VQGAN. The paper discusses the two-stage approach of image quantization and downstream tasks, and how the idea of representing images with quantized indices has limitations due to broken translation equivariance in the quantized space. The paper demonstrates that current image quantization methods fail to satisfy translation equivariance due to aliasing caused by downsampling operations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trq%3A_Ternary_neural_networks_with_residual_quantization/","title":"Trq: Ternary neural networks with residual quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trq%3A_Ternary_neural_networks_with_residual_quantization/#summary","title":"Summary","text":"<p>The paper proposes Ternary Residual Quantization (TRQ) which is a stem-residual framework that performs quantization on full-precision weights for a refined reconstruction by combining the binarized stem and residual parts. TRQ has high flexibility and precision which can be extended to multiple bits. The TRQ model provides for great recognition accuracy while being accelerated."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trq%3A_Ternary_neural_networks_with_residual_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Trq%3A_Ternary_neural_networks_with_residual_quantization/#content","title":"Content","text":"<p>Ternary neural networks (TNNs) have the potential to accelerate network operations by reducing full-precision weights to ternary ones. However, existing TNNs have a significant accuracy loss due to simple thresholding operations. In this paper, we propose Ternary Residual Quantization (TRQ) to achieve more powerful TNNs with a stem-residual framework. TRQ recursively performs quantization on full-precision weights for a refined reconstruction by combining the binarized stem and residual parts. This unique quantization process endows TRQ with high flexibility and precision, and it can be easily extended to multiple bits. Experimental results demonstrate that TRQ yields great recognition accuracy while being accelerated."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Two-step_quantization_for_low-bit_neural_networks/","title":"Two-step quantization for low-bit neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Two-step_quantization_for_low-bit_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Two-Step Quantization (TSQ) framework for training lower-bit neural networks which consists of code learning and transformation functions. It uses a sparse quantization method for code learning and non-linear least square regression with low-bit constraints for the second step. The proposed method outperforms the state-of-the-art on CIFAR-10 and ILSVRC-12 datasets with a drop in accuracy of only 0.5 points compared to full-precision on 2-bit activation and ternary weight quantization of AlexNet.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Two-step_quantization_for_low-bit_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Two-step_quantization_for_low-bit_neural_networks/#content","title":"Content","text":"<p>In this paper, we propose a simple yet effective Two-Step Quantization (TSQ) framework for training extremely-low-bit neural networks with high accuracy. The framework decomposes the network quantization problem into two steps: code learning and transformation function learning based on the learned codes. The proposed sparse quantization method for code learning and the non-linear least square regression problem with low-bit constraints for the second step make the optimization problem easier to solve. Extensive experiments on CIFAR-10 and ILSVRC-12 datasets show that the proposed TSQ outperforms the state-of-the-art by a large margin, with the accuracy of our TSQ dropping only about 0.5 points compared with the full-precision counterpart for 2-bit activation and ternary weight quantization of AlexNet."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ultra-low_precision_4-bit_training_of_deep_neural_networks/","title":"Ultra-low precision 4-bit training of deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ultra-low_precision_4-bit_training_of_deep_neural_networks/#summary","title":"Summary","text":"<p> This paper introduces new techniques and numerical representation formats that enable aggressive scaling of training systems from 8-bits to 4-bits precision. It explores the use of a novel adaptive gradient scaling technique (GradScale) that addresses challenges of insufficient range and resolution in quantized gradients. The paper also proposes solutions to mitigate the impact of bias on model convergence and examines the techniques in a variety of deep learning models with non-significant loss in accuracy while enabling significant hardware acceleration."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ultra-low_precision_4-bit_training_of_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Ultra-low_precision_4-bit_training_of_deep_neural_networks/#content","title":"Content","text":"<p>In this paper, we propose a number of novel techniques and numerical representation formats that enable, for the very first time, the precision of training systems to be aggressively scaled from 8-bits to 4-bits. To enable this advance, we explore a novel adaptive Gradient Scaling technique (GradScale) that addresses the challenges of insufficient range and resolution in quantized gradients as well as explores the impact of quantization errors observed during model training. We theoretically analyze the role of bias in gradient quantization and propose solutions that mitigate the impact of this bias on model convergence. Finally, we examine our techniques on a spectrum of deep learning models in computer vision, speech and NLP. In combination with previously proposed solutions for 4-bit quantization of weight and activation tensors, 4-bit training shows non-significant loss in accuracy across application domains while enabling significant hardware acceleration (&gt;7 over state of the art FP16 systems)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Understanding_straight-through_estimator_in_training_activation_quantized_neural_nets/","title":"Understanding straight-through estimator in training activation quantized neural nets","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Understanding_straight-through_estimator_in_training_activation_quantized_neural_nets/#summary","title":"Summary","text":"<p> The paper discusses the issue of a vanishing gradient when training activation quantized neural networks and proposes the use of a straight-through estimator (STE) in the backward pass to provide a non-trivial gradient. The paper provides theoretical justification for the concept of STE and warns that a poor choice of STE may lead to instability in the training algorithm near certain local minima as confirmed by CIFAR-10 experiments."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Understanding_straight-through_estimator_in_training_activation_quantized_neural_nets/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Understanding_straight-through_estimator_in_training_activation_quantized_neural_nets/#content","title":"Content","text":"<p> Training activation quantized neural networks involves minimizing a piecewise constant function whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE) in the backward pass only, so that the \u201cgradient\u201d through the modified chain rule becomes non-trivial. In this paper, we provide the theoretical justification of the concept of STE by answering this question. We further show that a poor choice of STE leads to instability of the training algorithm near certain local minima, which is verified with CIFAR-10 experiments."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Understanding_the_impact_of_quantization%2C_accuracy%2C_and_radiation_on_the_reliability_of_convolutional_neural_networks_on_FPGAs/","title":"Understanding the impact of quantization, accuracy, and radiation on the reliability of convolutional neural networks on FPGAs","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Understanding_the_impact_of_quantization%2C_accuracy%2C_and_radiation_on_the_reliability_of_convolutional_neural_networks_on_FPGAs/#summary","title":"Summary","text":"<p>Summary: The paper analyzes the impact of binary quantization on the convolutional layers of neural networks on FPGAs in terms of radiation cross-section, model accuracy, resource utilization, and error criticality. The design with quantized convolutional layers can be 39% less sensitive to radiation, and the portion of errors considered critical in the network is increased by 12%. Generic equations are derived to model the overall failure rate of neural networks. The Modified National Institute of Standards and Technology (MNIST) CNN is used as a case study to evaluate trade-offs between various factors. Fault injection experiments are conducted to determine which portions of the CNN\u2019s circuit generate tolerable or critical errors due to the vulnerability of the feature extraction phase, and the error rate in a given radiation environment is estimated using experimental data.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Understanding_the_impact_of_quantization%2C_accuracy%2C_and_radiation_on_the_reliability_of_convolutional_neural_networks_on_FPGAs/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Understanding_the_impact_of_quantization%2C_accuracy%2C_and_radiation_on_the_reliability_of_convolutional_neural_networks_on_FPGAs/#content","title":"Content","text":"<p> Convolutional neural networks (CNNs) implemented on field-programmable gate arrays (FPGAs) are becoming promising solutions for safety-critical applications. However, radiation-induced errors, such as single-event upsets (SEUs), can affect the reliability of static random-access memory (SRAM)-based FPGAs. In this paper, we analyze the impact of binary quantization on the convolutional layers of neural networks on FPGAs, especially in terms of radiation cross-section, model accuracy, resource utilization, and error criticality. We find that a design with quantized convolutional layers can be 39% less sensitive to radiation, and the portion of errors considered critical in the network is increased by 12%. Additionally, we derive generic equations that consider both accuracy and radiation in order to model the overall failure rate of neural networks. We use a case study of the Modified National Institute of Standards and Technology (MNIST) CNN to evaluate the trade-offs between various factors. Through extensive fault injection experiments, we determine which portions of the CNN\u2019s circuit are more likely to generate tolerable or critical errors at the output due to the vulnerability of the feature extraction phase. We also estimate the error rate in a given radiation environment using experimental data."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Universally_quantized_neural_compression/","title":"Universally quantized neural compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Universally_quantized_neural_compression/#summary","title":"Summary","text":"<p>Summary: The paper discusses using a uniform noise channel for both training and testing phases of a lossy compression model. It explains how this approach is more efficient and easier to implement than other methods, and demonstrates how quantization can be obtained through a soft quantizer applied to the uniform noise channel.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Universally_quantized_neural_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Universally_quantized_neural_compression/#content","title":"Content","text":"<p> A popular approach to learning encoders for lossy compression is to use additive uniform noise during training as a differentiable approximation to test-time quantization. We demonstrate that a uniform noise channel can also be implemented at test time using universal quantization (Ziv, 1985). This allows us to eliminate the mismatch between training and test phases while maintaining a completely differentiable loss function. Implementing the uniform noise channel is a special case of the more general problem of communicating a sample, which we prove is computationally hard if we do not make assumptions about its distribution. However, the uniform special case is efficient as well as easy to implement and thus of great interest from a practical point of view. Finally, we show that quantization can be obtained as a limiting case of a soft quantizer applied to the uniform noise channel, bridging compression with and without quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Unsupervised_texture_classification_using_vector_quantization_and_deterministic_relaxation_neural_network/","title":"Unsupervised texture classification using vector quantization and deterministic relaxation neural network","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Unsupervised_texture_classification_using_vector_quantization_and_deterministic_relaxation_neural_network/#summary","title":"Summary","text":"<p>Summary: The paper proposes using a neural network architecture to classify textured images in an unsupervised manner that incorporates image-specific constraints. The model includes feature quantization, partition, and competition processes using Gabor filters arranged as a set of wavelet bases. A derived energy function represented by the neural network with a set of label constraints for each pixel in the image is used to adjust the state of the network and vector quantizer until a stable state is reached, which provides a classification of the textured image.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Unsupervised_texture_classification_using_vector_quantization_and_deterministic_relaxation_neural_network/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Unsupervised_texture_classification_using_vector_quantization_and_deterministic_relaxation_neural_network/#content","title":"Content","text":"<p>This paper describes a neural network architecture used to classify textured images in an unsupervised manner, while incorporating image-specific constraints. The features are extracted using a set of 2D Gabor filters arranged as a set of wavelet bases. The classification model includes feature quantization, partition, and competition processes, and is determined by a derived energy function represented by a neural network, using a set of label constraints for each pixel in the image. The state of the network and vector quantizer are adjusted using a deterministic relaxation procedure until a stable state is reached, and the final equilibrium state of the vector quantizer gives a classification of the textured image."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Up_or_down%3F_adaptive_rounding_for_post-training_quantization/","title":"Up or down? adaptive rounding for post-training quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Up_or_down%3F_adaptive_rounding_for_post-training_quantization/#summary","title":"Summary","text":"<p>Summary:  This paper presents AdaRound, a weight-rounding mechanism for post-training quantization that is adaptable to the data and task loss. The proposed method outperforms the predominant approach of rounding weights to the nearest fixed-point value and achieved a new state-of-the-art for post-training quantization on several networks and tasks. The method is fast and does not require fine-tuning of the network, and only uses a small amount of unlabelled data. The weights of Resnet18 and Resnet50 can be quantized to 4 bits without fine-tuning while staying within an accuracy loss of 1%.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Up_or_down%3F_adaptive_rounding_for_post-training_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Up_or_down%3F_adaptive_rounding_for_post-training_quantization/#content","title":"Content","text":"<p> When quantizing neural networks, assigning each \ufb02oating-point weight to its nearest \ufb01xed-point value is the predominant approach. We \ufb01nd that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without \ufb01ne-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1%."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Urban_driving_with_multi-objective_deep_reinforcement_learning/","title":"Urban driving with multi-objective deep reinforcement learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Urban_driving_with_multi-objective_deep_reinforcement_learning/#summary","title":"Summary","text":"<p>The paper presents a deep learning variant of thresholded lexicographic Q-learning for urban driving task, where a multi-objective DQN agent learns to drive on multi-lane roads and intersections obeying traffic rules. The authors also propose an extension for factored Markov Decision Processes to the DQN architecture, which significantly improves data efficiency. They demonstrate that the learned policy can zero-shot transfer to a ring road without sacrificing performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Urban_driving_with_multi-objective_deep_reinforcement_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Urban_driving_with_multi-objective_deep_reinforcement_learning/#content","title":"Content","text":"<p> Autonomous driving is a challenging domain that entails multiple aspects: a vehicle should be able to drive to its destination as fast as possible while avoiding collision, obeying traffic rules and ensuring the comfort of passengers. In this paper, we present a deep learning variant of thresholded lexicographic Q-learning for the task of urban driving. Our multi-objective DQN agent learns to drive on multi-lane roads and intersections, yielding and changing lanes according to traffic rules. We also propose an extension for factored Markov Decision Processes to the DQN architecture that provides auxiliary features for the Q function. This is shown to significantly improve data efficiency. We then show that the learned policy is able to zero-shot transfer to a ring road without sacrificing performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Using_quantization-aware_training_technique_with_post-training_fine-tuning_quantization_to_implement_a_mobilenet_hardware_accelerator/","title":"Using quantization-aware training technique with post-training fine-tuning quantization to implement a mobilenet hardware accelerator","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Using_quantization-aware_training_technique_with_post-training_fine-tuning_quantization_to_implement_a_mobilenet_hardware_accelerator/#summary","title":"Summary","text":"<p>The paper presented at the Indo \u2013 Taiwan 2nd International Conference on Computing, Analytics and Networks implemented a MobileNet hardware accelerator for low power consumption and energy efficiency suitable for edge devices in IoT applications using quantization-aware training technique with post-training fine-tuning quantization. The proposed MobileNet hardware accelerator can achieve low power consumption making it apt for edge devices."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Using_quantization-aware_training_technique_with_post-training_fine-tuning_quantization_to_implement_a_mobilenet_hardware_accelerator/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Using_quantization-aware_training_technique_with_post-training_fine-tuning_quantization_to_implement_a_mobilenet_hardware_accelerator/#content","title":"Content","text":"<p>In this paper presented at the Indo \u2013 Taiwan 2nd International Conference on Computing, Analytics and Networks, a hardware accelerator for the MobileNet model is implemented using the quantization-aware training technique with post-training fine-tuning quantization to achieve low power consumption and energy efficiency suitable for edge devices in internet of things (IoT) applications. The proposed MobileNet hardware accelerator can achieve low power consumption and is suitable for the edge devices."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Utilizing_explainable_AI_for_quantization_and_pruning_of_deep_neural_networks/","title":"Utilizing explainable AI for quantization and pruning of deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Utilizing_explainable_AI_for_quantization_and_pruning_of_deep_neural_networks/#summary","title":"Summary","text":"<p>Summary: The use of Deep Neural Networks (DNNs) in various applications requires optimization for factors such as energy consumption, memory requirement, and throughput. DNN compression can reduce memory footprint and complexity before deployment on hardware. A new research area, called explainable AI, has emerged from recent efforts to understand and explain AI methods.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Utilizing_explainable_AI_for_quantization_and_pruning_of_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Utilizing_explainable_AI_for_quantization_and_pruning_of_deep_neural_networks/#content","title":"Content","text":"<p>For many applications, utilizing Deep Neural Networks (DNNs) requires their implementation on a target architecture in an optimized manner concerning energy consumption, memory requirement, throughput, etc. DNN compression is used to reduce the memory footprint and complexity of a DNN before its deployment on hardware. Recent efforts to understand and explain AI (Artificial Intelligence) methods have led to a new research area, termed as explainable AI."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/VC_dimension_of_partially_quantized_neural_networks_in_the_overparametrized_regime/","title":"VC dimension of partially quantized neural networks in the overparametrized regime","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/VC_dimension_of_partially_quantized_neural_networks_in_the_overparametrized_regime/#summary","title":"Summary","text":"<p>The paper discusses the effectiveness of hyperplane arrangement neural networks (HANNs) as partially quantized neural networks in terms of their Vapnik-Chervonenkis (VC) dimension and expressivity. The authors empirically demonstrate the performance of overparameterized HANNs on several datasets, showing that they can match the performance of full-precision models. Furthermore, they show that empirical risk minimization over HANNs achieves the minimax rate for classification with Lipschitz posterior class probability. Finally, the paper suggests future research directions related to overparameterized neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/VC_dimension_of_partially_quantized_neural_networks_in_the_overparametrized_regime/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/VC_dimension_of_partially_quantized_neural_networks_in_the_overparametrized_regime/#content","title":"Content","text":"<p>Using a sample compression analysis, this research paper explores a class of partially quantized neural networks called hyperplane arrangement neural networks (HANNs) and shows that they can have Vapnik-Chervonenkis (VC) dimension significantly smaller than the number of weights, while being highly expressive. The authors demonstrate the expressivity of HANNs empirically on a panel of 121 UCI datasets, where overparameterized HANNs match the performance of state-of-the-art full-precision models. The authors also show that empirical risk minimization over HANNs achieves the minimax rate for classification with Lipschitz posterior class probability. The paper highlights the generalization puzzle of overparameterized neural networks and suggests directions for future research."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/VQ-GNN%3A_A_Universal_Framework_to_Scale_up_Graph_Neural_Networks_using_Vector_Quantization/","title":"VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/VQ-GNN%3A_A_Universal_Framework_to_Scale_up_Graph_Neural_Networks_using_Vector_Quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel approach, VQ-GNN, to address the scaling problem of Graph Neural Networks (GNNs) using Vector Quantization (VQ). Unlike sampling-based methods, VQ-GNN can preserve all the messages passed to a mini-batch of nodes by learning a small number of quantized reference vectors of global node representations within each GNN layer. The low-rank version of the convolution matrix combined with VQ can avoid the \u201cneighbor explosion\u201d problem of GNNs. The authors demonstrate the scalability and competitive performance of VQ-GNN on large-graph node classification and link prediction benchmarks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/VQ-GNN%3A_A_Universal_Framework_to_Scale_up_Graph_Neural_Networks_using_Vector_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/VQ-GNN%3A_A_Universal_Framework_to_Scale_up_Graph_Neural_Networks_using_Vector_Quantization/#content","title":"Content","text":"<p>Most state-of-the-art Graph Neural Networks (GNNs) can be de\ufb01ned as a form of graph convolution which can be realized by message passing between direct neighbors or beyond. To scale such GNNs to large graphs, various neighbor-, layer-, or subgraph-sampling techniques are proposed to alleviate the \u201cneighbor explosion\u201d problem by considering only a small subset of messages passed to the nodes in a mini-batch. However, sampling-based methods are dif\ufb01cult to apply to GNNs that utilize many-hops-away or global context each layer, show unstable performance for different tasks and datasets, and do not speed up model inference. We propose a principled and fundamentally different approach, VQ-GNN, a universal framework to scale up any convolution-based GNNs using Vector Quantization (VQ) without compromising the performance. In contrast to sampling-based techniques, our approach can effectively preserve all the messages passed to a mini-batch of nodes by learning and updating a small number of quantized reference vectors of global node representations, using VQ within each GNN layer. Our framework avoids the \u201cneighbor explosion\u201d problem of GNNs using quantized representations combined with a low-rank version of the graph convolution matrix. We show that such a compact low-rank version of the gigantic convolution matrix is suf\ufb01cient both theoretically and experimentally. In company with VQ, we design a novel approximated message passing algorithm and a nontrivial back-propagation rule for our framework. Experiments on various types of GNN backbones demonstrate the scalability and competitive performance of our framework on large-graph node classi\ufb01cation and link prediction benchmarks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Value-aware_quantization_for_training_and_inference_of_neural_networks/","title":"Value-aware quantization for training and inference of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Value-aware_quantization_for_training_and_inference_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel value-aware quantization method that reduces precision for majority of data while handling small amount of large values in high precision, leading to reduced quantization errors. The proposed method offers significant memory cost reductions for activations in ResNet-152 and Inception-v3 compared to state-of-the-art methods, while maintaining the same training accuracy with 3-bit activations. The experiments also demonstrate that deep networks like Inception-v3, ResNet-101 and DenseNet-121 can be quantized for inference with 4-bit weights and activations within 1% top-1 accuracy drop.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Value-aware_quantization_for_training_and_inference_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Value-aware_quantization_for_training_and_inference_of_neural_networks/#content","title":"Content","text":"<p> We propose a novel value-aware quantization which applies aggressively reduced precision to the majority of data while separately handling a small amount of large values in high precision, which reduces total quantization errors under very low precision. We present new techniques to apply the proposed quantization to training and inference. The experiments show that our method with 3-bit activations (with 2% of large ones) can give the same training accuracy as full-precision one while offering significant (41.6% and 53.7%) reductions in the memory cost of activations in ResNet-152 and Inception-v3 compared with the state-of-the-art method. Our experiments also show that deep networks such as Inception-v3, ResNet-101 and DenseNet-121 can be quantized for inference with 4-bit weights and activations (with 1% 16-bit data) within 1% top-1 accuracy drop."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Variable_weighted_convolutional_neural_network_for_the_nitrogen_content_quantization_of_Masson_pine_seedling_leaves_with_near-infrared_spectroscopy/","title":"Variable weighted convolutional neural network for the nitrogen content quantization of Masson pine seedling leaves with near-infrared spectroscopy","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Variable_weighted_convolutional_neural_network_for_the_nitrogen_content_quantization_of_Masson_pine_seedling_leaves_with_near-infrared_spectroscopy/#summary","title":"Summary","text":"<p>Summary: The paper proposes the use of spectroscopy to predict the nitrogen content of Masson pine seedling leaves. An improved 1D convolutional neural network architecture, VWCNN, is developed to obtain better prediction accuracy and robustness. VWCNN outperforms traditional shallow prediction models and other CNN-based models, achieving high R2 and low RMSE values in both training and test datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Variable_weighted_convolutional_neural_network_for_the_nitrogen_content_quantization_of_Masson_pine_seedling_leaves_with_near-infrared_spectroscopy/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Variable_weighted_convolutional_neural_network_for_the_nitrogen_content_quantization_of_Masson_pine_seedling_leaves_with_near-infrared_spectroscopy/#content","title":"Content","text":"<p>Spectroscopy is a powerful non-destructive quantization tool. In this paper, the technology is used to predict the nitrogen content of Masson pine seedling leaves. To establish a better prediction model, an improved 1D convolutional neural network architecture, named the variable weighted convolutional neural network (VWCNN), is proposed in this research. For 219 fresh Masson pine seedling leaves, it shows better results in the prediction accuracy and robustness compared to those derived from the traditional shallow prediction model and other CNN-based models. VWCNN achieves a 0.984 R2 and 0.038 RMSE value in the training dataset and 0.925 R2 value and 0.075 RMSE value in the test dataset."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Vector-quantized_image_modeling_with_improved_VQGAN/","title":"Vector-quantized image modeling with improved VQGAN","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Vector-quantized_image_modeling_with_improved_VQGAN/#summary","title":"Summary","text":"<p>The paper explores the Vector-quantized Image Modeling (VIM) approach that includes pretraining a Transformer to predict rasterized image tokens autoregressively. The learning of discrete image tokens is done from a learned Vision-Transformer-based VQGAN. The proposed method achieves Inception Score (IS) of 175.1 and Fr\u00e9chet Inception Distance (FID) of 4.17 on ImageNet at 256x256 resolution, which is a significant improvement over vanilla VQGAN. The pretrained Transformer by averaging intermediate features performs well and outperforms iGPT-XL."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Vector-quantized_image_modeling_with_improved_VQGAN/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Vector-quantized_image_modeling_with_improved_VQGAN/#content","title":"Content","text":"<p>We explore a Vector-quantized Image Modeling (VIM) approach that includes pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN (ViT-VQGAN). Multiple improvements to vanilla VQGAN from architecture to codebook learning are proposed, resulting in better efficiency and reconstruction fidelity. The improved ViT-VQGAN further improves vector-quantized image modeling tasks, including unconditional, class-conditioned image generation, and unsupervised representation learning. When trained on ImageNet at 256x256 resolution, the proposed method achieves Inception Score (IS) of 175.1 and Fr\u00e9chet Inception Distance (FID) of 4.17, a significant improvement over vanilla VQGAN. Pretrained Transformer by averaging intermediate features also performs well and outperforms iGPT-XL."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Verifying_Quantized_Neural_Networks_using_SMT-Based_Model_Checking/","title":"Verifying Quantized Neural Networks using SMT-Based Model Checking","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Verifying_Quantized_Neural_Networks_using_SMT-Based_Model_Checking/#summary","title":"Summary","text":"<p> The paper discusses the development and evaluation of a symbolic verification framework for Artificial Neural Networks (ANNs) through software model checking and satisfiability modulo theories to address concerns relating to their reliability, black-box nature, and sensitivity to quantization errors when deployed in safety-critical systems. The verification approach was found to analyze larger ANN implementations and reduce the verification time compared to state-of-the-art techniques, also providing formal guarantees on their safe behavior."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Verifying_Quantized_Neural_Networks_using_SMT-Based_Model_Checking/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Verifying_Quantized_Neural_Networks_using_SMT-Based_Model_Checking/#content","title":"Content","text":"<p>Artificial Neural Networks (ANNs) are being deployed for an increasing number of safety-critical applications, including autonomous cars and medical diagnosis. However, concerns about their reliability have been raised due to their black-box nature and apparent fragility to adversarial attacks. These concerns are amplified when ANNs are deployed on restricted system, which limit the precision of mathematical operations and thus introduce additional quantization errors. Here, we develop and evaluate a novel symbolic verification framework using software model checking (SMC) and satisfiability modulo theories (SMT) to check for vulnerabilities in ANNs. More specifically, we propose several ANN-related optimizations for SMC, including invariant inference via interval analysis, slicing, expression simplifications, and discretization of non-linear activation functions. With this verification framework, we can provide formal guarantees on the safe behavior of ANNs implemented both in floating- and fixed-point arithmetic. In this regard, our verification approach was able to verify and produce adversarial examples for 52test cases spanning image classification and general machine learning applications. Furthermore, for small- to medium-sized ANN, our approach completes most of its verification runs in minutes. Moreover, in contrast to most state-of-the-art methods, our approach is not restricted to specific choices regarding activation functions and non-quantized representations. Our experiments show that our approach can analyze larger ANN implementations and substantially reduce the verification time compared to state-of-the-art techniques that use SMT solving."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Video_emotion_recognition_with_transferred_deep_feature_encodings/","title":"Video emotion recognition with transferred deep feature encodings","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Video_emotion_recognition_with_transferred_deep_feature_encodings/#summary","title":"Summary","text":"<p>This paper tackles the challenge of emotion understanding for user-generated videos by transferring deep feature encodings. It presents a framework for large-scale video emotion recognition, including zero-shot emotion recognition. The paper proposes a novel auxiliary Image Transfer Encoding (ITE) process to efficiently encode and generate video representation, and investigates different configurations of convolutional neural networks. Comprehensive experiments on multiple datasets demonstrate the effectiveness of the framework."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Video_emotion_recognition_with_transferred_deep_feature_encodings/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Video_emotion_recognition_with_transferred_deep_feature_encodings/#content","title":"Content","text":"<p>Despite growing research interest, emotion understanding for user-generated videos remains a challenging problem. Major obstacles include the diversity and complexity of video content, as well as the sparsity of expressed emotions. For the first time, we systematically study large-scale video emotion recognition by transferring deep feature encodings. In addition to the traditional, supervised recognition, we study the problem of zero-shot emotion recognition, where emotions in the test set are unseen during training. To cope with this task, we utilize knowledge transferred from auxiliary image and text corpora. A novel auxiliary Image Transfer Encoding (ITE) process is proposed to efficiently encode and generate video representation. We also thoroughly investigate different configurations of convolutional neural networks. Comprehensive experiments on multiple datasets demonstrate the effectiveness of our framework."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Vs-quant%3A_Per-vector_scaled_quantization_for_accurate_low-precision_neural_network_inference/","title":"Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Vs-quant%3A_Per-vector_scaled_quantization_for_accurate_low-precision_neural_network_inference/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new method of per-vector scaled quantization to accelerate deep neural networks through quantization without resulting in accuracy degradation. The method uses a separate scale factor for each small vector with low-bitwidth integers and achieves better inference accuracy at low precision compared to conventional scaling techniques for popular neural networks. The method resulted in energy and area saving over an 8-bit baseline and maintained over 75% accuracy for ResNet50 on ImageNet and achieved near-full-precision accuracy for both BERT-base and BERT-large on SQuAD without requiring retraining.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Vs-quant%3A_Per-vector_scaled_quantization_for_accurate_low-precision_neural_network_inference/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Vs-quant%3A_Per-vector_scaled_quantization_for_accurate_low-precision_neural_network_inference/#content","title":"Content","text":"<p>Quantization is a way of accelerating deep neural networks that reduces memory footprint and exploits low-cost integer math hardware. However, excessive quantization can cause accuracy degradation. To address this, a new method called per-vector scaled quantization is proposed, which uses a separate scale factor for each small vector of (16-64) elements within a single dimension of a tensor. To achieve an efficient hardware implementation, the per-vector scale factors can be implemented with low-bitwidth integers when calibrated using a two-level quantization scheme. The method achieves better inference accuracy at low precision compared to conventional scaling techniques for popular neural networks without requiring retraining. It also resulted in energy saving and area saving over an 8-bit baseline, while maintaining over 75% accuracy for ResNet50 on ImageNet, and achieving near-full-precision accuracy for both BERT-base and BERT-large on SQuAD."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weakly_supervised_deep_hyperspherical_quantization_for_image_retrieval/","title":"Weakly supervised deep hyperspherical quantization for image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weakly_supervised_deep_hyperspherical_quantization_for_image_retrieval/#summary","title":"Summary","text":"<p>This paper proposes a new method called Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ) that learns deep quantization from weakly tagged images. The proposed method achieved better performance on weakly-supervised compact coding compared to existing models that heavily rely on ground-truth information."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weakly_supervised_deep_hyperspherical_quantization_for_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weakly_supervised_deep_hyperspherical_quantization_for_image_retrieval/#content","title":"Content","text":"<p>Deep quantization methods have shown high ef\ufb01ciency on large-scale image retrieval. However, current models heavily rely on ground-truth information, hindering the application of quantization in label-hungry scenarios. A more realistic demand is to learn from inexhaustible uploaded images that are associated with informal tags provided by amateur users. To this end, we propose Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the \ufb01rst work to learn deep quantization from weakly tagged images. Exten-sive experiments show that WSDHQ can achieve state-of-art performance on weakly-supervised compact coding."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weight_equalizing_shift_scaler-coupled_post-training_quantization/","title":"Weight equalizing shift scaler-coupled post-training quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weight_equalizing_shift_scaler-coupled_post-training_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a weight equalizing shift scaler to mitigate the accuracy degradation that occurs during neural network model quantization, especially for the MobileNet family. The method rescales the per-channel weight range prior to layer-wise quantization and efficiently fuses inverse binary shifting to recover the original output range in the fixed-computing convolutional operator. The proposed method significantly improved the accuracy performance without increasing the memory footprint and achieved competitive results compared to channel-wise quantization in varying network models and tasks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weight_equalizing_shift_scaler-coupled_post-training_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weight_equalizing_shift_scaler-coupled_post-training_quantization/#content","title":"Content","text":"<p> Post-training, layer-wise quantization is preferable because it is free from retraining and is hardware-friendly. Nevertheless, accuracy degradation has occurred when a neural network model has a big difference of per-out-channel weight ranges. In particular, the MobileNet family has a tragedy drop in top-1 accuracy from 70.60% \u001871.87% to 0.1% on the ImageNet dataset after 8-bit weight quantization. To mitigate this significant accuracy reduction, we propose a new weight equalizing shift scaler, i.e. rescaling the weight range per channel by a 4-bit binary shift, prior to a layer-wise quantization. To recover the original output range, inverse binary shifting is efficiently fused to the existing per-layer scale compounding in the fixed-computing convolutional operator of the custom neural processing unit. The binary shift is a key feature of our algorithm, which significantly improved the accuracy performance without impeding the memory footprint. As a result, our proposed method achieved a top-1 accuracy of 69.78% \u001870.96% in MobileNets and showed robust performance in varying network models and tasks, which is competitive to channel-wise quantization results."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weight_normalization_based_quantization_for_deep_neural_network_compression/","title":"Weight normalization based quantization for deep neural network compression","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weight_normalization_based_quantization_for_deep_neural_network_compression/#summary","title":"Summary","text":"<p>  The paper proposes a new method called weight normalization based quantization (WNQ) for deep learning model compression. WNQ uses weight normalization to reduce the quantization error and improve performance. The experiments conducted on CIFAR-100 and ImageNet datasets show that WNQ outperforms other baseline methods and achieves state-of-the-art performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weight_normalization_based_quantization_for_deep_neural_network_compression/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weight_normalization_based_quantization_for_deep_neural_network_compression/#content","title":"Content","text":"<p>In this paper, we propose a novel quantization method, called weight normalization based quantization (WNQ), for model compression. WNQ adopts weight normalization to avoid the long-tail distribution of network weights and subsequently reduces the quantization error. Experiments on CIFAR-100 and ImageNet show that WNQ can outperform other baselines to achieve state-of-the-art performance."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weighted-entropy-based_quantization_for_deep_neural_networks/","title":"Weighted-entropy-based quantization for deep neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weighted-entropy-based_quantization_for_deep_neural_networks/#summary","title":"Summary","text":"<p>This paper proposes a method for multi-bit quantization of neural network models based on weighted entropy. The proposed approach achieves significant reductions in model size and computation with minimal accuracy loss, while providing higher accuracy and requiring lower design effort compared to existing quantization schemes. The method allows for more flexible exploitation of accuracy-performance trade-off and provides an automated quantization flow based on conventional training algorithms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weighted-entropy-based_quantization_for_deep_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weighted-entropy-based_quantization_for_deep_neural_networks/#content","title":"Content","text":"<p>Quantization is a method for optimizing the inference cost of neural network models for deployment to mobile and embedded systems with resource constraints. In this paper, we propose a method for quantizing weights and activations based on the concept of weighted entropy. Our approach is multi-bit quantization, which allows for more flexible exploitation of accuracy-performance trade-off. Our scheme provides automated quantization flow based on conventional training algorithms and achieves significant reductions in model size and computation with minimal accuracy loss. Compared to existing quantization schemes, ours provides higher accuracy with a similar resource constraint and requires much lower design effort."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weighted_quantization-regularization_in_DNNs_for_weight_memory_minimization_toward_HW_implementation/","title":"Weighted quantization-regularization in DNNs for weight memory minimization toward HW implementation","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weighted_quantization-regularization_in_DNNs_for_weight_memory_minimization_toward_HW_implementation/#summary","title":"Summary","text":"<p>The paper proposes weight quantization to optimize weight memory while converting weights to hardware-friendly data types to deploy neural networks on hardware platforms with limited memory and computational power. Dynamic fixed point and power-of-two quantization techniques are combined with layer-wise precision scaling and quantization-aware fine-tuning. Different bit-widths are allowed for each layer, and retraining for Po2 quantization allows for higher compression rates. The approach achieves compression ratios of 7.34 for CIFAR-10, 4.7 for CIFAR-100, and 9.33 for SVHN with accuracy degradation of only 0.10% points, as verified on an all-convolutional network."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weighted_quantization-regularization_in_DNNs_for_weight_memory_minimization_toward_HW_implementation/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Weighted_quantization-regularization_in_DNNs_for_weight_memory_minimization_toward_HW_implementation/#content","title":"Content","text":"<p>Deployment of deep neural networks on hardware platforms is often constrained by limited on-chip memory and computational power. The proposed weight quantization offers the possibility of optimizing weight memory alongside transforming the weights to hardware friendly data types. We apply dynamic fixed point (DFP) and power-of-two (Po2) quantization in conjunction with layer-wise precision scaling to minimize the weight memory. To alleviate accuracy degradation due to precision scaling, we employ quantization-aware fine-tuning. For fine-tuning, quantization-regularization (QR) and weighted QR are introduced to force the trained quantization by adding the distance of the weights to the desired quantization levels as a regularization term to the loss-function. While DFP quantization performs better when allowing different bit-widths for each layer, Po2 quantization in combination with retraining allows higher compression rates for equal bit-width quantization. The techniques are verified on an all-convolutional network. With accuracy degradation of 0.10% points, for DFP with layer-wise precision scaling we achieve compression ratios of 7.34 for CIFAR-10, 4.7 for CIFAR-100, and 9.33 for SVHN dataset. Index Terms \u2014Convolutional neural networks, memory minimization, quantization, regularization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/What_can_we_learn_from_misclassified_ImageNet_images%3F/","title":"What can we learn from misclassified ImageNet images?","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/What_can_we_learn_from_misclassified_ImageNet_images%3F/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Superclassing ImageNet dataset, a subset of ImageNet, to help find patterns of misclassification. It is found that misclassifications are mainly among subclasses within a superclass. A two-stage Super-Sub framework is proposed which improves overall classification performance and reduces parameter storage cost using delta and quantization aware training techniques. This framework is more scalable and generalizable than simply scaling up a vanilla network in size.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/What_can_we_learn_from_misclassified_ImageNet_images%3F/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/What_can_we_learn_from_misclassified_ImageNet_images%3F/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/What_do_compressed_deep_neural_networks_forget%3F/","title":"What do compressed deep neural networks forget?","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/What_do_compressed_deep_neural_networks_forget%3F/#summary","title":"Summary","text":"<p> The paper discusses the significance of deep neural network pruning and quantization techniques in achieving high levels of compression without significant degradation to test set accuracy. It is suggested that although the models have comparable top-line performance metrics, there are considerable differences in behavior for a small subset of data points that are impacted by sparsity. The paper provides a formal framework to audit the disparate harm incurred by compression, and highlights the need to understand this impact, considering the widespread deployment of compressed models in the wild."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/What_do_compressed_deep_neural_networks_forget%3F/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/What_do_compressed_deep_neural_networks_forget%3F/#content","title":"Content","text":"<p>  Deep neural network pruning and quantization techniques have demonstrated it is possible to achieve high levels of compression with surprisingly little degradation to test set accuracy. However, this measure of performance conceals signi\ufb01cant differences in how different classes and images are impacted by model compression techniques. We \ufb01nd that models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset. This small subset of data points, which we term Pruning Identi\ufb01ed Exemplars (PIEs), are systematically more impacted by the introduction of sparsity. Our work is the \ufb01rst to provide a formal framework for auditing the disparate harm incurred by compression and a way to quantify the trade- offs involved. An understanding of this disparate impact is critical given the widespread deployment of compressed models in the wild."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Xnor-net%2B%2B%3A_Improved_binary_neural_networks/","title":"Xnor-net++: Improved binary neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Xnor-net%2B%2B%3A_Improved_binary_neural_networks/#summary","title":"Summary","text":"<p>The paper presents an improved training algorithm for binary neural networks that combines the scaling factors of weights and activations into a single factor. This factor is learned discriminatively through backpropagation, and its shape is constructed in a few different ways while keeping the computational budget the same. The approach outperforms XNOR-Net in accuracy up to 6% within the same computational budget and is tested on the challenging task of ImageNet classification."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Xnor-net%2B%2B%3A_Improved_binary_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Xnor-net%2B%2B%3A_Improved_binary_neural_networks/#content","title":"Content","text":"<p>This paper proposes an improved training algorithm for binary neural networks in which both weights and activations are binary numbers. The authors argue that analytic calculation of the real-valued scaling factors used in the current state-of-the-art method of XNOR-Net is sub-optimal. Instead, the authors propose to fuse the activation and weight scaling factors into a single one that is learned discriminatively via backpropagation. They also explore several ways of constructing the shape of the scale factors while keeping the computational budget fixed, and empirically measure the accuracy of their approximations. They show that their approach significantly outperforms XNOR-Net within the same computational budget when tested on the challenging task of ImageNet classification, offering up to 6% accuracy gain."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Xpulpnn%3A_Enabling_energy_efficient_and_flexible_inference_of_quantized_neural_networks_on_risc-v_based_iot_end_nodes/","title":"Xpulpnn: Enabling energy efficient and flexible inference of quantized neural networks on risc-v based iot end nodes","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Xpulpnn%3A_Enabling_energy_efficient_and_flexible_inference_of_quantized_neural_networks_on_risc-v_based_iot_end_nodes/#summary","title":"Summary","text":"<p> This paper introduces lightweight extensions to the RISC-V ISA to improve the efficiency of heavily quantized neural network inference on microcontroller-class cores. By extending the ISA with nibble and crumb SIMD instructions, the authors demonstrated near-linear speedup with respect to higher precision integer computation for QNN computation, and a custom execution paradigm for SIMD sum-of-dot-product operations which brings an improvement in peak MAC/cycle. The proposed solution achieves efficiency levels comparable with dedicated DNN inference accelerators and up to three orders of magnitude better than state-of-the-art ARM Cortex-M based microcontroller systems."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Xpulpnn%3A_Enabling_energy_efficient_and_flexible_inference_of_quantized_neural_networks_on_risc-v_based_iot_end_nodes/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Xpulpnn%3A_Enabling_energy_efficient_and_flexible_inference_of_quantized_neural_networks_on_risc-v_based_iot_end_nodes/#content","title":"Content","text":"<p>Strongly quantized fixed-point arithmetic is now considered a well-established solution to deploy Convolutional Neural Networks (CNNs) on limited-memory low-power IoT end-nodes. Emerging open-source ISAs such as RISC-V provide a flexible way to address challenges in low bitwidth fixed-point instructions. This work introduces lightweight extensions to the RISC-V ISA to boost the efficiency of heavily quantized neural network (QNN) inference on microcontroller-class cores. By extending the ISA with nibble and crumb SIMD instructions, the authors are able to show near-linear speedup with respect to higher precision integer computation on the key kernels for QNN computation. Additionally, the authors propose a custom execution paradigm for SIMD sum-of-dot-product operations, which consists of fusing a dot product with a load operation, and show an up to 1.64 peak MAC/cycle improvement compared to a standard execution scenario. The proposed solution achieves efficiency levels comparable with dedicated DNN inference accelerators and up to three orders of magnitude better than state-of-the-art ARM Cortex-M based microcontroller systems such as the low-end STM32L4 MCU and the high-end STM32H7 MCU."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/YOLO-Based_Face_Mask_Detection_on_Low-End_Devices_Using_Pruning_and_Quantization/","title":"YOLO-Based Face Mask Detection on Low-End Devices Using Pruning and Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/YOLO-Based_Face_Mask_Detection_on_Low-End_Devices_Using_Pruning_and_Quantization/#summary","title":"Summary","text":"<p>The paper talks about the use of pruning and quantization techniques to compress a Deep Learning object detection model based on YOLOv4 to recognize the presence of face masks, to be deployed on a Raspberry Pi 4. The performance is measured in terms of Mean Average Precision (mAP) and FPS. The results suggest that with proper pruning and quantization, the FPS can be doubled with a moderate loss in mAP."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/YOLO-Based_Face_Mask_Detection_on_Low-End_Devices_Using_Pruning_and_Quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/YOLO-Based_Face_Mask_Detection_on_Low-End_Devices_Using_Pruning_and_Quantization/#content","title":"Content","text":"<p>Deploying Deep Learning (DL) based object detection (OD) models in low-end devices may lead to poor performance in terms of frames-per-second (FPS). Pruning and quantization are well-known compression techniques that can potentially lead to a reduction of the computational burden of a DL model, with a possible decrease of performance in terms of detection accuracy. Motivated by the widespread introduction of face mask mandates by many institutions during the Covid-19 pandemic, we aim at training and compressing an OD model based on YOLOv4 to recognize the presence of face masks, to be deployed on a Raspberry Pi 4. We investigate the capability of different kinds of pruning and quantization techniques of increasing the FPS with respect to the uncompressed model, while retaining the detection accuracy. We quantitatively assess the pruned and quantized models in terms of Mean Average Precision (mAP) and FPS, and show that with proper pruning and quantization, the FPS can be doubled with a moderate loss in mAP. The results provide guidelines for compression of other OD models based on YOLO."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/YOLOv6%3A_A_single-stage_object_detection_framework_for_industrial_applications/","title":"YOLOv6: A single-stage object detection framework for industrial applications","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/YOLOv6%3A_A_single-stage_object_detection_framework_for_industrial_applications/#summary","title":"Summary","text":"<p> The paper discusses the improvements made to the popular YOLO series of object detection models, resulting in the creation of YOLOv6. The authors assimilate various advancements in network design, training strategies, testing techniques, quantization, and optimization methods. They have built a suite of deployment-ready networks at various scales to accommodate different use cases. The model achieves state-of-the-art results, with the quantized version of YOLOv6-S bringing a new state-of-the-art 43.3% AP at 869 FPS, while YOLOv6-M/L achieves better accuracy performance than other detectors with similar inference speed. Their code is available on GitHub."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/YOLOv6%3A_A_single-stage_object_detection_framework_for_industrial_applications/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/YOLOv6%3A_A_single-stage_object_detection_framework_for_industrial_applications/#content","title":"Content","text":"<p> For years, YOLO series have been de facto industry-level standard for efficient object detection. The YOLO community has prospered overwhelmingly to enrich its use in a multitude of hardware platforms and abundant scenarios. In this technical report, we strive to push its limits to the next level, stepping forward with an unwavering mindset for industry application. Considering the diverse requirements for speed and accuracy in the real environment, we extensively examine the up-to-date object detection advancements either from industry or academy. Specifically, we heavily assimilate ideas from recent network design, training strategies, testing techniques, quantization and optimization methods. On top of this, we integrate our thoughts and practice to build a suite of deployment-ready networks at various scales to accommodate diversified use cases. With the generous permission of YOLO authors, we name it YOLOv6. We also express our warm welcome to users and contributors for further enhancement. For a glimpse of performance, our YOLOv6-N hits 35.9% AP on COCO dataset at a throughput of 1234 FPS on an NVIDIA Tesla T4 GPU. YOLOv6-S strikes 43.5% AP at 495 FPS, outperforming other mainstream detectors at the same scale (YOLOv5-S, YOLOX-S and PPYOLOE-S). Our quantized version of YOLOv6-S even brings a new state-of-the-art 43.3% AP at 869 FPS. Furthermore, YOLOv6-M/L also achieves better accuracy performance (i.e., 49.5%/52.3%) than other detectors with the similar inference speed. We carefully conducted experiments to validate the effectiveness of each component. Our code is made available at https://github.com/meituan/YOLOv6."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_adversarial_quantization/","title":"Zero-shot adversarial quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_adversarial_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes a zero-shot adversarial quantization (ZAQ) framework which allows effective knowledge transfer from a full-precision model to its quantized model without accessing training data. The framework uses a novel two-level discrepancy modeling to synthesize informative and diverse data examples and optimize the quantized model in an adversarial learning fashion. The experiments on three vision tasks show the superiority of ZAQ over strong zero-shot baselines and validate its main components.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_adversarial_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_adversarial_quantization/#content","title":"Content","text":"<p> Model quantization is an effective approach to compress deep neural networks for deployment on mobile and edge devices. However, traditional methods for fine-tuning quantized models require access to training datasets, making them inapplicable in real situations where data privacy and security are concerns. To address this issue, we propose a zero-shot adversarial quantization (ZAQ) framework that facilitates effective discrepancy estimation and knowledge transfer from a full-precision model to its quantized model without accessing training data. ZAQ achieves this using a novel two-level discrepancy modeling to drive a generator to synthesize informative and diverse data examples to optimize the quantized model in an adversarial learning fashion. We conduct extensive experiments on three fundamental vision tasks, demonstrating the superiority of ZAQ over strong zero-shot baselines and validating the effectiveness of its main components."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_deep_hashing_and_neural_network_based_error_correction_for_face_template_protection/","title":"Zero-shot deep hashing and neural network based error correction for face template protection","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_deep_hashing_and_neural_network_based_error_correction_for_face_template_protection/#summary","title":"Summary","text":"<p> The paper presents a new architecture integrating deep hashing and neural network decoder for face template protection. The proposed model can handle zero-shot, one-shot, and multi-shot enrollments. It consists of two components for mapping of face images to intermediate binary codes using deep hashing and for error correction using neural network decoder. The model achieves GAR of around 85% with 0.01% FAR with zero-shot enrollment and around 99.95% GAR with 0.01% FAR with one-shot and multi-shot enrollments."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_deep_hashing_and_neural_network_based_error_correction_for_face_template_protection/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_deep_hashing_and_neural_network_based_error_correction_for_face_template_protection/#content","title":"Content","text":"<p> In this paper, the authors propose a novel architecture that integrates a deep hashing framework with a neural network decoder for application to face template protection. The proposed architecture can be used with zero-shot, one-shot, and multi-shot enrollments and it consists of two major components: a deep hashing component, which is used for robust mapping of face images to their corresponding intermediate binary codes, and a neural network decoder component, which corrects errors in the intermediate binary codes that are caused by differences in the enrollment and probe biometrics due to factors such as variation in pose, illumination, and other factors. The system achieves approximately 85% genuine accept rates (GAR) at 0.01% false accept rate (FAR) with zero-shot enrollment, and approximately 99.95% GAR at 0.01% FAR with one-shot and multi-shot enrollments."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_hashing_with_orthogonal_projection_for_image_retrieval/","title":"Zero-shot hashing with orthogonal projection for image retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_hashing_with_orthogonal_projection_for_image_retrieval/#summary","title":"Summary","text":"<p>This paper presents a novel hashing method that uses orthogonal projection of both image and semantic attribute to generate binary codes. This guarantees hash codes from different classes have equal Hamming distance, making the space more discriminative. The proposed method also includes a deep model and has shown competitive results compared to current approaches in large-scale image retrieval."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_hashing_with_orthogonal_projection_for_image_retrieval/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_hashing_with_orthogonal_projection_for_image_retrieval/#content","title":"Content","text":"<p>Hashing is a well-known large-scale image retrieval technique that has been greatly improved by integrating supervised information and Convolutional Neural Network (CNN) to generate quality hash codes and hash functions. However, existing hashing methods have limitations in dealing with emerging images of new classes. In this paper, a novel hashing method based on orthogonal projection of both image and semantic attribute is proposed to generate binary codes that are orthogonal with each other when they belong to different classes, and same otherwise. This guarantees that hash codes from different classes have equal Hamming distance and makes the space more discriminative. The proposed method is also extended with a deep model. Experimental results show the competitiveness of the method compared to state-of-the-art approaches. &lt;\\Abstract&gt;"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_knowledge_distillation_from_a_decision-based_black-box_model/","title":"Zero-shot knowledge distillation from a decision-based black-box model","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_knowledge_distillation_from_a_decision-based_black-box_model/#summary","title":"Summary","text":"<p> The paper proposes a new concept of decision-based black-box (DB3) knowledge distillation, which enables training a compact network (student) using a black-box teacher that only returns classes. The approach computes a sample's robustness against other classes and constructs a soft label for each training sample to train the student via standard knowledge distillation. The proposed method is evaluated on various networks and datasets, and the results demonstrate their effectiveness, even in scenarios where the training data is not accessible."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_knowledge_distillation_from_a_decision-based_black-box_model/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_knowledge_distillation_from_a_decision-based_black-box_model/#content","title":"Content","text":"<p>Knowledge distillation is a popular approach for deep neural network acceleration, with a pre-trained high-capacity network (teacher) used to train a compact network (student) by mimicking the softmax output. However, accessing the white-box teacher's parameters and training samples is not always feasible due to privacy or storage costs. Here, we propose the concept of decision-based black-box (DB3) knowledge distillation that trains a student using a black-box teacher that only returns classes. Our approach computes a sample's robustness against other classes and constructs a soft label for each training sample to train the student via standard KD. In more challenging scenarios when the training data is not accessible, we generate pseudo samples and construct soft labels using decision boundaries of the DB3 teacher. We evaluate our approach on various networks and datasets, and results demonstrate their effectiveness."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_knowledge_transfer_via_adversarial_belief_matching/","title":"Zero-shot knowledge transfer via adversarial belief matching","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_knowledge_transfer_via_adversarial_belief_matching/#summary","title":"Summary","text":"<p>The paper proposes a method to train a smaller student network to match the predictions of a larger teacher network without using any data or metadata. The method involves training an adversarial generator to find images on which the student poorly matches the teacher, which are then used to train the student. The resulting student approximates its teacher on simple datasets, and outperforms the state-of-the-art for few-shot distillation on CIFAR10 with 100 images per class. The paper also proposes a metric to quantify the degree of belief matching, and observes a higher match between the zero-shot student and teacher compared to a student distilled with real data."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_knowledge_transfer_via_adversarial_belief_matching/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_knowledge_transfer_via_adversarial_belief_matching/#content","title":"Content","text":"<p>Performing knowledge transfer from a large teacher network to a smaller student is a popular task in modern deep learning applications. However, due to growing dataset sizes and stricter privacy regulations, it is increasingly common not to have access to the data that was used to train the teacher. We propose a novel method which trains a student to match the predictions of its teacher without using any data or metadata. We achieve this by training an adversarial generator to search for images on which the student poorly matches the teacher, and then using them to train the student. Our resulting student closely approximates its teacher for simple datasets like SVHN, and on CIFAR10 we improve on the state-of-the-art for few-shot distillation (with 100 images per class), despite using no data. Finally, we also propose a metric to quantify the degree of belief matching between teacher and student in the vicinity of decision boundaries, and observe a significantly higher match between our zero-shot student and the teacher, than between a student distilled with real data and the teacher."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_learning_through_cross-modal_transfer/","title":"Zero-shot learning through cross-modal transfer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_learning_through_cross-modal_transfer/#summary","title":"Summary","text":"<p>Summary: The paper presents a model for recognizing objects in images without any training data using unsupervised text corpora as the only knowledge about unseen visual categories. The model maps images into a semantic space of words and uses novelty detection to determine the novelty of the image. It achieves state-of-the-art accuracy on known classes and reasonable performance on unseen classes without requiring any manually defined semantic or visual features.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_learning_through_cross-modal_transfer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_learning_through_cross-modal_transfer/#content","title":"Content","text":"<p>This work presents a model that recognizes objects in images with no training data for the object class. The model operates on a mixture of seen and unseen classes, and uses unsupervised text corpora as the only knowledge about unseen visual categories. The model is based on two main ideas: images are mapped into a semantic space of words, and novelty detection is used to determine whether a new image is on the manifold of known categories. Two novelty detection strategies are demonstrated. This method achieves state-of-the-art accuracy on known classes and reasonable performance on unseen classes, without requiring any manually defined semantic or visual features."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_medical_image_artifact_reduction/","title":"Zero-shot medical image artifact reduction","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_medical_image_artifact_reduction/#summary","title":"Summary","text":"<p> This paper introduces a \"Zero-Shot\" medical image Artifact Reduction (ZSAR) framework, which leverages the power of deep learning but without pre-trained networks or clean image reference. ZSAR uses the low internal visual entropy of an image and trains a lightweight image-specific artifact reduction network to reduce artifacts in medical imaging, such as CT and MRI, better than state-of-the-art both qualitatively and quantitatively, while using shorter execution time. This is the first deep learning framework introduced for reducing artifacts in medical imaging without using a priori training set, which has wide-ranging clinical adoption."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_medical_image_artifact_reduction/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_medical_image_artifact_reduction/#content","title":"Content","text":"<p> Medical images may contain various types of artifacts with different patterns and mixtures, which depend on many factors such as scan setting, machine condition, patients\u2019 characteristics, surrounding environment, etc. However, existing deep learning based artifact reduction methods are restricted by their training set with specific predetermined artifact type and pattern. As such, they have limited clinical adoption. In this paper, we introduce a \u201cZero-Shot\u201d medical image Artifact Reduction (ZSAR) framework, which leverages the power of deep learning but without using general pre-trained networks or any clean image reference. Specifically, we utilize the low internal visual entropy of an image and train a lightweight image-specific artifact reduction network to reduce artifacts in an image at test-time. We use Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) as vehicles to show that ZSAR can reduce artifacts better than state-of-the-art both qualitatively and quantitatively, while using shorter execution time. To the best of our knowledge, this is the first deep learning framework that reduces artifacts in medical images without using a priori training set."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_object_recognition_system_based_on_topic_model/","title":"Zero-shot object recognition system based on topic model","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_object_recognition_system_based_on_topic_model/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new zero-shot learning approach for object recognition, which eliminates the need for human annotation by using a topic model and hierarchical class concept. The method utilizes a Bag-of-Words model, probabilistic Latent Semantic Analysis, and signature topics representation, achieving comparable performance with state-of-the-art algorithms in four public datasets. Experimental results demonstrate the effectiveness of the proposed approach.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_object_recognition_system_based_on_topic_model/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_object_recognition_system_based_on_topic_model/#content","title":"Content","text":"<p> In this research paper, the authors propose a novel zero-shot learning strategy based on a topic model and hierarchical class concept for object recognition where training samples are missing. The proposed method eliminates the human annotation stage and achieves comparable performance with state-of-the-art algorithms in four public datasets. They utilize a Bag-of-Words (BoW) model for image features, probabilistic Latent Semantic Analysis (pLSA) for topic modeling, and signature topics representation for both seen and unseen classes. Experimental results using four publicly available datasets have shown the effectiveness of the proposed method."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_scene_classification_for_high_spatial_resolution_remote_sensing_images/","title":"Zero-shot scene classification for high spatial resolution remote sensing images","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_scene_classification_for_high_spatial_resolution_remote_sensing_images/#summary","title":"Summary","text":"<p>The paper proposes a novel approach to recognize images from unseen scene classes known as zero-shot scene classification. The proposed approach outperforms conventional approaches to remote sensing scene classification by utilizing information from high spatial resolution (HSR) image data. The experimental results show that the proposed approach significantly outperforms the state-of-the-art approaches in zero-shot scene classification. The study highlights the importance of zero-shot learning in scene classification from HSR remote sensing images."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_scene_classification_for_high_spatial_resolution_remote_sensing_images/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_scene_classification_for_high_spatial_resolution_remote_sensing_images/#content","title":"Content","text":"<p>Due to the rapid technological development of various sensors, a huge volume of high spatial resolution (HSR) image data can now be acquired. How to efficiently recognize the scenes from such HSR image data has become a critical task. Conventional approaches to remote sensing scene classification only utilize information from HSR images. To overcome this drawback, we propose a novel approach for recognizing images from unseen scene classes, i.e. zero-shot scene classification. Experimental results show that the proposed approach significantly outperforms the state-of-the-art approaches in zero-shot scene classification. Index Terms \u2014Zero-shot learning, scene classification, high spatial resolution remote sensing images."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_sketch-image_hashing/","title":"Zero-shot sketch-image hashing","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_sketch-image_hashing/#summary","title":"Summary","text":"<p>Summary: This paper proposes a zero-shot sketch-image hashing model for efficient large-scale sketch-based image retrieval. The proposed model uses an end-to-end three-network architecture with a generative hashing scheme for zero-shot retrieval, and outperforms related works on Sketchy and TU-Berlin datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_sketch-image_hashing/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero-shot_sketch-image_hashing/#content","title":"Content","text":"<p> Recent studies show that large-scale sketch-based image retrieval (SBIR) can be efficiently tackled by cross-modal binary representation learning methods, where Hamming distance matching significantly speeds up the process of similarity search. In this paper, the authors propose a zero-shot sketch-image hashing (ZSIH) model to tackle the challenge of querying sketches with unseen categories during training. An end-to-end three-network architecture is built, with a generative hashing scheme formulated to reconstruct semantic knowledge representations for zero-shot retrieval. The proposed ZSIH model is the first zero-shot hashing work suitable for SBIR and cross-modal search, and it outperforms related works on two extended datasets, Sketchy and TU-Berlin."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero_shot_hashing/","title":"Zero shot hashing","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero_shot_hashing/#summary","title":"Summary","text":"<p>Summary: The paper proposes a framework for generating hash codes for images with unknown object classes using textual corpus to predict labels of unseen classes with good accuracy.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero_shot_hashing/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zero_shot_hashing/#content","title":"Content","text":"<p>This paper provides a framework to hash images containing instances of unknown object classes. We attempt to generate the hash codes for images belonging to unseen classes, information of which is available only through the textual corpus. We show that the proposed solution is able to generate hash codes which can predict labels corresponding to unseen classes with appreciably good precision."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zeroq%3A_A_novel_zero_shot_quantization_framework/","title":"Zeroq: A novel zero shot quantization framework","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zeroq%3A_A_novel_zero_shot_quantization_framework/#summary","title":"Summary","text":"<p> The paper introduces ZEROQ, a novel zero-shot quantization framework that enables mixed-precision quantization without training or validation data access by optimizing for a distilled dataset. The framework can support uniform and mixed-precision quantization, has low computational overhead and can complete the whole quantization process in less than 30 seconds. The proposed method was tested on various models and achieved higher accuracy compared to DFQ. The paper is open-sourced on GitHub."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zeroq%3A_A_novel_zero_shot_quantization_framework/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/Zeroq%3A_A_novel_zero_shot_quantization_framework/#content","title":"Content","text":"<p> Here is the abstract of the research paper \"ZeroQ: A Novel Zero Shot Quantization Framework.\" The paper proposes a new zero-shot quantization framework called ZEROQ, which enables mixed-precision quantization without any access to the training or validation data. This is achieved by optimizing for a Distilled dataset, which matches the statistics of batch normalization across different layers of the network. ZEROQ can support both uniform and mixed-precision quantization. The paper tested the proposed method on a diverse set of models and achieved 1.71% higher accuracy on MobileNetV2, as compared to the recently proposed DFQ method. The framework has a very low computational overhead, and it can finish the entire quantization process in less than 30 seconds (0.5% of one epoch training time of ResNet50 on ImageNet). The paper is open-sourced on GitHub."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ZipML%3A_Training_linear_models_with_end-to-end_low_precision%2C_and_a_little_bit_of_deep_learning/","title":"ZipML: Training linear models with end-to-end low precision, and a little bit of deep learning","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ZipML%3A_Training_linear_models_with_end-to-end_low_precision%2C_and_a_little_bit_of_deep_learning/#summary","title":"Summary","text":"<p>Summary: The paper discusses training machine learning models with reduced precision, which can significantly reduce computation and communication. The authors focus on linear models and propose a simple framework called ZipML based on double sampling, which can execute training at low precision with no bias and guarantee convergence. The framework is validated across a range of applications and can lead to speedups of up to 6.5 times compared to full 32-bit precision. The authors also propose a variance-optimal stochastic quantization strategy that can save up to 1.7 times in data movement compared with uniform quantization, and achieve higher accuracy than state-of-the-art XNOR-Net when training deep networks with quantized models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ZipML%3A_Training_linear_models_with_end-to-end_low_precision%2C_and_a_little_bit_of_deep_learning/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/ZipML%3A_Training_linear_models_with_end-to-end_low_precision%2C_and_a_little_bit_of_deep_learning/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_A-GLVQ-Making_Generalized_Learning_Vector_Quantization_Aware_of_Context/","title":"[PDF][PDF] A-GLVQ-Making Generalized Learning Vector Quantization Aware of Context","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_A-GLVQ-Making_Generalized_Learning_Vector_Quantization_Aware_of_Context/#summary","title":"Summary","text":"<p> This paper proposes a framework for adding context information to prototype generation in Generalized Learning Vector Quantization (GLVQ) models. The framework can model dependencies in a modular way using polynomials or neural networks. The evaluations on real-world and artificial datasets demonstrate improved performance and meaningful prototype adaptations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_A-GLVQ-Making_Generalized_Learning_Vector_Quantization_Aware_of_Context/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_A-GLVQ-Making_Generalized_Learning_Vector_Quantization_Aware_of_Context/#content","title":"Content","text":"<p>Generalized Learning Vector Quantization (GLVQ) is a powerful and robust approach to classification tasks, but it has the disadvantage of prototypes not accounting for changes in the environment. In this paper, we propose a framework for incorporating context information into prototype generation that can model dependencies in a modular way ranging from polynomials to neural networks. The evaluations on artificial and real-world datasets show an increase in performance and meaningful prototype adaptations."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Deep_Learning_Vector_Quantization./","title":"[PDF][PDF] Deep Learning Vector Quantization.","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Deep_Learning_Vector_Quantization./#summary","title":"Summary","text":"<p> The paper proposes combining neural networks with Learning Vector Quantization (LVQ), called Deep LVQ (DLVQ), to reduce fooling examples caused by the extrapolating nature of the log-softmax. The proposed method achieves comparable performance on MNIST and is more robust against fooling and adversarial examples."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Deep_Learning_Vector_Quantization./#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Deep_Learning_Vector_Quantization./#content","title":"Content","text":"<p>While deep neural nets (DNN\u2019s) achieve impressive performance on image recognition tasks, previous studies have reported that DNN\u2019s give high confidence predictions for unrecognizable images. Motivated by the observation that such fooling examples might be caused by the extrapolating nature of the log-softmax, we propose to combine neural networks with Learning Vector Quantization (LVQ). Our proposed method, called Deep LVQ (DLVQ), achieves comparable performance onMNIST while being more robust against fooling and adversarial examples."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_End-to-end_Model_Inference_and_Training_on_Gemmini/","title":"[PDF][PDF] End-to-end Model Inference and Training on Gemmini","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_End-to-end_Model_Inference_and_Training_on_Gemmini/#summary","title":"Summary","text":"<p>Summary: Cannot provide a summary as there is no abstract section available in the given text. Please provide the actual extracted text to extract the abstract section.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_End-to-end_Model_Inference_and_Training_on_Gemmini/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_End-to-end_Model_Inference_and_Training_on_Gemmini/#content","title":"Content","text":"<p>Sorry, I cannot extract the abstract section from the given text since it does not contain any abstract section. The given text is just the basic information about the research paper such as author name, university, technical report number, and publication date. Please provide me the actual extracted text so that I can extract the abstract section for you.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Fq-vit%3A_Post-training_quantization_for_fully_quantized_vision_transformer/","title":"[PDF][PDF] Fq-vit: Post-training quantization for fully quantized vision transformer","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Fq-vit%3A_Post-training_quantization_for_fully_quantized_vision_transformer/#summary","title":"Summary","text":"<p> This paper proposes a method called Power-of-Two Factor (PTF) to reduce the performance degradation and inference complexity of fully quantized Vision Transformers. They also propose Log-Int-Softmax (LIS) to simplify inference by using 4-bit quantization and the BitShift operator, and demonstrate that their Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works even while using lower bit-width on attention maps. The proposed method achieves lossless accuracy degradation on fully quantized vision transformers, and the code is available at https://github.com/megvii-research/FQ-ViT."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Fq-vit%3A_Post-training_quantization_for_fully_quantized_vision_transformer/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Fq-vit%3A_Post-training_quantization_for_fully_quantized_vision_transformer/#content","title":"Content","text":"<p>Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs), and suffer severe degradation when applied to fully quantized vision transformers. In this work, we demonstrate that many of these difficulties arise because of serious inter-channel variation in Lay- erNorm inputs, and present, Power-of-Two Fac- tor (PTF), a systematic method to reduce the per- formance degradation and inference complexity of fully quantized vision transformers. In addition, observing an extreme non-uniform distribution in attention maps, we propose Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4- bit quantization and the BitShift operator. Comprehensive experiments on various transformer-based architectures and benchmarks show that our Fully Quantized Vision Transformer (FQ-ViT) outper- forms previous works while even using lower bit- width on attention maps. For instance, we reach 84.89% top-1 accuracy with ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin- S) on COCO. To our knowledge, we are the first to achieve lossless accuracy degradation (\u223c1%) on fully quantized vision transformers. The code is available at https://github.com/megvii-research/ FQ-ViT."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Graph_and_autoencoder_based_feature_extraction_for_zero-shot_learning./","title":"[PDF][PDF] Graph and autoencoder based feature extraction for zero-shot learning.","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Graph_and_autoencoder_based_feature_extraction_for_zero-shot_learning./#summary","title":"Summary","text":"<p> The paper proposes a framework called Graph and Autoencoder Based Feature Extraction (GAFE) to improve Zero-shot learning (ZSL) imaging recognition of novel categories without training data. GAFE preserves the sub-manifold of samples in the embedding space and ensures that the mapping can precisely reconstruct the original visual feature. GAFE combines encoder and decoder parts, constructs a graph to preserve the local intrinsic structure of the data, and imposes an L21 norm sparsity constraint on the mapping to identify relevant features to the target domain, and achieves better results in five attribute datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Graph_and_autoencoder_based_feature_extraction_for_zero-shot_learning./#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Graph_and_autoencoder_based_feature_extraction_for_zero-shot_learning./#content","title":"Content","text":"<p>Zero-shot learning (ZSL) refers to models built to recognize novel visual categories that have no associated labelled training samples. Existing approaches, however, fail to preserve the sub-manifold of samples in the embedding space and do not investigate whether the mapping can precisely reconstruct the original visual feature. In this paper, the authors propose a Graph and Autoencoder Based Feature Extraction (GAFE) framework to address these issues. GAFE seeks a low-rank mapping to preserve the sub-manifold of samples by taking the encoder-decoder paradigm. The encoder part learns a mapping from the visual feature to the semantic space, while the decoder part reconstructs the original features with the learned mapping. Moreover, GAFE constructs a graph to guarantee the learned mapping can preserve the local intrinsic structure of the data, and an L21 norm sparsity constraint is imposed on the mapping to identify features relevant to the target domain. The effectiveness of the proposed model is demonstrated through extensive experiments on \ufb01ve attribute datasets."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_KCNN%3A_Kernel-wise_Quantization_to_Remarkably_Decrease_Multiplications_in_Convolutional_Neural_Network./","title":"[PDF][PDF] KCNN: Kernel-wise Quantization to Remarkably Decrease Multiplications in Convolutional Neural Network.","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_KCNN%3A_Kernel-wise_Quantization_to_Remarkably_Decrease_Multiplications_in_Convolutional_Neural_Network./#summary","title":"Summary","text":"<p>Summary: The paper proposes a new method called KCNN for quantizing floating-point weights separately in each kernel to reduce computational complexity in convolutional neural networks (CNNs). They use an aggressive Lloyd algorithm to obtain a closed-form solution and dual normalization to address the pathological curvature problem during fine-tuning. KCNN shows insignificant performance loss compared to floating-point counterparts.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_KCNN%3A_Kernel-wise_Quantization_to_Remarkably_Decrease_Multiplications_in_Convolutional_Neural_Network./#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_KCNN%3A_Kernel-wise_Quantization_to_Remarkably_Decrease_Multiplications_in_Convolutional_Neural_Network./#content","title":"Content","text":"<p> Convolutional neural networks (CNNs) are state-of-the-art in computer vision tasks. However, their high computational power demand has limited their widespread adoption. Several methods have quantized the weights and activations to decrease computational complexity, but fixed-point or binary values may cause degradation in performance due to numerical information loss. In this paper, the authors propose KCNN, a method that quantizes floating-point weights in each kernel separately to multiple bit planes to decrease multiplications. The authors obtain a closed-form solution via an aggressive Lloyd algorithm and fine-tuning, and propose dual normalization to solve the pathological curvature problem during fine-tuning. The method shows negligible performance loss compared to floating-point counterparts."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Larq%3A_An_open-source_library_for_training_binarized_neural_networks/","title":"[PDF][PDF] Larq: An open-source library for training binarized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Larq%3A_An_open-source_library_for_training_binarized_neural_networks/#summary","title":"Summary","text":"<p>Summary: larq is an open-source library that facilitates the training of binarized and other extremely quantized neural networks by providing tools, optimization strategies, and easy-to-use APIs. The library includes tested and maintained implementations and pretrained weights for popular models.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Larq%3A_An_open-source_library_for_training_binarized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Larq%3A_An_open-source_library_for_training_binarized_neural_networks/#content","title":"Content","text":"<p> larq is an open-source library for training binarized neural networks and other quantized neural networks. Binarized neural networks represent an extreme case of quantized networks that require special tools and optimization strategies. larq is intended to facilitate researchers in resolving research questions related to binarized neural networks and other extremely quantized neural networks. The larq API is designed to provide an easy to use, composable way to design and train binarized neural networks. The library provides tested and maintained implementations and pretrained weights for several popular extremely quantized models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Linear_symmetric_quantization_of_neural_networks_for_low-precision_integer_hardware/","title":"[PDF][PDF] Linear symmetric quantization of neural networks for low-precision integer hardware","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Linear_symmetric_quantization_of_neural_networks_for_low-precision_integer_hardware/#summary","title":"Summary","text":"<p>Summary: This paper proposes a learned linear symmetric quantizer for integer neural network processors that quantizes neural parameters and activations to low-bit integer and accelerates hardware inference. The method outperforms many previous approaches for various networks such as AlexNet, ResNet, and lightweight models like MobileNet while keeping friendly to the accelerator architecture. The proposed quantizer induces less than 0.4% accuracy drop in ResNet18, ResNet34, and AlexNet when quantizing the whole network as required by the integer processors. The quantized models are also deployed on a specialized integer-arithmetic-only DNN accelerator to show effectiveness.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Linear_symmetric_quantization_of_neural_networks_for_low-precision_integer_hardware/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Linear_symmetric_quantization_of_neural_networks_for_low-precision_integer_hardware/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Releq%3A_An_automatic_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/","title":"[PDF][PDF] Releq: An automatic reinforcement learning approach for deep quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Releq%3A_An_automatic_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper discusses the challenges of deploying deep neural networks (DNNs) to devices with limited resources because DNNs are computationally intense and have a large memory footprint. The paper proposes quantizing the weights of DNNs to lower bitwidths as a solution to these challenges. To address the difficulty of deep quantization while preserving accuracy and avoiding manual efforts, the paper proposes an end-to-end framework called ReLeQ which utilizes reinforcement learning algorithm, Proximal Policy Optimization (PPO), to efficiently explore the quantization design space. The proposed ReLeQ framework was evaluated on different neural networks and achieved an average bitwidths of 2.25, 5, and 4 with a final accuracy loss below 0.3%.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Releq%3A_An_automatic_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Releq%3A_An_automatic_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#content","title":"Content","text":"<p> Despite numerous state-of-the-art applications of Deep Neural Networks (DNNs) in a wide range of real-world tasks, two major challenges hinder further advances in DNNs: hyperparameter optimization and constrained power resources, which is a signi\ufb01cant concern in embedded devices. DNNs become increasingly dif\ufb01cult to train and deploy as they grow in size due to both computational intensity and the large memory footprint. Recent efforts show that quantizing weights of deep neural networks to lower bitwidths takes a signi\ufb01cant step toward mitigating the mentioned issues, by reducing memory bandwidth and using limited computational resources which is important for deploying DNN models to devices with limited resources. This paper builds upon the algorithmic insight that the bitwidth of operations in DNNs can be reduced without compromising their classi\ufb01cation accuracy. Deep quantization (quantizing bitwidths below eight) while maintaining accuracy, requires magni\ufb01cent manual effort and hyper-parameter tuning as well as re-training. This paper tackles the aforementioned problems by designing an end to end framework, dubbed ReLeQ, to automate DNN quantization. We formulate DNN quantization as an optimization problem and use a state-of-the-art policy gradient based Reinforcement Learning (RL) algorithm, Proximal Policy Optimiza- tion (PPO) to ef\ufb01ciently explore the large design space of DNN quantization and solve the de\ufb01ned optimization problem. To show the effectiveness of ReLeQ, we evaluated it across several neural networks including MNIST, CIFAR10, SVHN. ReLeQ quantizes the weights of these networks to average bitwidths of 2.25, 5 and 4 respectively while maintaining the \ufb01nal accuracy loss below 0.3%."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Releq%3A_an_automatic_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/","title":"[PDF][PDF] Releq: an automatic reinforcement learning approach for deep quantization of neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Releq%3A_an_automatic_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper discusses deep quantization and its ability to reduce computation and storage costs in deep neural networks. However, manually selecting quantization levels can lead to accuracy loss. To address this challenge, the paper proposes an automatic end-to-end deep reinforcement learning framework called ReLeQ, which balances speed and quality and can achieve a speedup of up to 2.2x over 8-bit execution with virtually preserved accuracy, making it a step towards automating deep quantization of neural networks.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Releq%3A_an_automatic_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Releq%3A_an_automatic_reinforcement_learning_approach_for_deep_quantization_of_neural_networks/#content","title":"Content","text":"<p> Deep neural networks (DNNs) are computation intensive and require significant amounts of resources for inference tasks in computer vision. Quantization can reduce storage and computation costs by decreasing the bitwidth of network encodings. Recent research has shown that selecting the quantization levels for each layer can preserve accuracy while pushing the bitwidth below eight bits. However, this deep quantization can lead to significant accuracy loss without manual effort. Deep quantization opens a large hyper-parameter space and becomes a major challenge. This paper proposes an automatic end-to-end deep reinforcement learning framework (ReLeQ) for discovering the quantization levels. ReLeQ balances speed and quality and provides a solution for quantization of a large variety of deep networks while minimizing computation and storage cost. Results show that ReLeQ can achieve a speedup of up to 2.2x over 8-bit execution, with virtually preserved accuracy, making it the initial step towards automating the deep quantization of neural networks."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_SINGLE-SHOT_SEMANTIC_MATCHER_FOR_UNSEEN_OBJECT_DETECTION./","title":"[PDF][PDF] SINGLE-SHOT SEMANTIC MATCHER FOR UNSEEN OBJECT DETECTION.","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_SINGLE-SHOT_SEMANTIC_MATCHER_FOR_UNSEEN_OBJECT_DETECTION./#summary","title":"Summary","text":"<p>Summary: The paper proposes a semantic matcher CNN architecture based on GoogleNet and YOLO/DetectNet architectures to detect unseen objects. The CNN takes two images and returns detected objects in the test image that correspond to the semantic class of the request image. The authors train and test their CNN on the ILSVRC 2014 dataset with 200 seen and 90 unseen classes, achieving an mAP of 23 for seen classes and 21 for unseen classes using detection-by-request training and testing protocols.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_SINGLE-SHOT_SEMANTIC_MATCHER_FOR_UNSEEN_OBJECT_DETECTION./#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_SINGLE-SHOT_SEMANTIC_MATCHER_FOR_UNSEEN_OBJECT_DETECTION./#content","title":"Content","text":"<p> In this paper, the authors propose a single-shot semantic matcher CNN architecture based on GoogleNet and YOLO/DetectNet architectures for the purpose of detecting unseen objects. The semantic matcher takes in two images (test and request) and returns detected objects (bounding boxes) on the test image that correspond to the semantic class represented by the request (sample) image. The authors propose the detection-by-request training and testing protocols for semantic matching algorithms and train and test their CNN on the ILSVRC 2014 dataset with 200 seen and 90 unseen classes. The real-time object detection achieved an mAP of 23 for seen classes and 21 for unseen classes."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_SitNet%3A_Discrete_Similarity_Transfer_Network_for_Zero-shot_Hashing./","title":"[PDF][PDF] SitNet: Discrete Similarity Transfer Network for Zero-shot Hashing.","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_SitNet%3A_Discrete_Similarity_Transfer_Network_for_Zero-shot_Hashing./#summary","title":"Summary","text":"<p>The paper proposes a new zero-shot hashing approach called Discrete Similarity Transfer Network (SitNet) that preserves the semantic similarity between images from both \"seen\" and new \"unseen\" concepts. A multi-task architecture is adopted to exploit the supervised information for seen concepts and semantic vectors, and a discrete hashing layer is integrated into the network for hashcode generation. The experiments on three benchmarks validate the superiority of SitNet over the state-of-the-art."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_SitNet%3A_Discrete_Similarity_Transfer_Network_for_Zero-shot_Hashing./#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_SitNet%3A_Discrete_Similarity_Transfer_Network_for_Zero-shot_Hashing./#content","title":"Content","text":"<p>Hashing has been widely used for fast image retrieval, but collecting supervised information for re-training hashing model infeasible. In this paper, a novel zero-shot hashing approach called Discrete Similarity Transfer Network (SitNet), is proposed to preserve the semantic similarity between images from both \u201cseen\u201d and new \u201cunseen\u201d concepts. The semantic vectors of concepts are adopted to capture the similarity structures among classes, making the model trained with seen concepts generalize well for unseen ones. A multi-task architecture is adopted to exploit the supervised information for seen concepts and the semantic vectors simultaneously, and a discrete hashing layer is integrated into the network for hashcode generating, which avoids the information loss caused by real-value relaxation in training phase. Experiments on three benchmarks validate the superiority of SitNet over the state-of-the-arts."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Soft-to-hard_vector_quantization_for_end-to-end_learned_compression_of_images_and_neural_networks/","title":"[PDF][PDF] Soft-to-hard vector quantization for end-to-end learned compression of images and neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Soft-to-hard_vector_quantization_for_end-to-end_learned_compression_of_images_and_neural_networks/#summary","title":"Summary","text":"<p>The paper proposes a novel approach for learning compressible representations in deep architectures through the use of a soft relaxation of quantization and entropy, which is eventually annealed to their discrete counterparts during training. The method is demonstrated to be effective in two applications, namely image compression and neural network compression, surpassing existing techniques."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Soft-to-hard_vector_quantization_for_end-to-end_learned_compression_of_images_and_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Soft-to-hard_vector_quantization_for_end-to-end_learned_compression_of_images_and_neural_networks/#content","title":"Content","text":"<p>In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_TensorFlow_Enabled_Deep_Learning_Model_Optimization_for_enhanced_Realtime_Person_Detection_using_Raspberry_Pi_operating_at_the_Edge./","title":"[PDF][PDF] TensorFlow Enabled Deep Learning Model Optimization for enhanced Realtime Person Detection using Raspberry Pi operating at the Edge.","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_TensorFlow_Enabled_Deep_Learning_Model_Optimization_for_enhanced_Realtime_Person_Detection_using_Raspberry_Pi_operating_at_the_Edge./#summary","title":"Summary","text":"<p>Summary: The paper explores the use of TensorFlow architectures to enable real-time person detection on a Raspberry Pi. The authors analyze the effects of quantization on the performance of deep learning object detection models and show that a suitably optimized TensorFlow solution architecture reduces inference time with only slight accuracy cost implications. The authors achieve industrial standard floor limit value greater than 70% on the quantized models considered, with detection time less than 3ms. Deep Neural Network model is trained using the INRIA person detection benchmark dataset.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_TensorFlow_Enabled_Deep_Learning_Model_Optimization_for_enhanced_Realtime_Person_Detection_using_Raspberry_Pi_operating_at_the_Edge./#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_TensorFlow_Enabled_Deep_Learning_Model_Optimization_for_enhanced_Realtime_Person_Detection_using_Raspberry_Pi_operating_at_the_Edge./#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Towards_a_deeper_understanding_of_training_quantized_neural_networks/","title":"[PDF][PDF] Towards a deeper understanding of training quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Towards_a_deeper_understanding_of_training_quantized_neural_networks/#summary","title":"Summary","text":"<p>This paper investigates the theory of training quantized neural networks using high-precision representations that have an annealing property, which is lacking in purely quantized training methods. The aim is to learn on embedded platforms with limited resources, memory capacity, and power consumption. The authors analyze the convergence properties of commonly used methods, with the main result explaining observed empirical differences between the training algorithms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Towards_a_deeper_understanding_of_training_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Towards_a_deeper_understanding_of_training_quantized_neural_networks/#content","title":"Content","text":"<p>Training neural networks with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. In this work, the authors investigate the theory of training quantized neural networks by analyzing the convergence properties of some commonly used methods. The main result shows that training algorithms that exploit high-precision representations have an important annealing property that purely quantized training methods lack, which explains many of the observed empirical differences between these types of algorithms."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Towards_lower_bit_multiplication_for_convolutional_neural_network_training/","title":"[PDF][PDF] Towards lower bit multiplication for convolutional neural network training","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Towards_lower_bit_multiplication_for_convolutional_neural_network_training/#summary","title":"Summary","text":"<p>Sure, I can help you summarize the paper's abstract. Please provide the entire abstract section of the research paper.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Towards_lower_bit_multiplication_for_convolutional_neural_network_training/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Towards_lower_bit_multiplication_for_convolutional_neural_network_training/#content","title":"Content","text":"<p>Sorry, but the text you provided does not contain the abstract section of the research paper. Can you please provide the entire extracted text so I can accurately extract the abstract section?</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Trace_weighted_hessian-aware_quantization/","title":"[PDF][PDF] Trace weighted hessian-aware quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Trace_weighted_hessian-aware_quantization/#summary","title":"Summary","text":"<p>This paper introduces a new method called trace-weighted Hessian-aware quantization to reduce memory and power requirements for neural networks used in mobile systems. The method uses the trace of the Hessian to avoid significant accuracy degradation caused by directly quantizing to ultra-low precision. Theoretical results provided show that the sensitivity of different layers to quantization can be determined using the trace of the Hessian, and this information is used for Hessian-aware fine-tuning. Experiments conducted on Inception-V3 and ResNet50 models trained on ImageNet dataset show that this approach outperforms expensive AutoML search methods and holds state-of-the-art results for quantized models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Trace_weighted_hessian-aware_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Trace_weighted_hessian-aware_quantization/#content","title":"Content","text":"<p>Quantization is a promising approach to reduce the memory footprint and power consumption of neural networks deployed on mobile systems with limited resources. However, directly quantizing a model to ultra-low precision can cause significant accuracy degradation. In this paper, we introduce a new second-order-based method called trace-weighted Hessian-aware quantization, which does not require any expensive search methods. We provide theoretical results that show the trace of the Hessian can be used to determine the sensitivity of different layers to quantization, and we use this information to perform Hessian-aware fine-tuning. We test our approach on Inception-V3 and ResNet50 models trained on the ImageNet dataset and show that it exceeds industry-scale results achieved with expensive AutoML search methods. Both results are state-of-the-art for quantized models."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Training_deep_learning_recommendation_model_with_quantized_collective_communications/","title":"[PDF][PDF] Training deep learning recommendation model with quantized collective communications","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Training_deep_learning_recommendation_model_with_quantized_collective_communications/#summary","title":"Summary","text":"<p>Summary: The paper discusses using integer quantization to reduce communication volume in synchronous training of DLRM. They explore quantizing alltoall and allreduce collectives and benchmark the accuracy loss with a representative DLRM model and Kaggle 7D dataset. Their results show that alltoall forward and backward passes, and dense allreduce can be quantized to 4 bits without accuracy loss compared to full-precision training.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Training_deep_learning_recommendation_model_with_quantized_collective_communications/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Training_deep_learning_recommendation_model_with_quantized_collective_communications/#content","title":"Content","text":"<p> Deep Learning Recommendation Model (DLRM) captures our representative model architectures developed for click-through-rate (CTR) prediction based on high-dimensional sparse categorical data. Collective communications can account for a significant fraction of time in synchronous training of DLRM at scale. In this work, we explore using fine-grain integer quantization to reduce the communication volume of alltoall and allreduce collectives. We benchmark accuracy loss of quantized alltoall and allreduce with a representative DLRM model and Kaggle 7D dataset. We show that alltoall forward and backward passes, and dense allreduce can be quantized to 4 bits without accuracy loss compared to full-precision training."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Unlabeled_Data_Driven_Channel-Wise_Bit-Width_Allocation_and_Quantization_Refinement./","title":"[PDF][PDF] Unlabeled Data Driven Channel-Wise Bit-Width Allocation and Quantization Refinement.","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Unlabeled_Data_Driven_Channel-Wise_Bit-Width_Allocation_and_Quantization_Refinement./#summary","title":"Summary","text":"<p>This paper proposes a two-stage quantization method that optimizes bit-width allocation for different channels and distills knowledge from pre-trained models to achieve effective low-bit quantization without time-consuming training or access to full datasets. Experiments on image classification and object detection show promising results in 4-bit quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Unlabeled_Data_Driven_Channel-Wise_Bit-Width_Allocation_and_Quantization_Refinement./#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Unlabeled_Data_Driven_Channel-Wise_Bit-Width_Allocation_and_Quantization_Refinement./#content","title":"Content","text":"<p>Network quantization is crucial for the deployment of complex Deep Neural Networks (DNNs) on mobile equipment. However, low-bit quantization without time-consuming training or access to full datasets is still challenging. In this paper, a two-stage quantization method is proposed that only requires a few unlabeled samples. Firstly, a gradient-based approach is presented to optimize bit-width allocation for different channels according to their sensitivity. Secondly, quantization is refined by distilling knowledge from the output and intermediate features of the pre-trained model. Experiments on image classification and object detection demonstrate the effectiveness of the proposed method in achieving promising results in 4-bit quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Variational_network_quantization/","title":"[PDF][PDF] Variational network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Variational_network_quantization/#summary","title":"Summary","text":"<p>The paper introduces a method for preparing neural networks for pruning and few-bit quantization by formulating it as a variational inference problem. A quantizing prior that leads to a multi-modal, sparse posterior distribution over weights is introduced, and a differentiable Kullback-Leibler divergence approximation for this prior is derived. Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10), and the method does not require fine-tuning after quantization."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Variational_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/%5BPDF%5D%5BPDF%5D_Variational_network_quantization/#content","title":"Content","text":"<p>In this paper, the preparation of a neural network for pruning and few-bit quantization is formulated as a variational inference problem. To this end, a quantizing prior that leads to a multi-modal, sparse posterior distribution over weights, is introduced and a differentiable Kullback-Leibler divergence approximation for this prior is derived. After training with Variational Network Quantization, weights can be replaced by deterministic quantization values with small to negligible loss of task accuracy (including pruning by setting weights to 0). The method does not require \ufb01ne-tuning after quantization. Results are shown for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10)."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/_-Norm_Batch_Normalization_for_Efficient_Training_of_Deep_Neural_Networks/","title":"-Norm Batch Normalization for Efficient Training of Deep Neural Networks","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/_-Norm_Batch_Normalization_for_Efficient_Training_of_Deep_Neural_Networks/#summary","title":"Summary","text":"<p>This paper proposes L1-norm batch normalization (L1BN), which involves only linear operations in training and is approximately equivalent to the conventional L2-norm BN (L2BN) by multiplying a scaling factor that equals to/radicalbig\u03c02. L1BN maintains the same performance and convergence rate as L2BN, but with higher computational efficiency. Experiment results show that on various convolutional neural networks (CNNs) and generative adversarial networks (GANs), L1BN achieves 25% speedup and 37% energy saving compared to the original L2BN in real ASIC synthesis with reduced resources."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/_-Norm_Batch_Normalization_for_Efficient_Training_of_Deep_Neural_Networks/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/_-Norm_Batch_Normalization_for_Efficient_Training_of_Deep_Neural_Networks/#content","title":"Content","text":"<p>Batch normalization (BN) is widely used for accelerating and improving the training of deep neural networks (DNNs), but it brings in additional calculations, consumes more memory, and significantly slows down the training iteration. This work proposes L1-norm batch normalization (L1BN), which involves only linear operations in both forward and backward propagation during training, and is approximately equivalent to the conventional L2-norm BN (L2BN) by multiplying a scaling factor that equals to/radicalbig\u03c02. Experiments on various convolutional neural networks (CNNs) and generative adversarial networks (GANs) show that L1BN maintains the same performance and convergence rate as L2BN, but with higher computational efficiency. In real ASIC synthesis with reduced resources, L1BN achieves 25% speedup and 37% energy saving compared to the original L2BN."},{"location":"Quantization-Survey-Paper-Notes/computer_vision/n-hot%3A_Efficient_bit-level_sparsity_for_powers-of-two_neural_network_quantization/","title":"n-hot: Efficient bit-level sparsity for powers-of-two neural network quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/computer_vision/n-hot%3A_Efficient_bit-level_sparsity_for_powers-of-two_neural_network_quantization/#summary","title":"Summary","text":"<p>Summary: The paper proposes an efficient Powers-of-two (PoT) quantization scheme that introduces bit-level sparsity to reduce the number of bit operations of deep neural networks while preserving the accuracy. The proposed method uses a two-stage fine-tuning algorithm to recover the accuracy drop and experimental results show that it reduces the number of operations by about 75% and model size by 11.5% compared to the uniform method, while suppressing the accuracy drop by 0.3% at most.</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/n-hot%3A_Efficient_bit-level_sparsity_for_powers-of-two_neural_network_quantization/#target-task","title":"Target Task","text":"<p>computer vision</p>"},{"location":"Quantization-Survey-Paper-Notes/computer_vision/n-hot%3A_Efficient_bit-level_sparsity_for_powers-of-two_neural_network_quantization/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Composite_correlation_quantization_for_efficient_multimodal_retrieval/","title":"Composite correlation quantization for efficient multimodal retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Composite_correlation_quantization_for_efficient_multimodal_retrieval/#summary","title":"Summary","text":"<p>Summary: The proposed Composite Correlation Quantization (CCQ) model is designed for efficient similarity retrieval from large-scale multimodal databases. CCQ learns composite quantizers to convert the latent features into compact binary codes and preserves both intra-modal similarity and inter-modal correlation. The model outperforms state-of-the-art hashing methods for both unimodal and cross-modal retrieval.</p>"},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Composite_correlation_quantization_for_efficient_multimodal_retrieval/#target-task","title":"Target Task","text":"<p>information retrieval</p>"},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Composite_correlation_quantization_for_efficient_multimodal_retrieval/#content","title":"Content","text":"<p>  Ef\ufb01cient similarity retrieval from large-scale multimodal databases is a challenging task faced by modern search engines and social networks. To retrieve semantically relevant results across heterogeneous data modalities, a system should enable cross-modal correlation and computation-ef\ufb01cient indexing. In this paper, we propose Composite Correlation Quantization (CCQ), a novel model towards seamless multimodal hashing. CCQ jointly \ufb01nds correlation-maximal mappings that transform different modalities into an isomorphic latent space and learns composite quantizers that convert the isomorphic latent features into compact binary codes. An optimization framework is devised to preserve both intra-modal similarity and inter-modal correlation through minimizing both reconstruction and quantization errors. CCQ outperforms state-of-the-art hashing methods for both unimodal and cross-modal retrieval."},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Jointly_optimizing_query_encoder_and_product_quantization_to_improve_retrieval_performance/","title":"Jointly optimizing query encoder and product quantization to improve retrieval performance","text":""},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Jointly_optimizing_query_encoder_and_product_quantization_to_improve_retrieval_performance/#summary","title":"Summary","text":"<p> The paper proposes a new method, Joint optimization of query encoding and Product Quantization (JPQ), to overcome the efficiency problem in Dense Retrieval (DR) with vector compression methods such as Product Quantization (PQ). The evaluation shows that JPQ significantly outperforms popular vector compression methods with 30x compression on index size, bringing 10x speedup on CPU and 2x speedup on GPU in query latency."},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Jointly_optimizing_query_encoder_and_product_quantization_to_improve_retrieval_performance/#target-task","title":"Target Task","text":"<p>information retrieval</p>"},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Jointly_optimizing_query_encoder_and_product_quantization_to_improve_retrieval_performance/#content","title":"Content","text":"<p>Recently, Information Retrieval community has witnessed fast-paced advances in Dense Retrieval (DR), which performs first-stage retrieval with embedding-based search. To overcome the efficiency problem in practical web search scenarios, vector compression methods have been adopted. One of the most popular methods is Product Quantization (PQ). However, vector compression methods incur severely decayed retrieval performance due to the separation between encoding and compression. To tackle this problem, the authors present JPQ, which stands for Joint optimization of query encoding and Product Quantization. They evaluate JPQ on two publicly available retrieval benchmarks and show that it significantly outperforms popular vector compression methods. Compared with previous DR models that use brute-force search, JPQ almost matches the best retrieval performance with 30x compression on index size. The compressed index further brings 10x speedup on CPU and 2x speedup on GPU in query latency."},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Product_quantizer_aware_inverted_index_for_scalable_nearest_neighbor_search/","title":"Product quantizer aware inverted index for scalable nearest neighbor search","text":""},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Product_quantizer_aware_inverted_index_for_scalable_nearest_neighbor_search/#summary","title":"Summary","text":"<p>This paper proposes a joint optimization method for the coarse and fine quantizers in product quantization for non-exhaustive nearest neighbor search. The authors suggest substituting the original objective of the coarse quantizer to end-to-end quantization distortion and enabling a generic approach for different combinations of coarse and fine quantizers."},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Product_quantizer_aware_inverted_index_for_scalable_nearest_neighbor_search/#target-task","title":"Target Task","text":"<p>information retrieval</p>"},{"location":"Quantization-Survey-Paper-Notes/information_retrieval/Product_quantizer_aware_inverted_index_for_scalable_nearest_neighbor_search/#content","title":"Content","text":"<p>The inverted index is one of the most commonly used structures for non-exhaustive nearest neighbor search on large-scale datasets. It allows a significant factor of acceleration by a reduced number of distance computations with only a small fraction of the database. In particular, the inverted index enables the product quantization (PQ) to learn their codewords in the residual vector space. The quantization error of the PQ can be substantially improved in such combination since the residual vector space is much more quantization-friendly thanks to their compact distribution compared to the original data. In this paper, we first raise an unremarked but crucial question; why the inverted index and the product quantizer are optimized separately even though they are closely related? For instance, changes on the inverted index distort the whole residual vector space. To address the raised question, we suggest a joint optimization of the coarse and fine quantizers by substituting the original objective of the coarse quantizer to end-to-end quantization distortion. Moreover, our method is generic and applicable to different combinations of coarse and fine quantizers such as inverted multi-index and optimized PQ."},{"location":"Quantization-Survey-Paper-Notes/nlp/A_Fast_Post-Training_Pruning_Framework_for_Transformers/","title":"A Fast Post-Training Pruning Framework for Transformers","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/A_Fast_Post-Training_Pruning_Framework_for_Transformers/#summary","title":"Summary","text":"<p> The paper proposes a fast post-training pruning framework for Transformers that automatically prunes the model using structured sparsity methods. The framework introduces three novel techniques to retain high accuracy without retraining. The proposed method achieved up to 2.0 reduction in FLOPs and 1.56 speedup in inference latency while maintaining &lt;1% loss in accuracy. The method requires less than 3 minutes on a single GPU, making it over two orders of magnitude faster than existing pruning approaches that require retraining models."},{"location":"Quantization-Survey-Paper-Notes/nlp/A_Fast_Post-Training_Pruning_Framework_for_Transformers/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/A_Fast_Post-Training_Pruning_Framework_for_Transformers/#content","title":"Content","text":"<p>Pruning is an effective way to reduce the huge inference cost of Transformer models. However, prior work on pruning Transformers requires retraining the models. This can add high training cost and high complexity to model deployment, making it difficult to use in many practical situations. To address this, we propose a fast post-training pruning framework for Transformers that does not require any retraining. Given a resource constraint and a sample dataset, our framework automatically prunes the Transformer model using structured sparsity methods. To retain high accuracy without retraining, we introduce three novel techniques: (i) a lightweight mask search algorithm that finds which heads and filters to prune based on the Fisher information; (ii) mask rearrangement that complements the search algorithm; and (iii) mask tuning that reconstructs the output activations for each layer. We apply our method to BERT BASE and DistilBERT, and we evaluate its effectiveness on GLUE and SQuAD benchmarks. Our framework achieves up to 2.0 reduction in FLOPs and 1.56 speedup in inference latency, while maintaining &lt;1% loss in accuracy. Importantly, our framework prunes Transformers in less than 3 minutes on a single GPU, which is over two orders of magnitude faster than existing pruning approaches that retrain the models."},{"location":"Quantization-Survey-Paper-Notes/nlp/A_compact_and_language-sensitive_multilingual_translation_method/","title":"A compact and language-sensitive multilingual translation method","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/A_compact_and_language-sensitive_multilingual_translation_method/#summary","title":"Summary","text":"<p>Summary: The paper proposes a compact and language-sensitive method for multilingual translation using a universal representor to replace both encoder and decoder models, and introducing language-sensitive embedding, attention, and discriminator. This method outperforms strong standard multilingual translation systems in various scenarios, including one-to-many, many-to-many, and zero-shot, especially in low-resource and zero-shot translation scenarios.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/A_compact_and_language-sensitive_multilingual_translation_method/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/A_compact_and_language-sensitive_multilingual_translation_method/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/A_holistic_assessment_of_the_carbon_footprint_of_noor%2C_a_very_large_arabic_language_model/","title":"A holistic assessment of the carbon footprint of noor, a very large arabic language model","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/A_holistic_assessment_of_the_carbon_footprint_of_noor%2C_a_very_large_arabic_language_model/#summary","title":"Summary","text":"<p>Summary: The paper proposes a holistic assessment of the footprint of an extreme-scale language model. They assess the total carbon bill of the entire project which includes data collection, storage costs, research and development budgets, pretraining costs, and future serving estimates. The paper discusses pathways to reduce the carbon footprint of extreme-scale models.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/A_holistic_assessment_of_the_carbon_footprint_of_noor%2C_a_very_large_arabic_language_model/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/A_holistic_assessment_of_the_carbon_footprint_of_noor%2C_a_very_large_arabic_language_model/#content","title":"Content","text":"<p>As ever larger language models grow more ubiquitous, it is crucial to consider their environmental impact. Characterised by extreme size and resource use, recent generations of models have been criticised for their voracious appetite for compute, and thus significant carbon footprint. In this work, we propose a holistic assessment of the footprint of an extreme-scale language model, Noor. We assess the total carbon bill of the entire project, starting with data collection and storage costs, including research and development budgets, pretraining costs, future serving estimates, and other exogenous costs necessary for this international cooperation. Notably, we find that inference costs and exogenous factors can have a significant impact on the total budget. Finally, we discuss pathways to reduce the carbon footprint of extreme-scale models."},{"location":"Quantization-Survey-Paper-Notes/nlp/A_survey_on_model_compression_for_natural_language_processing/","title":"A survey on model compression for natural language processing","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/A_survey_on_model_compression_for_natural_language_processing/#summary","title":"Summary","text":"<p>Summary: This paper discusses the challenges in using Transformer-based pretrained language models (PLMs) for edge and mobile computing due to high energy cost and inference delay. The paper reviews the state of model compression and acceleration for PLMs, highlighting benchmarks, metrics and methodology, as part of efforts to consider computation, time and carbon emission in NLP.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/A_survey_on_model_compression_for_natural_language_processing/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/A_survey_on_model_compression_for_natural_language_processing/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Algorithm-hardware_co-design_of_adaptive_floating-point_encodings_for_resilient_deep_learning_inference/","title":"Algorithm-hardware co-design of adaptive floating-point encodings for resilient deep learning inference","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Algorithm-hardware_co-design_of_adaptive_floating-point_encodings_for_resilient_deep_learning_inference/#summary","title":"Summary","text":"<p> This paper presents an approach centered around a novel floating-point inspired number format, AdaptivFloat, for quantization of neural networks. The approach maximizes and optimally clips its available dynamic range at a layer granularity to create faithful encodings, exhibiting narrow to wide weight distribution, resulting in higher inference accuracies at low bit precision. Additionally, this paper presents experimental results that demonstrate per-operation energy and area that is 0.9x and 1.14x, respectively, that of an equivalent bit width NVDLA-like integer-based PE."},{"location":"Quantization-Survey-Paper-Notes/nlp/Algorithm-hardware_co-design_of_adaptive_floating-point_encodings_for_resilient_deep_learning_inference/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Algorithm-hardware_co-design_of_adaptive_floating-point_encodings_for_resilient_deep_learning_inference/#content","title":"Content","text":"<p> Conventional hardware-friendly quantization methods like fixed-point or integer don't perform well at low precision as their dynamic ranges are not able to capture the wide data distributions seen in sequence transcription models. In this paper, we present an algorithm-hardware co-design approach centered around a novel floating-point inspired number format, AdaptivFloat that dynamically maximizes and optimally clips its available dynamic range at a layer granularity in order to create faithful encodings of neural network parameters. The results show that AdaptivFloat consistently produces higher inference accuracies compared to block floating-point, uniform, IEEE-like float or posit encodings at low bit precision (&lt;=8-bit) across a diverse set of state-of-the-art neural networks, exhibiting narrow to wide weight distribution. At 4-bit weight precision, only a 2.1 degradation in BLEU score is observed on the AdaptivFloat-quantized Transformer network compared to total accuracy loss when encoded in the above-mentioned prominent datatypes. Furthermore, the paper presents experimental results on a DNN processing element (PE), exploiting AdaptivFloat logic in its computational datapath, that demonstrate per-operation energy and area that is 0.9x and 1.14x, respectively, that of an equivalent bit width NVDLA-like integer-based PE."},{"location":"Quantization-Survey-Paper-Notes/nlp/Align_and_prompt%3A_Video-and-language_pre-training_with_entity_prompts/","title":"Align and prompt: Video-and-language pre-training with entity prompts","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Align_and_prompt%3A_Video-and-language_pre-training_with_entity_prompts/#summary","title":"Summary","text":"<p>The paper proposes a new video-and-language pre-training framework called Align and Prompt (ALPRO) that aligns unimodal video-text features without explicit object detectors. This is achieved through a video-text contrastive (VTC) loss to align unimodal video-text features and a novel visually-grounded pre-training task that learns fine-grained alignment between visual region and text entity through an entity prompter module in a self-supervised way. The pre-trained model achieves state-of-the-art performance on both text-video retrieval and videoQA."},{"location":"Quantization-Survey-Paper-Notes/nlp/Align_and_prompt%3A_Video-and-language_pre-training_with_entity_prompts/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Align_and_prompt%3A_Video-and-language_pre-training_with_entity_prompts/#content","title":"Content","text":"<p>Video-and-language pre-training has shown promising improvements on various downstream tasks. Most previous methods capture cross-modal interactions with a standard transformer-based multimodal encoder, not fully addressing the misalignment between unimodal video and text features. In this paper, we propose Align and Prompt: a new video-and-language pre-training framework (ALPRO), which operates on sparsely-sampled video frames and achieves more effective cross-modal alignment without explicit object detectors. First, we introduce a video-text contrastive (VTC) loss to align unimodal video-text features at the instance level, which eases the modeling of cross-modal interactions. Then, we propose a novel visually-grounded pre-training task, prompting entity modeling (PEM), which learns fine-grained alignment between visual region and text entity via an entity prompter module in a self-supervised way. Finally, we pretrain the video-and-language transformer models on large webly-source video-text pairs using the proposed VTC and PEM losses as well as two standard losses of masked language modeling (MLM) and video-text matching (VTM). The resulting pre-trained model achieves state-of-the-art performance on both text-video retrieval and videoQA, outperforming prior work by a substantial margin."},{"location":"Quantization-Survey-Paper-Notes/nlp/An_efficiency_study_for_SPLADE_models/","title":"An efficiency study for SPLADE models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/An_efficiency_study_for_SPLADE_models/#summary","title":"Summary","text":"<p>Summary: This paper focuses on improving the efficiency of the SPLADE model, which has achieved state-of-the-art zero-shot performance and competitive results on TREC collections, by proposing various techniques, including L1 regularization for queries, a separation of document/query encoders, and the use of faster query encoders. The proposed models achieve similar latency as traditional BM25 and similar performance as state-of-the-art single-stage neural rankers on in-domain data while having drastically improved efficiency.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/An_efficiency_study_for_SPLADE_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/An_efficiency_study_for_SPLADE_models/#content","title":"Content","text":"<p>Latency and efficiency issues are often overlooked when evaluating IR models based on Pretrained Language Models (PLMs) in reason of multiple hardware and software testing scenarios. Nevertheless, efficiency is an important part of such systems and should not be overlooked. In this paper, we focus on improving the efficiency of the SPLADE model since it has achieved state-of-the-art zero-shot performance and competitive results on TREC collections. SPLADE efficiency can be controlled via a regularization factor, but solely controlling this regularization has been shown to not be efficient enough. In order to reduce the latency gap between SPLADE and traditional retrieval systems, we propose several techniques including L1 regularization for queries, a separation of document/query encoders, a FLOPS-regularized middle-training, and the use of faster query encoders. Our benchmark demonstrates that we can drastically improve the efficiency of these models while increasing the performance metrics on in-domain data. To our knowledge, we propose the first neural models that, under the same computing constraints, achieve similar latency (less than 4ms difference) as traditional BM25 , while having similar performance (less than 10% MRR@10 reduction) as the state-of-the-art single-stage neural rankers on in-domain data."},{"location":"Quantization-Survey-Paper-Notes/nlp/Analysis_of_joint_multilingual_sentence_representations_and_semantic_k-nearest_neighbor_graphs/","title":"Analysis of joint multilingual sentence representations and semantic k-nearest neighbor graphs","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Analysis_of_joint_multilingual_sentence_representations_and_semantic_k-nearest_neighbor_graphs/#summary","title":"Summary","text":"<p>Summary: The authors propose a joint multilingual sentence encoder and a k-nn graph over a large space of news sentences in seven languages, achieving excellent multilingual retrieval quality on the UN corpus of 11.3M sentences. Their multilingual encoder outperforms previous work on large-scale similarity search and they define new quantitative evaluation tasks to analyze the generalization behavior of multilingual sentence embeddings with respect to unseen domains and languages. Their system can also handle zero-shot transfers to several linguistically related languages without using any resources of those languages. The code used in this paper is freely available in the LASER toolkit.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Analysis_of_joint_multilingual_sentence_representations_and_semantic_k-nearest_neighbor_graphs/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Analysis_of_joint_multilingual_sentence_representations_and_semantic_k-nearest_neighbor_graphs/#content","title":"Content","text":"<p> Multilingual sentence and document representations have been gaining importance lately due to the desire to transfer NLP applications across different languages. In this paper, the authors investigate a joint multilingual sentence encoder and a k-nn graph over the space of 566 million news sentences in seven different languages. They provide a detailed analysis of both the multilingual sentence encoder and the learned graph, which show excellent multilingual retrieval quality on the UN corpus of 11.3M sentences. The authors show that their multilingual encoder outperforms previous work on large-scale similarity search and define new quantitative evaluation tasks to analyze the generalization behavior of multilingual sentence embeddings with respect to unseen domains and languages. They also demonstrate that their system can handle zero-shot transfers to several linguistically related languages without using any resources of those languages. The code used in this paper is freely available in the LASER toolkit."},{"location":"Quantization-Survey-Paper-Notes/nlp/Attribute_propagation_network_for_graph_zero-shot_learning/","title":"Attribute propagation network for graph zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Attribute_propagation_network_for_graph_zero-shot_learning/#summary","title":"Summary","text":"<p>Summary: The paper proposes an attribute propagation network (APNet) to improve zero-shot learning (ZSL) by refining the semantic attributes of each class based on its neighbors and related classes on a graph of classes. The APNet is composed of a graph propagation model and a parameterized nearest neighbor (NN) classifier. The paper introduces a meta-learning strategy to train the propagation mechanism and the similarity metric for the NN classifier on multiple sub-graphs. The APNet achieves compelling performance or new state-of-the-art results in experiments with two ZSL settings and five benchmark datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Attribute_propagation_network_for_graph_zero-shot_learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Attribute_propagation_network_for_graph_zero-shot_learning/#content","title":"Content","text":"<p> The paper proposes an attribute propagation network (APNet) to optimize the attribute space for zero-shot learning (ZSL) by training a propagation mechanism to refine the semantic attributes of each class based on its neighbors and related classes on a graph of classes. The propagated attributes can produce classifiers for zero-shot classes with significantly improved performance in different ZSL settings. The APNet is composed of a graph propagation model generating attribute vectors for each class and a parameterized nearest neighbor (NN) classifier categorizing an image to the class with the nearest attribute vector to the image\u2019s embedding. The paper also introduces a meta-learning strategy to train the propagation mechanism and the similarity metric for the NN classifier on multiple sub-graphs, each associated with a classification task over a subset of training classes. The APNet achieves compelling performance or new state-of-the-art results in experiments with two zero-shot learning settings and five benchmark datasets."},{"location":"Quantization-Survey-Paper-Notes/nlp/Automatic_mixed-precision_quantization_search_of_BERT/","title":"Automatic mixed-precision quantization search of BERT","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Automatic_mixed-precision_quantization_search_of_BERT/#summary","title":"Summary","text":"<p>Summary: The paper proposes an automatic mixed-precision quantization framework for BERT which conducts quantization and pruning in a subgroup-wise level. This approach leads to a much smaller model size without sacrificing performance, making it more practical for deployment on resource-constrained devices.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Automatic_mixed-precision_quantization_search_of_BERT/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Automatic_mixed-precision_quantization_search_of_BERT/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Avoiding_inference_heuristics_in_few-shot_prompt-based_finetuning/","title":"Avoiding inference heuristics in few-shot prompt-based finetuning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Avoiding_inference_heuristics_in_few-shot_prompt-based_finetuning/#summary","title":"Summary","text":"<p>Summary: The paper discusses the issue of prompt-based models in sentence pair classification tasks that adopt inference heuristics based on lexical overlap, which causes incorrect assumptions of sentence meaning in low data regimes. The paper suggests that this issue can be mitigated by adding regularization that preserves pre-training weights. The evaluation on three datasets shows promising improvements in mitigating the destructive tendency of few-shot finetuning.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Avoiding_inference_heuristics_in_few-shot_prompt-based_finetuning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Avoiding_inference_heuristics_in_few-shot_prompt-based_finetuning/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Beyond_attributes%3A_High-order_attribute_features_for_zero-shot_learning/","title":"Beyond attributes: High-order attribute features for zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Beyond_attributes%3A_High-order_attribute_features_for_zero-shot_learning/#summary","title":"Summary","text":"<p> The authors propose a zero-shot learning framework, SeeNet-HAF, that utilizes high-order attribute features to obtain discriminative semantic vectors for each class. SeeNet-HAF consists of two streams for dynamic object region localization and complementary object region discovery. The authors also discuss various approaches for establishing the correlation between image and semantic spaces in zero-shot learning and compare different methods for learning a projection function. Experimental results on four benchmark datasets demonstrate the effectiveness of SeeNet-HAF over existing approaches."},{"location":"Quantization-Survey-Paper-Notes/nlp/Beyond_attributes%3A_High-order_attribute_features_for_zero-shot_learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Beyond_attributes%3A_High-order_attribute_features_for_zero-shot_learning/#content","title":"Content","text":"<p>In this paper, the authors propose SeeNet-HAF, a framework for zero-shot learning (ZSL) that integrates high-order attribute features (HAF) to obtain discriminative semantic vectors for each class. SeeNet-HAF consists of two streams, one for dynamic object region localization and the other for complementary object region discovery. The proposed framework also includes a fast hyperparameter search strategy. Experiments on four benchmark datasets demonstrate the superiority of SeeNet-HAF over existing methods. The paper discusses various approaches for establishing the correlation between image and semantic spaces in ZSL and highlights representative methods based on matrix optimization and convolutional neural networks. The authors describe three groups of methods for learning a projection function from visual feature to semantic space, and compare various approaches within each group, such as SOC classifier, ALE, DeViSE, and SJE.  </p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Beyond_preserved_accuracy%3A_Evaluating_loyalty_and_robustness_of_BERT_compression/","title":"Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Beyond_preserved_accuracy%3A_Evaluating_loyalty_and_robustness_of_BERT_compression/#summary","title":"Summary","text":"<p>Summary: This paper proposes two new metrics for evaluating compressed language models that measure how closely the compressed model mimics the original model. The paper also examines the effects of compression on robustness under adversarial attacks.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Beyond_preserved_accuracy%3A_Evaluating_loyalty_and_robustness_of_BERT_compression/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Beyond_preserved_accuracy%3A_Evaluating_loyalty_and_robustness_of_BERT_compression/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Compression_of_deep_learning_models_for_text%3A_A_survey/","title":"Compression of deep learning models for text: A survey","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Compression_of_deep_learning_models_for_text%3A_A_survey/#summary","title":"Summary","text":"<p>Summary: This survey discusses the compression of deep learning models used for Natural Language Processing (NLP) and Information Retrieval (IR) tasks for real-world applications. The authors mention various models such as BERT, GPT-2, MT-DNN, XLNet, T5, T-NLG, and GShard and list six different methods for compression such as pruning, quantization, knowledge distillation, parameter sharing, tensor decomposition, and sub-quadratic transformer based methods in order to achieve small model size and low response times. The survey aims to present a coherent story of the work done by the deep learning for NLP community in the past few years.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Compression_of_deep_learning_models_for_text%3A_A_survey/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Compression_of_deep_learning_models_for_text%3A_A_survey/#content","title":"Content","text":"<p> In this survey, the authors discuss the methods of compressing deep learning models used for natural language processing (NLP) and information retrieval (IR) tasks. The authors mention the various types of models which have made significant contributions to this field such as BERT, GPT-2, MT-DNN, XLNet, T5, T-NLG, and GShard. But, in real-world applications, small model size and low response times are required. Hence, the authors discuss six different types of methods for compression of such models, which include pruning, quantization, knowledge distillation, parameter sharing, tensor decomposition, and sub-quadratic transformer based methods. The authors believe that this survey organizes the plethora of work done by the \u2018deep learning for NLP\u2019 community in the past few years and presents it as a coherent story."},{"location":"Quantization-Survey-Paper-Notes/nlp/ConveRT%3A_Efficient_and_accurate_conversational_representations_from_transformers/","title":"ConveRT: Efficient and accurate conversational representations from transformers","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/ConveRT%3A_Efficient_and_accurate_conversational_representations_from_transformers/#summary","title":"Summary","text":"<p> This paper introduces ConveRT, a pre-training framework for conversational AI applications that is computationally efficient and fast to train. It uses quantization and subword-level parameterization to build a lightweight and energy-efficient model with state-of-the-art performance in response selection tasks. Furthermore, the use of extended dialog history as context yields further performance gains. It can also be transferred to the intent classification task with strong results. ConveRT trains substantially faster than standard sentence encoders or previous state-of-the-art dual encoders, making it more portable and scalable for conversational AI applications."},{"location":"Quantization-Survey-Paper-Notes/nlp/ConveRT%3A_Efficient_and_accurate_conversational_representations_from_transformers/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/ConveRT%3A_Efficient_and_accurate_conversational_representations_from_transformers/#content","title":"Content","text":"<p> General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pre-training framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-level parameterization in the dual encoder to build a lightweight memory- and energy-efficient model. We show that ConveRT achieves state-of-the-art performance across widely established response selection tasks. We also demonstrate that the use of extended dialog history as context yields further performance gains. Finally, we show that pretrained representations from the proposed encoder can be transferred to the intent classification task, yielding strong results across three diverse datasets. ConveRT trains substantially faster than standard sentence encoders or previous state-of-the-art dual encoders. With its reduced size and superior performance, we believe this model promises wider portability and scalability for Conversational AI applications."},{"location":"Quantization-Survey-Paper-Notes/nlp/Crossing_the_format_boundary_of_text_and_boxes%3A_Towards_unified_vision-language_modeling/","title":"Crossing the format boundary of text and boxes: Towards unified vision-language modeling","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Crossing_the_format_boundary_of_text_and_boxes%3A_Towards_unified_vision-language_modeling/#summary","title":"Summary","text":"<p> The paper proposes a UniTAB model that unifies text and box outputs for grounded vision-language modeling. The model generates a text and aligns predicted words with object regions using a shared token sequence, introducing a special \"\" token to indicate word-box alignments naturally. UniTAB provides a more comprehensive and interpretable image description, outperforming state-of-the-art models in grounded captioning evaluations. In general VL tasks, UniTAB achieves better or comparable performance than task-specific models."},{"location":"Quantization-Survey-Paper-Notes/nlp/Crossing_the_format_boundary_of_text_and_boxes%3A_Towards_unified_vision-language_modeling/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Crossing_the_format_boundary_of_text_and_boxes%3A_Towards_unified_vision-language_modeling/#content","title":"Content","text":"<p> We propose UniTAB that Unifies Text And Box outputs for grounded vision-language (VL) modeling. Grounded VL tasks such as grounded captioning require the model to generate a text description and align predicted words with object regions. To achieve this, models must generate desired text and box outputs together, and meanwhile indicate the alignments between words and boxes. UniTAB represents both text and box outputs with a shared token sequence, and introduces a special  token to naturally indicate word-box alignments in the sequence. UniTAB thus could provide a more comprehensive and interpretable image description, by freely grounding generated words to object regions. On grounded captioning, UniTAB presents a simpler solution with a single output head, and significantly outperforms state of the art in both grounding and captioning evaluations. On general VL tasks that have different desired output formats ( i.e., text, box, or their combination), UniTAB with a single network achieves better or comparable performance than task-specific state of the art."},{"location":"Quantization-Survey-Paper-Notes/nlp/DQ-BART%3A_Efficient_Sequence-to-Sequence_Model_via_Joint_Distillation_and_Quantization/","title":"DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/DQ-BART%3A_Efficient_Sequence-to-Sequence_Model_via_Joint_Distillation_and_Quantization/#summary","title":"Summary","text":"<p> This paper proposes a method to jointly distill and quantize large-scale pre-trained sequence-to-sequence models, such as BART and T5, to alleviate memory requirements and high latency in resource-constrained scenarios. The proposed method transfers knowledge from full-precision teacher models to quantized and distilled low-precision student models. Empirical analyses show that this method achieves a 16.5x model footprint compression ratio with little performance drop, relative to full-precision models, on multiple summarization and QA datasets. This study pushes the limit of compression ratio to 27.7x and demonstrates the performance-efficiency trade-off for generative tasks using pre-trained models. This is the first work that aims to effectively distill and quantize sequence-to-sequence pre-trained models for language generation tasks."},{"location":"Quantization-Survey-Paper-Notes/nlp/DQ-BART%3A_Efficient_Sequence-to-Sequence_Model_via_Joint_Distillation_and_Quantization/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/DQ-BART%3A_Efficient_Sequence-to-Sequence_Model_via_Joint_Distillation_and_Quantization/#content","title":"Content","text":"<p>Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve state-of-the-art performance on many generative NLP tasks. However, such models pose a great challenge in resource-constrained scenarios owing to their large memory requirements and high latency. To alleviate this issue, we propose to jointly distill and quantize the model, where knowledge is transferred from the full-precision teacher model to the quantized and distilled low-precision student model. Empirical analyses show that, despite the challenging nature of generative tasks, we were able to achieve a 16.5x model footprint compression ratio with little performance drop relative to the full-precision counterparts on multiple summarization and QA datasets. We further pushed the limit of compression ratio to 27.7x and presented the performance-efficiency trade-off for generative tasks using pre-trained models. To the best of our knowledge, this is the first work aiming to effectively distill and quantize sequence-to-sequence pretrained models for language generation tasks."},{"location":"Quantization-Survey-Paper-Notes/nlp/Domain_adaptation_for_memory-efficient_dense_retrieval/","title":"Domain adaptation for memory-efficient dense retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Domain_adaptation_for_memory-efficient_dense_retrieval/#summary","title":"Summary","text":"<p>Summary: The paper proposes a modification to the training procedure of binary embedding models such as BPR and JPQ, which enables their adaptation to any corpus without requiring labeled data. The proposed method, called GPL, improves the nDCG@10 metric across the BEIR benchmark by up to 19.3 and 11.6 points compared to BPR and JPQ while maintaining 32x memory efficiency.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Domain_adaptation_for_memory-efficient_dense_retrieval/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Domain_adaptation_for_memory-efficient_dense_retrieval/#content","title":"Content","text":"<p>Dense retrievers encode documents into fixed dimensional embeddings. Recently, binary embedding models like BPR and JPQ have been proposed which train the model to produce binary document vectors, reducing the index 32x or more, achieving an improvement in nDCG@10 across the BEIR benchmark. However, these models can perform significantly worse than baselines once there is a domain-shift involved. In this paper, the authors propose a modification to the training procedure of BPR and JPQ and combine it with a corpus specific generative procedure which enables the adaptation of BPR and JPQ to any corpus without requiring labeled training data. Their domain adapted strategy known as GPL is model agnostic, achieving an improvement by up to 19.3 and 11.6 points in nDCG@10 across the BEIR benchmark in comparison to BPR and JPQ while maintaining its 32x memory efficiency."},{"location":"Quantization-Survey-Paper-Notes/nlp/Domain_segmentation_and_adjustment_for_generalized_zero-shot_learning/","title":"Domain segmentation and adjustment for generalized zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Domain_segmentation_and_adjustment_for_generalized_zero-shot_learning/#summary","title":"Summary","text":"<p>The paper proposes a method for generalized zero-shot recognition to address the imbalance between training data for seen and unseen classes. The approach uses a threshold and probabilistic distribution joint method to segment testing instances into seen, unseen, and uncertain domains. The uncertain domain is further adjusted to address domain shift. Experiments on five benchmark datasets demonstrate the competitive performance of the approach."},{"location":"Quantization-Survey-Paper-Notes/nlp/Domain_segmentation_and_adjustment_for_generalized_zero-shot_learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Domain_segmentation_and_adjustment_for_generalized_zero-shot_learning/#content","title":"Content","text":"<p>In this paper, the authors propose a method to realize the generalized zero-shot recognition in different domains to address the imbalance of training data between seen and unseen classes. They argue that synthesizing unseen data may not be an ideal approach for addressing the domain shift caused by the imbalance of the training data. The authors propose a threshold and probabilistic distribution joint method to segment the testing instances into seen, unseen and uncertain domains. Moreover, the uncertain domain is further adjusted to alleviate the domain shift. Extensive experiments on five benchmark datasets show that the proposed method exhibits competitive performance compared with that based on generative models."},{"location":"Quantization-Survey-Paper-Notes/nlp/ERNIE-ViLG%3A_Unified_generative_pre-training_for_bidirectional_vision-language_generation/","title":"ERNIE-ViLG: Unified generative pre-training for bidirectional vision-language generation","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/ERNIE-ViLG%3A_Unified_generative_pre-training_for_bidirectional_vision-language_generation/#summary","title":"Summary","text":"<p>The paper proposes ERNIE-ViLG, a generative pre-training framework for bidirectional image-text generation using transformer models. The framework explores large-scale pre-training, achieving state-of-the-art performance on text-to-image synthesis and image captioning tasks, with an FID of 7.9 on MS-COCO."},{"location":"Quantization-Survey-Paper-Notes/nlp/ERNIE-ViLG%3A_Unified_generative_pre-training_for_bidirectional_vision-language_generation/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/ERNIE-ViLG%3A_Unified_generative_pre-training_for_bidirectional_vision-language_generation/#content","title":"Content","text":"<p>In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning."},{"location":"Quantization-Survey-Paper-Notes/nlp/Effective_quantization_methods_for_recurrent_neural_networks/","title":"Effective quantization methods for recurrent neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Effective_quantization_methods_for_recurrent_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper proposes a method for quantizing the structure of gates and interlinks in LSTM and GRU cells, as well as balanced quantization methods for weights to reduce performance degradation. The experiments on PTB and IMDB datasets prove the effectiveness of these methods and show that the performance matches or surpasses the previous state-of-the-art of quantized RNN.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Effective_quantization_methods_for_recurrent_neural_networks/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Effective_quantization_methods_for_recurrent_neural_networks/#content","title":"Content","text":"<p> Reducing bit-widths of weights, activations, and gradients of a Neural Network can shrink its storage size and memory usage, and also allow for faster training and inference by exploiting bitwise operations. However, previous attempts for quantization of RNNs show considerable performance degradation when using low-bit-width weights and activations. In this paper, we propose methods to quantize the structure of gates and interlinks in LSTM and GRU cells. In addition, we propose balanced quantization methods for weights to further reduce performance degradation. Experiments on PTB and IMDB datasets confirm effectiveness of our methods as performances of our models match or surpass the previous state-of-the-art of quantized RNN."},{"location":"Quantization-Survey-Paper-Notes/nlp/Event-triggered_H%E2%88%9E_filter_design_for_delayed_neural_network_with_quantization/","title":"Event-triggered H\u221e filter design for delayed neural network with quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Event-triggered_H%E2%88%9E_filter_design_for_delayed_neural_network_with_quantization/#summary","title":"Summary","text":"<p>Summary: This paper proposes a new event-triggered communication scheme for quantized neural network systems, using a logarithmic quantizer to reduce data transmission rate. The study investigates the H\u221e filter design problem for a class of event-triggered neural network systems with quantization, presenting delay-dependent stability conditions for the designed filter parameters. The results are validated through numerical examples.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Event-triggered_H%E2%88%9E_filter_design_for_delayed_neural_network_with_quantization/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Event-triggered_H%E2%88%9E_filter_design_for_delayed_neural_network_with_quantization/#content","title":"Content","text":"<p>This paper is concerned with H\u221e filter design for a class of neural network systems with event-triggered communication scheme and quantization. Firstly, a new event-triggered communication scheme is introduced to determine whether or not the current sampled sensor data should be broadcasted and transmitted to quantizer, which can save the limited communication resource. Secondly, a logarithmic quantizer is used to quantify the sampled data, which can reduce the data transmission rate in the network. Thirdly, considering the influence of the constrained network resource, we investigate the problem of H\u221e filter design for a class of event-triggered neural network systems with quantization. By using Lyapunov functional and linear matrix inequality (LMI) techniques, some delay-dependent stability conditions for the existence of the desired filter are obtained. Furthermore, the explicit expression is given for the designed filter parameters in terms of LMIs. Finally, a numerical example is given to show the usefulness of the obtained theoretical results."},{"location":"Quantization-Survey-Paper-Notes/nlp/Extreme_zero-shot_learning_for_extreme_text_classification/","title":"Extreme zero-shot learning for extreme text classification","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Extreme_zero-shot_learning_for_extreme_text_classification/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new pre-training method called MACLR to address the Extreme Zero-Shot XMC problem which requires no supervision and has access only to raw text of instances and labels. The proposed method utilizes self-supervised contrastive losses, Multi-scale Adaptive Clustering, Label Regularization, and self-training with pseudo-positive pairs. The authors show that the proposed method achieves superior performance compared to other baseline methods and fine-tuning the pre-trained encoder on a few-shot subset improves the performance on the Few-Shot XMC setup.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Extreme_zero-shot_learning_for_extreme_text_classification/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Extreme_zero-shot_learning_for_extreme_text_classification/#content","title":"Content","text":"<p> In this research paper, the authors address the eXtreme Multi-label text Classification (XMC) problem, which involves finding the most relevant labels for a given input text instance from a large label set. They propose a new scenario called Extreme Zero-Shot XMC (EZ-XMC) that requires no supervision and only has access to raw text of instances and labels. To address this problem, they propose a pre-training method called MACLR that utilizes self-supervised contrastive losses and other techniques such as Multi-scale Adaptive Clustering, Label Regularization, and self-training with pseudo-positive pairs. The authors evaluate the proposed method on four public EZ-XMC datasets and show that it achieves superior performance compared to other baseline methods with approximately 5-10% improvement in precision and recall on average. They also show that fine-tuning the pre-trained encoder on a few-shot subset improves the performance on the Few-Shot XMC (FS-XMC) setup."},{"location":"Quantization-Survey-Paper-Notes/nlp/From_zero-shot_learning_to_conventional_supervised_classification%3A_Unseen_visual_data_synthesis/","title":"From zero-shot learning to conventional supervised classification: Unseen visual data synthesis","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/From_zero-shot_learning_to_conventional_supervised_classification%3A_Unseen_visual_data_synthesis/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Zero-shot learning (ZSL) framework for synthesizing visual features for classes without real images, using the Unseen Visual Data Synthesis (UVDS) algorithm that utilizes semantic attributes as an intermediate clue. This converts ZSL recognition into a supervised problem and improves state-of-the-art results on benchmark datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/From_zero-shot_learning_to_conventional_supervised_classification%3A_Unseen_visual_data_synthesis/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/From_zero-shot_learning_to_conventional_supervised_classification%3A_Unseen_visual_data_synthesis/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Fully_quantized_transformer_for_improved_translation/","title":"Fully quantized transformer for improved translation","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Fully_quantized_transformer_for_improved_translation/#summary","title":"Summary","text":"<p> The paper proposes a quantization strategy inclusive of all components of the Transformer, and it is the first to show that it is possible to avoid any loss in translation quality with a fully quantized network. The 8-bit models consistently score equal or higher BLEU than the full-precision variant on multiple translation datasets, and achieved state-of-the-art quantization results when compared to all previously proposed methods."},{"location":"Quantization-Survey-Paper-Notes/nlp/Fully_quantized_transformer_for_improved_translation/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Fully_quantized_transformer_for_improved_translation/#content","title":"Content","text":"<p> State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To the best of our knowledge, we are the \ufb01rst to propose a quantization strategy inclusive of all components of the Transformer (Vaswani et al., 2017). We are also the \ufb01rst to show that it is possible to avoid any loss in translation quality with a fully quantized network. Indeed, our 8-bit models consistently score equal or higher BLEU than the full-precision variant on multiple translation datasets. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results."},{"location":"Quantization-Survey-Paper-Notes/nlp/Godiva%3A_Generating_open-domain_videos_from_natural_descriptions/","title":"Godiva: Generating open-domain videos from natural descriptions","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Godiva%3A_Generating_open-domain_videos_from_natural_descriptions/#summary","title":"Summary","text":"<p>This paper proposes GODIVA, an open-domain text-to-video pretrained model that can generate videos from text using a three-dimensional sparse attention mechanism. It shows that GODIVA can be fine-tuned on downstream video generation tasks and has a good zero-shot capability. A new metric called Relative Matching (RM) is also introduced to automatically evaluate the video generation quality. The paper discusses several challenges for future work."},{"location":"Quantization-Survey-Paper-Notes/nlp/Godiva%3A_Generating_open-domain_videos_from_natural_descriptions/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Godiva%3A_Generating_open-domain_videos_from_natural_descriptions/#content","title":"Content","text":"<p>Generating videos from text is a challenging task due to its high computational requirements for training and infinite possible answers for evaluation. Existing works typically experiment on simple or small datasets, where the generalization ability is quite limited. In this work, we propose GODIVA, an open-domain text-to-video pretrained model that can generate videos from text in an auto-regressive manner using a three-dimensional sparse attention mechanism. Experiments show that GODIVA not only can be fine-tuned on downstream video generation tasks but also has a good zero-shot capability on unseen texts. We also propose a new metric called Relative Matching (RM) to automatically evaluate the video generation quality. Several challenges are listed and discussed as future work."},{"location":"Quantization-Survey-Paper-Notes/nlp/I-bert%3A_Integer-only_bert_quantization/","title":"I-bert: Integer-only bert quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/I-bert%3A_Integer-only_bert_quantization/#summary","title":"Summary","text":"<p>This paper proposes I-BERT, a quantization scheme for Transformer-based models that utilizes integer-only arithmetic for the whole inference, unlike previous quantization schemes, which employ simulated quantization. The authors demonstrate that their method achieves similar or slightly higher accuracy compared to the full-precision baseline while providing a speedup of 2.4-4.0 for INT8 inference on a T4 GPU system as compared to FP32 inference."},{"location":"Quantization-Survey-Paper-Notes/nlp/I-bert%3A_Integer-only_bert_quantization/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/I-bert%3A_Integer-only_bert_quantization/#content","title":"Content","text":"<p>Transformer-based models such as BERT and RoBERTa have shown impressive performance in many NLP tasks. However, their memory footprint, inference latency, and power consumption make them inefficient for deployment at the edge or even in data centers, especially for real-time inference. One solution to this problem is quantization which compresses NN models by representing parameters and/or activations with low bit precision, such as 8-bit integer (INT8) instead of 32-bit floating-point (FP32). However, previous quantization schemes for Transformer based models use simulated quantization, which carries out operations using floating-point arithmetic during the inference, preventing efficient use of integer-only logical units. In this work, we propose I-BERT, a novel quantization scheme for Transformer-based models that uses only integer-only arithmetic for the entire inference. I-BERT uses lightweight integer-only approximation methods for nonlinear operations such as GELU, Softmax, and Layer Normalization. We evaluate I-BERT on GLUE downstream tasks using RoBERTa-Base/Large and show that I-BERT achieves similar (and slightly higher) accuracy compared to the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4-4.0 for INT8 inference on a T4 GPU system as compared to FP32 inference. </p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Improved_vector_quantized_diffusion_models/","title":"Improved vector quantized diffusion models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Improved_vector_quantized_diffusion_models/#summary","title":"Summary","text":"<p>Summary: The paper proposes two techniques to improve the sample quality of Vector quantized diffusion (VQ-Diffusion) in text-to-image synthesis. It explores a more general and effective implementation of the classi\ufb01er-free guidance sampling for discrete denoising diffusion model and presents a high-quality inference strategy to alleviate the joint distribution issue. The experiments on various datasets validate the effectiveness of the proposed techniques and show that the improved VQ-Diffusion outperforms the vanilla version by a large margin.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Improved_vector_quantized_diffusion_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Improved_vector_quantized_diffusion_models/#content","title":"Content","text":"<p> Vector quantized diffusion (VQ-Diffusion) is a powerful generative model for text-to-image synthesis, but sometimes can still generate low-quality samples or weakly correlated images with text input. We \ufb01nd these issues are mainly due to the \ufb02awed sampling strategy. In this paper, we propose two important techniques to further improve the sample quality of VQ-Diffusion. 1) We explore classi\ufb01er-free guidance sampling for discrete denoising diffusion model and propose a more general and effective implementation of classi\ufb01er-free guidance. 2) We present a high-quality inference strategy to alleviate the joint distribution issue in VQ-Diffusion. Finally, we conduct experiments on various datasets to validate their effectiveness and show that the improved VQ-Diffusion suppresses the vanilla version by large margins."},{"location":"Quantization-Survey-Paper-Notes/nlp/Improving_passage_retrieval_with_zero-shot_question_generation/","title":"Improving passage retrieval with zero-shot question generation","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Improving_passage_retrieval_with_zero-shot_question_generation/#summary","title":"Summary","text":"<p>The proposed paper presents a re-ranking method which uses a zero-shot question generation model to improve passage retrieval in open question-answering systems. It is a simple and effective approach that can be applied to any retrieval method and provides rich cross-attention between query and passage. It does not require any domain or task-specific training and provides new state-of-the-art results on full open-domain question answering."},{"location":"Quantization-Survey-Paper-Notes/nlp/Improving_passage_retrieval_with_zero-shot_question_generation/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Improving_passage_retrieval_with_zero-shot_question_generation/#content","title":"Content","text":"<p>We propose a simple and effective re-ranking method for improving passage retrieval in open question answering. The re-ranker re-scores retrieved passages with a zero-shot question generation model, which uses a pre-trained language model to compute the probability of the input question conditioned on a retrieved passage. This approach can be applied on top of any retrieval method (e.g. neural or keyword-based), does not require any domain- or task-specific training (and therefore is expected to generalize better to data distribution shifts), and provides rich cross-attention between query and passage (i.e. it must explain every token in the question). When evaluated on a number of open-domain retrieval datasets, our re-ranker improves strong unsupervised retrieval models by 6%-18% absolute and strong supervised models by up to 12% in terms of top-20 passage retrieval accuracy. We also obtain new state-of-the-art results on full open-domain question answering by simply adding the new re-ranker to existing models with no further changes."},{"location":"Quantization-Survey-Paper-Notes/nlp/Improving_the_Cross-Lingual_Generalisation_in_Visual_Question_Answering/","title":"Improving the Cross-Lingual Generalisation in Visual Question Answering","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Improving_the_Cross-Lingual_Generalisation_in_Visual_Question_Answering/#summary","title":"Summary","text":"<p>The paper proposes three strategies to improve cross-lingual transfer of multilingual vision-language pretrained models in a zero-shot cross-lingual VQA task, including introducing a linguistic prior objective, learning a task-specific subnetwork, and augmenting training examples using synthetic code-mixing. These strategies are shown to be effective in improving performance and outperform existing methods with sparse models on 7 typologically diverse languages."},{"location":"Quantization-Survey-Paper-Notes/nlp/Improving_the_Cross-Lingual_Generalisation_in_Visual_Question_Answering/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Improving_the_Cross-Lingual_Generalisation_in_Visual_Question_Answering/#content","title":"Content","text":"<p>In this research paper, the authors address the low performance of multilingual vision-language pretrained models on cross-lingual visual question answering (VQA) tasks. They explore the poor performance of such models on a zero-shot cross-lingual VQA task, where the models are fine-tuned on English visual-question data and evaluated on 7 typologically diverse languages. They propose three strategies to improve cross-lingual transfer: (1) introducing a linguistic prior objective to augment the cross-entropy loss, (2) learning a task-specific subnetwork to improve cross-lingual generalisation, and (3) augmenting training examples using synthetic code-mixing to promote alignment of embeddings between source and target languages. Their experiments on xGQA demonstrate the effectiveness of the proposed fine-tuning strategy for 7 languages, outperforming existing transfer methods with sparse models."},{"location":"Quantization-Survey-Paper-Notes/nlp/KDLSQ-BERT%3A_A_quantized_bert_combining_knowledge_distillation_with_learned_step_size_quantization/","title":"KDLSQ-BERT: A quantized bert combining knowledge distillation with learned step size quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/KDLSQ-BERT%3A_A_quantized_bert_combining_knowledge_distillation_with_learned_step_size_quantization/#summary","title":"Summary","text":"<p>The paper proposes a novel quantization method named KDLSQ-BERT which combines knowledge distillation with learned step size quantization for language model quantization. The model was tested on GLUE benchmark and SQuAD and showcases effective performance, even outperforming existing BERT quantization methods while maintaining comparable performance to the full-precision baseline model with 14.9x compression ratio."},{"location":"Quantization-Survey-Paper-Notes/nlp/KDLSQ-BERT%3A_A_quantized_bert_combining_knowledge_distillation_with_learned_step_size_quantization/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/KDLSQ-BERT%3A_A_quantized_bert_combining_knowledge_distillation_with_learned_step_size_quantization/#content","title":"Content","text":"<p>Recently, transformer-based language models such as BERT have shown tremendous performance improvement for a range of natural language processing tasks. However, these language models usually are computation expensive and memory intensive during inference. To improve the inference performance, as well as reduce the model size while maintaining the model accuracy, we propose a novel quantization method named KDLSQ-BERT that combines knowledge distillation (KD) with learned step size quantization (LSQ) for language model quantization. The main idea of our method is that the KD technique is leveraged to transfer the knowledge from a \u201dteacher\u201d model to a \u201dstudent\u201d model when exploiting LSQ to quantize that \u201dstudent\u201d model during the quantization training process. Extensive experiment results on GLUE benchmark and SQuAD demonstrate that our proposed KDLSQ-BERT not only performs effectively when doing different bit (e.g. 2-bit\u00188-bit) quantization, but also outperforms the existing BERT quantization methods, and even achieves comparable performance as the full-precision base-line model while obtaining 14.9x compression ratio."},{"location":"Quantization-Survey-Paper-Notes/nlp/Knowledgeable_prompt-tuning%3A_Incorporating_knowledge_into_prompt_verbalizer_for_text_classification/","title":"Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Knowledgeable_prompt-tuning%3A_Incorporating_knowledge_into_prompt_verbalizer_for_text_classification/#summary","title":"Summary","text":"<p>Summary: The paper proposes knowledgeable prompt-tuning (KPT) to improve and stabilize prompt-tuning for text classification by incorporating external knowledge into the verbalizer through expansion and refinement of the label word space. The approach is evaluated through extensive experiments on zero and few-shot text classification tasks, showing its effectiveness.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Knowledgeable_prompt-tuning%3A_Incorporating_knowledge_into_prompt_verbalizer_for_text_classification/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Knowledgeable_prompt-tuning%3A_Incorporating_knowledge_into_prompt_verbalizer_for_text_classification/#content","title":"Content","text":"<p>Abstract: Tuning pre-trained language models with task-specific prompts has been a promising approach for text classification. In this work, the authors propose a knowledgeable prompt-tuning (KPT) approach to improve and stabilize prompt-tuning by incorporating external knowledge into the verbalizer. The authors expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/L-verse%3A_Bidirectional_generation_between_image_and_text/","title":"L-verse: Bidirectional generation between image and text","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/L-verse%3A_Bidirectional_generation_between_image_and_text/#summary","title":"Summary","text":"<p>Summary: L-Verse is a new architecture that combines an AugVAE and BiART to generate text from images and images from text. It outperforms previous models in both image-to-text and text-to-image generation on the MS-COCO Captions dataset, and shows impressive results on Conceptual Captions. The architecture can be used directly without finetuning or an extra object detection framework.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/L-verse%3A_Bidirectional_generation_between_image_and_text/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/L-verse%3A_Bidirectional_generation_between_image_and_text/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Laion-5b%3A_An_open_large-scale_dataset_for_training_next_generation_image-text_models/","title":"Laion-5b: An open large-scale dataset for training next generation image-text models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Laion-5b%3A_An_open_large-scale_dataset_for_training_next_generation_image-text_models/#summary","title":"Summary","text":"<p>Summary: The paper presents LAION-5B, a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32 billion pairs contain English language, which was created to study the training and capabilities of large-scale language-vision models. The dataset enables successful replication and fine-tuning of foundational models and facilitates further experiments. The paper also includes several nearest neighbor indices, an improved web-interface, and detection scores for watermark, NSFW, and toxic content detection.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Laion-5b%3A_An_open_large-scale_dataset_for_training_next_generation_image-text_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Laion-5b%3A_An_open_large-scale_dataset_for_training_next_generation_image-text_models/#content","title":"Content","text":"<p> Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classi\ufb01cation with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-\ufb01ltered image-text pairs, of which 2.32B contain English language. We show successful replication and \ufb01ne-tuning of foundational models like CLIP, GLIDE and Stable Di\ufb00usion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection."},{"location":"Quantization-Survey-Paper-Notes/nlp/Learning_an_Artificial_Language_for_Knowledge-Sharing_in_Multilingual_Translation/","title":"Learning an Artificial Language for Knowledge-Sharing in Multilingual Translation","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Learning_an_Artificial_Language_for_Knowledge-Sharing_in_Multilingual_Translation/#summary","title":"Summary","text":"<p>Summary: This paper proposes a method to discretize the encoder output latent space of multilingual models by assigning encoder states to entries in a codebook to improve the learning of a common representation, which increases robustness in unseen testing conditions. The approach is validated through large-scale experiments and is competitive with strong alternatives from literature in zero-shot conditions. Furthermore, the authors use the learned artificial language to analyze model behavior and find that using a similar bridge language increases knowledge-sharing among the remaining languages.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Learning_an_Artificial_Language_for_Knowledge-Sharing_in_Multilingual_Translation/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Learning_an_Artificial_Language_for_Knowledge-Sharing_in_Multilingual_Translation/#content","title":"Content","text":"<p>The cornerstone of multilingual neural translation is shared representations across languages. Given the theoretically infinite representation power of neural networks, semantically identical sentences are likely represented differently. While representing sentences in the continuous latent space ensures expressiveness, it introduces the risk of capturing of irrelevant features which hinders the learning of a common representation. In this work, we discretize the encoder output latent space of multilingual models by assigning encoder states to entries in a codebook, which in effect represents source sentences in a new artificial language. This discretization process not only offers a new way to interpret the otherwise black-box model representations, but, more importantly, gives potential for increasing robustness in unseen testing conditions. We validate our approach on large-scale experiments with realistic data volumes and domains. When tested in zero-shot conditions, our approach is competitive with two strong alternatives from the literature. We also use the learned artificial language to analyze model behavior, and discover that using a similar bridge language increases knowledge-sharing among the remaining languages."},{"location":"Quantization-Survey-Paper-Notes/nlp/Learning_cross-aligned_latent_embeddings_for_zero-shot_cross-modal_retrieval/","title":"Learning cross-aligned latent embeddings for zero-shot cross-modal retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Learning_cross-aligned_latent_embeddings_for_zero-shot_cross-modal_retrieval/#summary","title":"Summary","text":"<p> <p>The paper proposes a new method named Learning Cross-Aligned Latent Embeddings (LCALE) for Zero-Shot Cross-Modal Retrieval (ZS-CMR) that addresses the problem of retrieving data across different modalities belonging to new classes. Unlike methods relying on class-embeddings as the semantic space, LCALE seeks a shared low-dimensional latent space using modality-specific variational autoencoders. The method aligns the learned multimodal and class-embedding distributions to construct latent embeddings with essential cross-modal correlation. The paper develops effective cross-reconstruction and cross-alignment criteria that preserve class-discriminative information for efficient retrieval and knowledge transfer to unseen classes. The proposed method outperforms existing methods on image-text and image-sketch retrieval datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Learning_cross-aligned_latent_embeddings_for_zero-shot_cross-modal_retrieval/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Learning_cross-aligned_latent_embeddings_for_zero-shot_cross-modal_retrieval/#content","title":"Content","text":"<p>Zero-Shot Cross-Modal Retrieval (ZS-CMR) is an emerging research hotspot that aims to retrieve data of new classes across different modality data. A handful of recently proposed methods typically borrow the idea from zero-shot learning, i.e., exploiting word embeddings of class labels (i.e., class-embeddings) as common semantic space, and using generative adversarial network (GAN) to capture the underlying multimodal data structures, as well as strengthen relations between input data and semantic space to generalize across seen and unseen classes. In this paper, we propose a novel method termed Learning Cross-Aligned Latent Embeddings (LCALE) as an alternative to these GAN based methods for ZS-CMR. Unlike using the class-embeddings as the semantic space, our method seeks for a shared low-dimensional latent space of input multimodal features and class-embeddings by modality-specific variational autoencoders. Notably, we align the distributions learned from multimodal input features and from class-embeddings to construct latent embeddings that contain the essential cross-modal correlation associated with unseen classes. Effective cross-reconstruction and cross-alignment criterions are further developed to preserve class-discriminative information in latent space, which benefits the efficiency for retrieval and enable the knowledge transfer to unseen classes. We evaluate our model using four benchmark datasets on image-text retrieval tasks and one large-scale dataset on image-sketch retrieval tasks. The experimental results show that our method establishes the new state-of-the-art performance for both tasks on all datasets."},{"location":"Quantization-Survey-Paper-Notes/nlp/Limitations_of_knowledge_distillation_for_zero-shot_transfer_learning/","title":"Limitations of knowledge distillation for zero-shot transfer learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Limitations_of_knowledge_distillation_for_zero-shot_transfer_learning/#summary","title":"Summary","text":"<p>Summary: This paper highlights the limitations of knowledge distillation for zero-shot transfer learning in multilingual pretrained transformer-based encoders such as mBERT, and evaluates the effectiveness of knowledge distillation during pretraining and fine-tuning stages. It indicates that distillation during pretraining is more effective than during fine-tuning for zero-shot transfer learning in multilingual settings. The paper also demonstrates that distillation during fine-tuning may harm zero-shot cross-lingual performance. The study concludes that distilling a larger model (BERT Large) yields the strongest distilled model that performs well both on the source language and the target languages in zero-shot settings.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Limitations_of_knowledge_distillation_for_zero-shot_transfer_learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Limitations_of_knowledge_distillation_for_zero-shot_transfer_learning/#content","title":"Content","text":"<p> This paper discusses the limitations of knowledge distillation for zero-shot transfer learning in multilingual pretrained transformer-based encoders such as mBERT. The paper evaluates the effectiveness of knowledge distillation during pretraining and fine-tuning stages and shows that in multilingual settings, distillation during pretraining is more effective than distillation during fine-tuning for zero-shot transfer learning. The paper also demonstrates that in some cases, distillation during fine-tuning may harm zero-shot cross-lingual performance. Finally, the paper concludes that distilling a larger model (BERT Large) results in the strongest distilled model that performs best both on the source language as well as target languages in zero-shot settings."},{"location":"Quantization-Survey-Paper-Notes/nlp/Low-bit_quantization_of_recurrent_neural_network_language_models_using_alternating_direction_methods_of_multipliers/","title":"Low-bit quantization of recurrent neural network language models using alternating direction methods of multipliers","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Low-bit_quantization_of_recurrent_neural_network_language_models_using_alternating_direction_methods_of_multipliers/#summary","title":"Summary","text":"<p> The paper proposes a method for training quantized RNNLMs using ADMM for low-bit compression. The method adjusts the balance between compression and model performance using tied low-bit quantization tables. The experiments show that the proposed ADMM quantization achieved up to 31 times model compression with faster convergence compared to baseline binarized RNNLM quantization."},{"location":"Quantization-Survey-Paper-Notes/nlp/Low-bit_quantization_of_recurrent_neural_network_language_models_using_alternating_direction_methods_of_multipliers/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Low-bit_quantization_of_recurrent_neural_network_language_models_using_alternating_direction_methods_of_multipliers/#content","title":"Content","text":"<p>The paper presents a novel method for training quantized Recurrent neural network language models (RNNLMs) using alternating direction methods of multipliers (ADMM) to achieve extremely low-bit compression. The proposed method can flexibly adjust the trade-off between compression rate and model performance using tied low-bit quantization tables. The experiments on Penn Treebank (PTB), and Switchboard (SWBD) suggest the proposed ADMM quantization achieved a model size compression factor of up to 31 times over the full precision baseline RNNLMs, with faster convergence of 5 times in model training over the baseline binarized RNNLM quantization."},{"location":"Quantization-Survey-Paper-Notes/nlp/Massive_Language_Models_Can_Be_Accurately_Pruned_in_One-Shot/","title":"Massive Language Models Can Be Accurately Pruned in One-Shot","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Massive_Language_Models_Can_Be_Accurately_Pruned_in_One-Shot/#summary","title":"Summary","text":"<p>Summary: Large-scale generative pre-trained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot without any retraining, and minimal loss of accuracy. A new pruning method called SparseGPT is designed to efficiently and accurately prune massive GPT-family models.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Massive_Language_Models_Can_Be_Accurately_Pruned_in_One-Shot/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Massive_Language_Models_Can_Be_Accurately_Pruned_in_One-Shot/#content","title":"Content","text":"<p> We show for the \ufb01rst time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining , at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. The code is available at: https://github.com/IST-DASLab/sparsegpt."},{"location":"Quantization-Survey-Paper-Notes/nlp/Meta-KD%3A_A_meta_knowledge_distillation_framework_for_language_model_compression_across_domains/","title":"Meta-KD: A meta knowledge distillation framework for language model compression across domains","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Meta-KD%3A_A_meta_knowledge_distillation_framework_for_language_model_compression_across_domains/#summary","title":"Summary","text":"<p>This paper proposes a Meta-Knowledge Distillation (Meta-KD) framework for compressing pre-trained language models (PLMs) with acceptable performance loss. The Meta-KD framework uses a meta-teacher model with transferable knowledge across domains to distill cross-domain knowledge to students. The experiments demonstrate the effectiveness and superiority of Meta-KD on public multi-domain NLP tasks even when the training data is scarce."},{"location":"Quantization-Survey-Paper-Notes/nlp/Meta-KD%3A_A_meta_knowledge_distillation_framework_for_language_model_compression_across_domains/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Meta-KD%3A_A_meta_knowledge_distillation_framework_for_language_model_compression_across_domains/#content","title":"Content","text":"<p>Pre-trained language models have been applied to various NLP tasks, but their large sizes and long inference times limit their deployment in real-time applications. Knowledge distillation (KD) is one of the promising ways to compress PLMs with acceptable performance loss; however, most KD approaches focus on single-domain KD without considering cross-domain knowledge. In this paper, we propose a Meta-Knowledge Distillation (Meta-KD) framework that builds a meta-teacher model with transferable knowledge across domains to distill such knowledge to students. Experiments show the effectiveness and superiority of Meta-KD on public multi-domain NLP tasks even when the training data is scarce."},{"location":"Quantization-Survey-Paper-Notes/nlp/Metaicl%3A_Learning_to_learn_in_context/","title":"Metaicl: Learning to learn in context","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Metaicl%3A_Learning_to_learn_in_context/#summary","title":"Summary","text":"<p>Summary: The paper presents a Meta-training framework for few-shot learning called MetaICL. In this approach, a pretrained language model is fine-tuned to perform in-context learning on a large set of training tasks which enable the model to more effectively learn a new task at test time. The experiment shows that MetaICL outperforms several baselines for NLP tasks including in-context learning without meta-training and multi-task learning followed by zero-shot transfer.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Metaicl%3A_Learning_to_learn_in_context/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Metaicl%3A_Learning_to_learn_in_context/#content","title":"Content","text":"<p> We introduce MetaICL (Meta-training for In-Context Learning), a new meta-training framework for few-shot learning where a pretrained language model is tuned to do in-context learning on a large set of training tasks. This meta-training enables the model to more effectively learn a new task in context at test time, by simply conditioning on a few training examples with no parameter updates or task-specific templates. We experiment on a large, diverse collection of tasks consisting of 142 NLP datasets including classification, question answering, natural language inference, paraphrase detection, and more, across seven different meta-training/target splits. MetaICL outperforms a range of baselines including in-context learning without meta-training and multi-task learning followed by zero-shot transfer."},{"location":"Quantization-Survey-Paper-Notes/nlp/Mixed_Precision_Low-Bit_Quantization_of_Neural_Network_Language_Models_for_Speech_Recognition/","title":"Mixed Precision Low-Bit Quantization of Neural Network Language Models for Speech Recognition","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Mixed_Precision_Low-Bit_Quantization_of_Neural_Network_Language_Models_for_Speech_Recognition/#summary","title":"Summary","text":"<p>Summary: This paper proposes novel mixed precision neural network language model quantization methods that automatically learn optimal local precision choices for LSTM-RNN and Transformer based LMs, providing a solution to reduce complex and expensive models' size. Current quantization methods based on uniform precision fail to account for performance sensitivity at different parts of LMs to quantization errors.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Mixed_Precision_Low-Bit_Quantization_of_Neural_Network_Language_Models_for_Speech_Recognition/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Mixed_Precision_Low-Bit_Quantization_of_Neural_Network_Language_Models_for_Speech_Recognition/#content","title":"Content","text":"<p>Abstract: State-of-the-art language models (LMs) represented by long-short term memory recurrent neural networks (LSTM-RNNs) and Transformers are becoming increasingly complex and expensive for practical applications. Low-bit neural network quantization provides a powerful solution to dramatically reduce their model size. Current quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different parts of LMs to quantization errors. To this end, novel mixed precision neural network LM quantization methods are proposed in this paper. The optimal local precision choices for LSTM-RNN and Transformer based neural LMs are automatically learned using three techniques.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Model_selection_for_generalized_zero-shot_learning/","title":"Model selection for generalized zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Model_selection_for_generalized_zero-shot_learning/#summary","title":"Summary","text":"<p> This paper proposes a model selection mechanism for generalized zero-shot learning, which utilizes auxiliary datapoints generated by a Generative Adversarial Network (GAN) to alleviate the challenge of unbalanced data distribution. The proposed mechanism trains a classifier to distinguish between the test datapoints from seen and unseen classes, using both the original and auxiliary datapoints. The experiments on four datasets show that the proposed model achieves state-of-the-art results. The paper includes keywords such as model selection, generalized zero-shot learning, and Generative Adversarial Network."},{"location":"Quantization-Survey-Paper-Notes/nlp/Model_selection_for_generalized_zero-shot_learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Model_selection_for_generalized_zero-shot_learning/#content","title":"Content","text":"<p>In the problem of generalized zero-shot learning, the datapoints from unknown classes are not available during training. The main challenge for generalized zero-shot learning is the unbalanced data distribution which makes it hard for the classifier to distinguish if a given testing sample comes from a seen or unseen class. However, using Generative Adversarial Network (GAN) to generate auxiliary datapoints by the semantic embeddings of unseen classes alleviates the above problem. Current approaches combine the auxiliary datapoints and original training data to train the generalized zero-shot learning model and obtain state-of-the-art results. Inspired by such models, we propose to feed the generated data via a model selection mechanism. Specifically, we leverage two sources of datapoints (observed and auxiliary) to train some classifier to recognize which test datapoints come from seen and which from unseen classes. This way, generalized zero-shot learning can be divided into two disjoint classification tasks, thus reducing the negative influence of the unbalanced data distribution. Our evaluations on four publicly available datasets for generalized zero-shot learning show that our model obtains state-of-the-art results. Keywords: model selection, generalized zero-shot learning, generative adversarial network"},{"location":"Quantization-Survey-Paper-Notes/nlp/Mr._BiQ%3A_Post-Training_Non-Uniform_Quantization_based_on_Minimizing_the_Reconstruction_Error/","title":"Mr. BiQ: Post-Training Non-Uniform Quantization based on Minimizing the Reconstruction Error","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Mr._BiQ%3A_Post-Training_Non-Uniform_Quantization_based_on_Minimizing_the_Reconstruction_Error/#summary","title":"Summary","text":"<p>Summary: The paper proposes a non-uniform quantization method called Mr.BiQ which can be used for low bit-width quantization on Transformer models. The method leverages multi-level binarization for weights and allows activations to be represented in various data formats. Mr.BiQ recognizes the quantization parameters as directly and jointly learnable parameters during optimization which shows significant improvements in accuracy on various models.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Mr._BiQ%3A_Post-Training_Non-Uniform_Quantization_based_on_Minimizing_the_Reconstruction_Error/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Mr._BiQ%3A_Post-Training_Non-Uniform_Quantization_based_on_Minimizing_the_Reconstruction_Error/#content","title":"Content","text":"<p> Post-training quantization is a technique used to compress neural networks. However, this method has only been discussed in the context of uniform quantization on convolutional neural networks. In this paper, the authors propose a non-uniform quantization method called Mr.BiQ, which allows low bit-width quantization on Transformer models. The authors leverage multi-level binarization for weights, while allowing activations to be represented as various data formats (e.g., INT8, bfloat16, binary-coding, and FP32). Unlike conventional methods, Mr.BiQ recognizes the quantization parameters as directly and jointly learnable parameters during optimization. The proposed quantization scheme is tested on various models, including convolutional neural networks and Transformer models, and shows significant improvement in accuracy."},{"location":"Quantization-Survey-Paper-Notes/nlp/Multi-task_retrieval_for_knowledge-intensive_tasks/","title":"Multi-task retrieval for knowledge-intensive tasks","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Multi-task_retrieval_for_knowledge-intensive_tasks/#summary","title":"Summary","text":"<p> The paper proposes a multi-task trained model for neural retrieval, aiming to solve the problem of performance degradation when applied to out-of-domain data. The method outperforms previous ones in the few-shot setting and rivals specialized neural retrievers even with abundant in-domain training data. The retriever improves existing models for downstream tasks and performs closely to the state of the art on multiple benchmarks."},{"location":"Quantization-Survey-Paper-Notes/nlp/Multi-task_retrieval_for_knowledge-intensive_tasks/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Multi-task_retrieval_for_knowledge-intensive_tasks/#content","title":"Content","text":"<p>Retrieving relevant contexts from a large corpus is a crucial step for tasks such as open-domain question answering and fact checking. Although neural retrieval outperforms traditional methods like tf-idf and BM25, its performance degrades considerably when applied to out-of-domain data. Driven by the question of whether a neural retrieval model can be universal and perform robustly on a wide variety of problems, we propose a multi-task trained model. Our approach not only outperforms previous methods in the few-shot setting, but also rivals specialized neural retrievers, even when in-domain training data is abundant. With the help of our retriever, we improve existing models for downstream tasks and closely match or improve the state of the art on multiple benchmarks."},{"location":"Quantization-Survey-Paper-Notes/nlp/Multilingual_mix%3A_Example_interpolation_improves_multilingual_neural_machine_translation/","title":"Multilingual mix: Example interpolation improves multilingual neural machine translation","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Multilingual_mix%3A_Example_interpolation_improves_multilingual_neural_machine_translation/#summary","title":"Summary","text":"<p>Summary: The paper introduces a multilingual crossover encoder-decoder to fuse language pairs at an instance level. Their approach interpolates instances from different language pairs into joint \u2018crossover examples\u2019 to encourage sharing input and output spaces across languages. They propose techniques to improve example interpolation across dissimilar languages under data imbalance. The approach significantly improves quality on English-to-Many, Many-to-English, and zero-shot translation tasks. The approach improves model generalization to out-of-distribution multilingual examples, and they analyze advantages at the representation level.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Multilingual_mix%3A_Example_interpolation_improves_multilingual_neural_machine_translation/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Multilingual_mix%3A_Example_interpolation_improves_multilingual_neural_machine_translation/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Muse%3A_Text-To-Image_Generation_via_Masked_Generative_Transformers/","title":"Muse: Text-To-Image Generation via Masked Generative Transformers","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Muse%3A_Text-To-Image_Generation_via_Masked_Generative_Transformers/#summary","title":"Summary","text":"<p>Summary: The paper presents Muse, a text-to-image Transformer model that uses a masked modeling task in discrete token space to predict randomly masked image tokens. The model is more efficient than diffusion or autoregressive models and achieves state-of-the-art image generation performance, with an FID score of 6.06 on CC3M and an FID of 7.88 on zero-shot COCO evaluation. Additionally, Muse enables image editing applications such as inpainting, outpainting, and mask-free editing without the need for fine-tuning or inverting the model.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Muse%3A_Text-To-Image_Generation_via_Masked_Generative_Transformers/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Muse%3A_Text-To-Image_Generation_via_Masked_Generative_Transformers/#content","title":"Content","text":"<p> We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing."},{"location":"Quantization-Survey-Paper-Notes/nlp/Non-volatile_memory_array_based_quantization-and_noise-resilient_LSTM_neural_networks/","title":"Non-volatile memory array based quantization-and noise-resilient LSTM neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Non-volatile_memory_array_based_quantization-and_noise-resilient_LSTM_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes the application of quantization-aware training algorithm to LSTM models in cloud and edge computing and shows that 4-bit NVM weights and 4-bit ADC/DACs are enough to produce equivalent LSTM network performance as floating-point baseline. The proposed LSTM accelerator for inference is much more efficient (2.4x computing efficiency and 40x area efficiency) than traditional digital approaches like GPU, FPGA, and ASIC.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Non-volatile_memory_array_based_quantization-and_noise-resilient_LSTM_neural_networks/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Non-volatile_memory_array_based_quantization-and_noise-resilient_LSTM_neural_networks/#content","title":"Content","text":"<p>Abstract: In cloud and edge computing models, it is important that compute devices at the edge be as power efficient as possible. Long short-term memory (LSTM) neural networks have been widely used for natural language processing, time series prediction, and many other sequential data tasks. In this paper, we focus on the application of quantization-aware training algorithm to LSTM models, and the benefits these models bring in terms of resilience against both quantization error and analog device noise. We have shown that only 4-bit NVM weights and 4-bit ADC/DACs are needed to produce equivalent LSTM network performance as floating-point baseline. Reasonable levels of ADC quantization noise and weight noise can be naturally tolerated within our NVM-based quantized LSTM network. Benchmark analysis of our proposed LSTM accelerator for inference has shown at least 2.4\u00d7 better computing efficiency and 40\u00d7 higher area efficiency than traditional digital approaches (GPU, FPGA, and ASIC).</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Out-of-domain_semantics_to_the_rescue%21_zero-shot_hybrid_retrieval_models/","title":"Out-of-domain semantics to the rescue! zero-shot hybrid retrieval models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Out-of-domain_semantics_to_the_rescue%21_zero-shot_hybrid_retrieval_models/#summary","title":"Summary","text":"<p> The paper studies the generalization of deep retrieval models over different tasks and domains. While they perform better than lexical models in many retrieval tasks, their performance is significantly weakened when the target domain is very different from the source domain they were trained on. Thus, the paper proposes a framework that combines lexical and deep retrieval models to achieve better and more robust performance. The hybrid model improved by an average of 20.4% relative gain over deep retrieval model and 9.54% over the lexical model in three out-of-domain datasets."},{"location":"Quantization-Survey-Paper-Notes/nlp/Out-of-domain_semantics_to_the_rescue%21_zero-shot_hybrid_retrieval_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Out-of-domain_semantics_to_the_rescue%21_zero-shot_hybrid_retrieval_models/#content","title":"Content","text":"<p> The pre-trained language model ( eg,BERT) based deep retrieval models achieved superior performance over lexical retrieval models (eg,BM25) in many passage retrieval tasks. However, limited work has been done to generalize a deep retrieval model to other tasks and domains. In this work, we carefully select five datasets, including two in-domain datasets and three out-of-domain datasets with different levels of domain shift, and study the generalization of a deep model in a zero-shot setting. Our findings show that the performance of a deep retrieval model is significantly deteriorated when the target domain is very different from the source domain that the model was trained on. On the contrary, lexical models are more robust across domains. We thus propose a simple yet effective framework to integrate lexical and deep retrieval models. Our experiments demonstrate that these two models are complementary, even when the deep model is weaker in the out-of-domain setting. The hybrid model obtains an average of 20.4% relative gain over the deep retrieval model, and an average of 9.54% over the lexical model in three out-of-domain datasets."},{"location":"Quantization-Survey-Paper-Notes/nlp/Outlier_suppression%3A_Pushing_the_limit_of_low-bit_transformer_language_models/","title":"Outlier suppression: Pushing the limit of low-bit transformer language models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Outlier_suppression%3A_Pushing_the_limit_of_low-bit_transformer_language_models/#summary","title":"Summary","text":"<p>Summary: This paper discusses the issue of memory and computation costs in large natural language processing models and how quantization can help solve this problem. The paper proposes an outlier suppression framework that includes two components: Gamma Migration and Token-Wise Clipping, which suppress the outliers effectively, push the 6-bit post-training BERT quantization to the full-precision level, and contribute to a more quantization-friendly model without adding any extra burden.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Outlier_suppression%3A_Pushing_the_limit_of_low-bit_transformer_language_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Outlier_suppression%3A_Pushing_the_limit_of_low-bit_transformer_language_models/#content","title":"Content","text":"<p>Transformer architecture has become a fundamental element of natural language processing models. With the growth of large NLP models, the memory and computation costs have become a major hindrance to their efficient deployment on resource-limited devices. Transformer quantization has attracted widespread research interest to solve this issue. Recent studies show that structured outliers are the critical bottleneck for quantization performance. However, the proposed methods increase computation overhead and leave the outliers in place. This paper delves into the inherent inducement and importance of the outliers and proposes an outlier suppression framework, including two components: Gamma Migration and Token-Wise Clipping. These components contribute to a more quantization-friendly model without any extra burden, suppress the outliers effectively, and push the 6-bit post-training BERT quantization to the full-precision level."},{"location":"Quantization-Survey-Paper-Notes/nlp/Post-training_weighted_quantization_of_neural_networks_for_language_models/","title":"Post-training weighted quantization of neural networks for language models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Post-training_weighted_quantization_of_neural_networks_for_language_models/#summary","title":"Summary","text":"<p>Summary: The paper proposes a non-uniform quantization scheme, namely binary-coding-based quantization for language models with significant memory. The scheme achieves high compression and effective computation without significant accuracy degradation. The paper develops quantization optimization approaches that consider the importance of each parameter. The methods demonstrate that post-training quantization and weight magnitude representation can significantly improve accuracy compared to other schemes. The proposed quantization is applied to several language models, including AWD-LSTM, Transformer, BERT, and DistilBERT, achieving 2-4 bits per weight with reasonable accuracy degradation.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Post-training_weighted_quantization_of_neural_networks_for_language_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Post-training_weighted_quantization_of_neural_networks_for_language_models/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Prune_once_for_all%3A_Sparse_pre-trained_language_models/","title":"Prune once for all: Sparse pre-trained language models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Prune_once_for_all%3A_Sparse_pre-trained_language_models/#summary","title":"Summary","text":"<p>This paper describes a method for sparse pre-training Transformer-based language models. The proposed method trains the sparse models using weight pruning and model distillation. The authors demonstrate that these compressed models transfer their knowledge to various natural language tasks as well as maintaining their sparsity pattern. Moreover, the paper shows how to further compress the sparse models' weights using quantization-aware training to achieve the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT models."},{"location":"Quantization-Survey-Paper-Notes/nlp/Prune_once_for_all%3A_Sparse_pre-trained_language_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Prune_once_for_all%3A_Sparse_pre-trained_language_models/#content","title":"Content","text":"<p>Transformer-based language models, such as BERT and RoBERTa, have become the standard approach for natural language processing. However, their large size requires high computational and memory resources, making their deployment challenging. Model compression methods, such as weight pruning, have been proposed to address this issue. In this paper, we present a method for training sparse pre-trained Transformer-based language models using weight pruning and model distillation. Our method demonstrates that these sparse pre-trained models can be used for a wide range of downstream natural language tasks while maintaining their sparsity pattern. The compressed sparse pre-trained models we trained transfer their knowledge to five different natural language tasks with minimal accuracy loss. Furthermore, we show how to further compress the sparse models' weights to 8-bit precision using quantization-aware training, achieving the best compression-to-accuracy ratio for BERT-Base, BERT-Large, and DistilBERT."},{"location":"Quantization-Survey-Paper-Notes/nlp/Q-bert%3A_Hessian_based_ultra_low_precision_quantization_of_bert/","title":"Q-bert: Hessian based ultra low precision quantization of bert","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Q-bert%3A_Hessian_based_ultra_low_precision_quantization_of_bert/#summary","title":"Summary","text":"<p>The paper proposes a novel method for quantizing BERT models to ultra low precision using group-wise quantization and Hessian-based mix-precision method. The proposed method achieves comparable performance to baseline with at most 2.3% performance degradation, even with ultra-low precision quantization down to 2 bits, corresponding to up to 13\u00d7 compression of the model parameters, and up to 4\u00d7 compression of the embedding table as well as activations. However, the highest performance loss is observed for BERT fine-tuned on SQuAD, which is related to the fact that the current training/fine-tuning strategy of BERT does not converge for SQuAD."},{"location":"Quantization-Survey-Paper-Notes/nlp/Q-bert%3A_Hessian_based_ultra_low_precision_quantization_of_bert/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Q-bert%3A_Hessian_based_ultra_low_precision_quantization_of_bert/#content","title":"Content","text":"<p>Transformer based architectures have become de-facto models used for a range of Natural Language Processing tasks. In particular, the BERT based models achieved significant accuracy gain for GLUE tasks, CoNLL-03 and SQuAD. However, BERT based models have a prohibitive memory footprint and latency. As a result, deploying BERT based models in resource constrained environments has become a challenging task. In this work, we perform an extensive analysis of fine-tuned BERT models using second order Hessian information, and we use our results to propose a novel method for quantizing BERT models to ultra low precision. In particular, we propose a new group-wise quantization scheme, and we use Hessian-based mix-precision method to compress the model further. We extensively test our proposed method on BERT downstream tasks of SST-2, MNLI, CoNLL-03, and SQuAD. We can achieve comparable performance to baseline with at most 2.3% performance degradation, even with ultra-low precision quantization down to 2 bits, corresponding up to 13\u00d7 compression of the model parameters, and up to 4\u00d7 compression of the embedding table as well as activations. Among all tasks, we observed the highest performance loss for BERT fine-tuned on SQuAD. By probing into the Hessian based analysis as well as visualization, we show that this is related to the fact that current training/fine-tuning strategy of BERT does not converge for SQuAD."},{"location":"Quantization-Survey-Paper-Notes/nlp/Re-imagen%3A_Retrieval-augmented_text-to-image_generator/","title":"Re-imagen: Retrieval-augmented text-to-image generator","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Re-imagen%3A_Retrieval-augmented_text-to-image_generator/#summary","title":"Summary","text":"<p>The paper presents the Retrieval-Augmented Text-to-Image Generator (Re-Imagen), which employs retrieved information to generate high-quality and accurate images of uncommon entities, overcoming the limitations faced by state-of-the-art text-to-image models."},{"location":"Quantization-Survey-Paper-Notes/nlp/Re-imagen%3A_Retrieval-augmented_text-to-image_generator/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Re-imagen%3A_Retrieval-augmented_text-to-image_generator/#content","title":"Content","text":"<p>Research on text-to-image generation has witnessed significant progress in generating diverse and photo-realistic images, driven by diffusion and auto-regressive models trained on large-scale image-text data. Though state-of-the-art models can generate high-quality images of common entities, they often have difficulty generating images of uncommon entities, such as \u2018Chortai (dog)\u2019 or \u2018Picarones (food)\u2019. To tackle this issue, we present the Retrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities."},{"location":"Quantization-Survey-Paper-Notes/nlp/Regionclip%3A_Region-based_language-image_pretraining/","title":"Regionclip: Region-based language-image pretraining","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Regionclip%3A_Region-based_language-image_pretraining/#summary","title":"Summary","text":"<p>The paper discusses the limitations of contrastive language-image pretraining (CLIP) on fine-grained object detection due to domain shift, and proposes a solution called RegionCLIP which extends CLIP to learn region-level visual representations, enabling proper alignment between image regions and textual concepts."},{"location":"Quantization-Survey-Paper-Notes/nlp/Regionclip%3A_Region-based_language-image_pretraining/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Regionclip%3A_Region-based_language-image_pretraining/#content","title":"Content","text":"<p>Contrastive language-image pretraining (CLIP) using image-text pairs has achieved impressive results on image classification in both zero-shot and transfer learning set- tings. However, we show that directly applying such mod- els to recognize image regions for object detection leads to unsatisfactory performance due to a major domain shift: CLIP was trained to match an image as a whole to a text de- scription, without capturing the fine-grained alignment be- tween image regions and text spans. To mitigate this issue, we propose a new method called RegionCLIP that signifi- cantly extends CLIP to learn region-level visual representa- tions, thus enabling fine-grained alignment between image regions and textual concepts."},{"location":"Quantization-Survey-Paper-Notes/nlp/Robust_retrieval_augmented_generation_for_zero-shot_slot_filling/","title":"Robust retrieval augmented generation for zero-shot slot filling","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Robust_retrieval_augmented_generation_for_zero-shot_slot_filling/#summary","title":"Summary","text":"<p>Summary: The paper presents a novel approach to zero-shot slot filling that uses dense passage retrieval and robust training procedures for retrieval augmented generation models. The system demonstrates promising results on both T-REx and zsRE slot filling datasets and exhibits domain adaptation capability on a new variant of the TACRED dataset.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Robust_retrieval_augmented_generation_for_zero-shot_slot_filling/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Robust_retrieval_augmented_generation_for_zero-shot_slot_filling/#content","title":"Content","text":"<p>Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning."},{"location":"Quantization-Survey-Paper-Notes/nlp/SPT%3A_Semi-Parametric_Prompt_Tuning_for_Multitask_Prompted_Learning/","title":"SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/SPT%3A_Semi-Parametric_Prompt_Tuning_for_Multitask_Prompted_Learning/#summary","title":"Summary","text":"<p>Summary: The paper proposes SPT, a semi-parametric prompt tuning method for multitask prompted learning, which uses a memory bank for memory prompts based on discrete prompts. The effectiveness of SPT is demonstrated through experiments in fine-tuning a full language model with SPT on 31 different tasks and evaluating zero-shot generalization on 9 heldout datasets and pretraining SPT on the GLUE datasets and evaluating fine-tuning on the SuperGLUE datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/SPT%3A_Semi-Parametric_Prompt_Tuning_for_Multitask_Prompted_Learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/SPT%3A_Semi-Parametric_Prompt_Tuning_for_Multitask_Prompted_Learning/#content","title":"Content","text":"<p> Pre-trained large language models can interpolate human-written prompts in a natural way. Multitask prompted learning can enhance the potential for more effective downstream fine-tuning. Prompt tuning has been proposed for efficient multitask-inference in the same batch but existing methods may lack generalization. This paper proposes SPT, a semi-parametric prompt tuning method for multitask prompted learning, which uses a memory bank for memory prompts based on discrete prompts. The effectiveness of SPT is demonstrated through experiments in fine-tuning a full language model with SPT on 31 different tasks and evaluating zero-shot generalization on 9 heldout datasets and pretraining SPT on the GLUE datasets and evaluating fine-tuning on the SuperGLUE datasets."},{"location":"Quantization-Survey-Paper-Notes/nlp/Smoothquant%3A_Accurate_and_efficient_post-training_quantization_for_large_language_models/","title":"Smoothquant: Accurate and efficient post-training quantization for large language models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Smoothquant%3A_Accurate_and_efficient_post-training_quantization_for_large_language_models/#summary","title":"Summary","text":"<p>The paper proposes SmoothQuant, a post-training quantization (PTQ) solution for large language models (LLMs) that enables 8-bit weight, 8-bit activation (W8A8) quantization. The proposed solution is training-free, accuracy-preserving, and applicable to LLMs such as OPT-175B, BLOOM-176B, GLM-130B, and MT-NLG 530B. SmoothQuant achieves better hardware efficiency than existing techniques, resulting in up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. The paper provides a turnkey solution that reduces hardware costs and democratizes LLMs."},{"location":"Quantization-Survey-Paper-Notes/nlp/Smoothquant%3A_Accurate_and_efficient_post-training_quantization_for_large_language_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Smoothquant%3A_Accurate_and_efficient_post-training_quantization_for_large_language_models/#content","title":"Content","text":"<p>Large language models (LLMs) show excellent performance but are compute- and memory-intensive. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT-175B, BLOOM-176B, GLM-130B, and MT-NLG 530B. SmoothQuant has better hardware efficiency than existing techniques. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. Our work offers a turnkey solution that reduces hardware costs and democratizes LLMs."},{"location":"Quantization-Survey-Paper-Notes/nlp/Span_pointer_networks_for_non-autoregressive_task-oriented_semantic_parsing/","title":"Span pointer networks for non-autoregressive task-oriented semantic parsing","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Span_pointer_networks_for_non-autoregressive_task-oriented_semantic_parsing/#summary","title":"Summary","text":"<p>Summary: The paper proposes a non-autoregressive parser, called span pointer networks that can map utterances to semantic frames in three steps with less latency and memory usage compared to prior non-autoregressive parsers. The proposed parser achieves 87 EM on TOPv2 after evaluating on several task-oriented semantic parsing datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Span_pointer_networks_for_non-autoregressive_task-oriented_semantic_parsing/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Span_pointer_networks_for_non-autoregressive_task-oriented_semantic_parsing/#content","title":"Content","text":"<p>An effective recipe for building seq2seq, non-autoregressive, task-oriented parsers to map utterances to semantic frames proceeds in three steps: encoding an utterance x, predicting a frame\u2019s length jyj, and decoding ajyj-sized frame with utterance and ontology tokens. We propose span pointer networks, non-autoregressive parsers which shift the decoding task from text generation to span prediction. We evaluate our approach on several task-oriented semantic parsing datasets, achieving 87 EM on TOPv2. We observe a 70% reduction in latency and 83% in memory at beam size 5 compared to prior non-autoregressive parsers."},{"location":"Quantization-Survey-Paper-Notes/nlp/Targeted_adversarial_training_for_natural_language_understanding/","title":"Targeted adversarial training for natural language understanding","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Targeted_adversarial_training_for_natural_language_understanding/#summary","title":"Summary","text":"<p>The paper introduces a Targeted Adversarial Training algorithm to enhance adversarial training for natural language understanding. This technique prioritizes adversarial training steps based on the model's current errors, resulting in improved accuracy and state-of-the-art zero-shot results on XNLI."},{"location":"Quantization-Survey-Paper-Notes/nlp/Targeted_adversarial_training_for_natural_language_understanding/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Targeted_adversarial_training_for_natural_language_understanding/#content","title":"Content","text":"<p>We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI."},{"location":"Quantization-Survey-Paper-Notes/nlp/The_case_for_4-bit_precision%3A_k-bit_Inference_Scaling_Laws/","title":"The case for 4-bit precision: k-bit Inference Scaling Laws","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/The_case_for_4-bit_precision%3A_k-bit_Inference_Scaling_Laws/#summary","title":"Summary","text":"<p> This paper studies the trade-off between reduced memory footprint and inference latency against accuracy for Large Language Models (LLMs) through quantization methods. The study found that 4-bit precision is almost always optimal for total model bits and zero-shot accuracy across all model scales and model families tested, indicating that reducing model bits through quantization can lower latency while maintaining optimal accuracy."},{"location":"Quantization-Survey-Paper-Notes/nlp/The_case_for_4-bit_precision%3A_k-bit_Inference_Scaling_Laws/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/The_case_for_4-bit_precision%3A_k-bit_Inference_Scaling_Laws/#content","title":"Content","text":"<p> Quantization methods are widely used to reduce the number of bits required to represent each parameter in a model. In this paper, the trade-off between reduced memory footprint and inference latency against accuracy is studied for Large Language Models (LLMs) using inference scaling laws of zero-shot performance. The study found that 4-bit precision is almost universally optimal for total model bits and zero-shot accuracy across all model scales and model families tested. The findings suggest that reducing the model bits through quantization can potentially reduce the latency of the model while maintaining optimal accuracy."},{"location":"Quantization-Survey-Paper-Notes/nlp/The_optimal_BERT_surgeon%3A_Scalable_and_accurate_second-order_pruning_for_large_language_models/","title":"The optimal BERT surgeon: Scalable and accurate second-order pruning for large language models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/The_optimal_BERT_surgeon%3A_Scalable_and_accurate_second-order_pruning_for_large_language_models/#summary","title":"Summary","text":"<p>The paper introduces optimal BERT Surgeon (oBERT), an efficient and accurate pruning method, for sparse BERT models. This method allows for pruning blocks of weights, and is effective in both pre-training and fine-tuning stages of language tasks. The paper also investigates compounding compression approaches for obtaining highly accurate models for deployment on edge devices. The proposed method improves the state-of-the-art sparse BERT models in terms of model size, inference speed and task accuracy. The code is available on Github."},{"location":"Quantization-Survey-Paper-Notes/nlp/The_optimal_BERT_surgeon%3A_Scalable_and_accurate_second-order_pruning_for_large_language_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/The_optimal_BERT_surgeon%3A_Scalable_and_accurate_second-order_pruning_for_large_language_models/#content","title":"Content","text":"<p> In this paper, we introduce the Optimal BERT Surgeon (oBERT), an efficient and accurate pruning method based on approximate second-order information, for the sparsification of BERT models. We extend existing work on second-order pruning by allowing for pruning blocks of weights, and show that oBERT yields state-of-the-art results for compression in both stages of language tasks: pre-training and fine-tuning. Furthermore, we investigate compounding compression approaches to obtain highly compressed but accurate models for deployment on edge devices. We obtain significant improvements over the current state-of-the-art sparse BERT models with respect to all metrics: model size, inference speed and task accuracy. Our code, fully integrated with Transformers and SparseML, is freely available at https://github.com/neuralmagic/sparseml/tree/main/research/optimal_BERT_surgeon_oBERT."},{"location":"Quantization-Survey-Paper-Notes/nlp/Towards_efficient_post-training_quantization_of_pre-trained_language_models/","title":"Towards efficient post-training quantization of pre-trained language models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Towards_efficient_post-training_quantization_of_pre-trained_language_models/#summary","title":"Summary","text":"<p> This paper proposes a module-wise quantization error minimization (MREM) method for post-training quantization (PTQ) of pre-trained language models (PLMs), to mitigate the slow training, large memory overhead and data security issues. The proposed method partitions the PLM into multiple modules and minimizes the reconstruction error of each module. Additionally, model parallel training technique is used to achieve faster training speeds. Results show that MREM performs closely to quantization-aware training (QAT) with significant improvements in training time, memory overhead, and data consumption."},{"location":"Quantization-Survey-Paper-Notes/nlp/Towards_efficient_post-training_quantization_of_pre-trained_language_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Towards_efficient_post-training_quantization_of_pre-trained_language_models/#content","title":"Content","text":"<p>Network quantization has gained increasing attention with the rapid growth of large pre-trained language models (PLMs). However, most existing quantization methods for PLMs follow quantization-aware training (QAT) that requires end-to-end training with full access to the entire dataset. Therefore, they suffer from slow training, large memory overhead, and data security issues. In this paper, we study post-training quantization (PTQ) of PLMs, and propose module-wise quantization error minimization (MREM), an efficient solution to mitigate these issues. By partitioning the PLM into multiple modules, we minimize the reconstruction error incurred by quantization for each module. In addition, we design a new model parallel training strategy such that each module can be trained locally on separate computing devices without waiting for preceding modules, which brings nearly the theoretical training speed-up (e.g. 4 on 4 GPUs). Experiments on GLUE and SQuAD benchmarks show that our proposed PTQ solution not only performs close to QAT, but also enjoys significant reductions in training time, memory overhead, and data consumption."},{"location":"Quantization-Survey-Paper-Notes/nlp/Towards_zero-shot_knowledge_distillation_for_natural_language_processing/","title":"Towards zero-shot knowledge distillation for natural language processing","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Towards_zero-shot_knowledge_distillation_for_natural_language_processing/#summary","title":"Summary","text":"<p>The paper presents the first work on Zero-Shot Knowledge Distillation for NLP, which uses out of domain data and adversarial training to transfer knowledge from a teacher model to a student model without access to the teacher's training data. The proposed method achieved between 75% and 92% of the teacher's classification score while compressing the model 30 times on six tasks from the GLUE benchmark."},{"location":"Quantization-Survey-Paper-Notes/nlp/Towards_zero-shot_knowledge_distillation_for_natural_language_processing/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Towards_zero-shot_knowledge_distillation_for_natural_language_processing/#content","title":"Content","text":"<p>Knowledge Distillation (KD) is a common knowledge transfer algorithm used for model compression across a variety of deep learning based natural language processing (NLP) solutions. In its regular manifestations, KD requires access to the teacher\u2019s training data for knowledge transfer to the student network. However, privacy concerns, data regulations and proprietary reasons may prevent access to such data. We present, to the best of our knowledge, the first work on Zero-Shot Knowledge Distillation for NLP, where the student learns from the much larger teacher without any task specific data. Our solution combines out of domain data and adversarial training to learn the teacher\u2019s output distribution. We investigate six tasks from the GLUE benchmark and demonstrate that we can achieve between 75% and 92% of the teacher\u2019s classification score (accuracy or F1) while compressing the model 30 times."},{"location":"Quantization-Survey-Paper-Notes/nlp/Training_with_quantization_noise_for_extreme_model_compression/","title":"Training with quantization noise for extreme model compression","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Training_with_quantization_noise_for_extreme_model_compression/#summary","title":"Summary","text":"<p>Summary: The paper proposes an extension to Quantization Aware Training approach to produce compact models with high accuracy, allowing unbiased gradients to flow through different weights during each forward. This approach achieves a new state-of-the-art compromise between accuracy and model size in natural language processing and image classification by demonstrating it through Transformer and ConvNet architectures.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Training_with_quantization_noise_for_extreme_model_compression/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Training_with_quantization_noise_for_extreme_model_compression/#content","title":"Content","text":"<p>We tackle the problem of producing compact models with high accuracy and propose an extension to the Quantization Aware Training approach. Our approach allows for unbiased gradients to flow through different weights during each forward, enabling extreme compression rates while maintaining performance. The approach is demonstrated through state-of-the-art Transformer and ConvNet architectures, achieving a new state-of-the-art compromise between accuracy and model size in natural language processing and image classification."},{"location":"Quantization-Survey-Paper-Notes/nlp/Transzero%3A_Attribute-guided_transformer_for_zero-shot_learning/","title":"Transzero: Attribute-guided transformer for zero-shot learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Transzero%3A_Attribute-guided_transformer_for_zero-shot_learning/#summary","title":"Summary","text":"<p> The paper introduces TransZero, a Transformer network for zero-shot learning that helps recognize novel classes by transferring knowledge from seen to unseen classes. It uses a feature augmentation encoder to improve the transferability of visual features and a visual-semantic decoder to localize the image regions most relevant to each attribute. The method achieves state-of-the-art performance in three ZSL benchmarks."},{"location":"Quantization-Survey-Paper-Notes/nlp/Transzero%3A_Attribute-guided_transformer_for_zero-shot_learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Transzero%3A_Attribute-guided_transformer_for_zero-shot_learning/#content","title":"Content","text":"<p> The paper proposes TransZero, an attribute-guided Transformer network for zero-shot learning (ZSL), which aims to recognize novel classes by transferring semantic knowledge from seen to unseen classes. TransZero takes a feature augmentation encoder to alleviate cross-dataset bias and improves the transferability of visual features by reducing the entangled relative geometry relationships among region features. It employs a visual-semantic decoder to localize the image regions most relevant to each attribute, and conducts effective visual-semantic interaction in a visual-semantic embedding network. The proposed method achieves state-of-the-art performance on three ZSL benchmarks."},{"location":"Quantization-Survey-Paper-Notes/nlp/Txt2Img-MHN%3A_Remote_sensing_image_generation_from_text_using_modern_Hopfield_networks/","title":"Txt2Img-MHN: Remote sensing image generation from text using modern Hopfield networks","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Txt2Img-MHN%3A_Remote_sensing_image_generation_from_text_using_modern_Hopfield_networks/#summary","title":"Summary","text":"<p>Summary: The paper presents a new approach called Txt2Img-MHN for generating high-resolution remote sensing images from text descriptions. The approach involves hierarchical prototype learning on both text and image embeddings with modern Hopfield layers. The aim is to learn the most representative prototypes from text-image embeddings and achieve a coarse-to-fine learning strategy in generating images. The learned prototypes can represent complex semantics in text-to-image generation. Zero-shot classification was conducted on real remote sensing data to better evaluate the realism and semantic consistency of the generated images. The proposed approach demonstrated better results than existing methods in generating more realistic remote sensing images.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Txt2Img-MHN%3A_Remote_sensing_image_generation_from_text_using_modern_Hopfield_networks/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Txt2Img-MHN%3A_Remote_sensing_image_generation_from_text_using_modern_Hopfield_networks/#content","title":"Content","text":"<p>The paper proposes a novel approach called Txt2Img-MHN for generating high-resolution remote sensing images based on text descriptions. The proposed method conducts hierarchical prototype learning on both text and image embeddings with modern Hop\ufb01eld layers. Instead of directly learning concrete but highly diverse text-image joint feature representations for different semantics, Txt2Img-MHN aims to learn the most representative prototypes from text-image embeddings, achieving a coarse-to-\ufb01ne learning strategy. The learned prototypes can then be utilized to represent more complex semantics in the text-to-image generation task. The paper also conducts zero-shot classification on real remote sensing data using the classification model trained on synthesized images for better evaluation of the realism and semantic consistency of the generated images. The experiments on the benchmark remote sensing text-image dataset demonstrate that the proposed Txt2Img-MHN can generate more realistic remote sensing images than existing methods."},{"location":"Quantization-Survey-Paper-Notes/nlp/UNIMO-2%3A_end-to-end_unified_vision-language_grounded_learning/","title":"UNIMO-2: end-to-end unified vision-language grounded learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/UNIMO-2%3A_end-to-end_unified_vision-language_grounded_learning/#summary","title":"Summary","text":"<p>The paper proposes a novel framework called UNIMO-2 for Joint Learning on both aligned image-caption data and unaligned image-only and text-only corpus. A Uni\ufb01ed Transformer Model is built to learn visual representations, textual representations, and semantic alignment between images and text while grounded sharing is conducted on both images and texts via a sharing grounded space, which aligns visual and textual semantic spaces on different types of corpora. The proposed grounded learning method improves textual and visual semantic alignment, thereby enhancing the cross-modal task performance. Additionally, the model achieves impressive performance on single-modal visual and textual tasks. Code and models are available at the UNIMO project page."},{"location":"Quantization-Survey-Paper-Notes/nlp/UNIMO-2%3A_end-to-end_unified_vision-language_grounded_learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/UNIMO-2%3A_end-to-end_unified_vision-language_grounded_learning/#content","title":"Content","text":"<p>Vision-Language Pre-training (VLP) has achieved impressive performance on various cross-modal downstream tasks. However, most existing methods can only learn from aligned image-caption data and rely heavily on expensive regional features, which greatly limits their scalability and performance. In this paper, we propose an end-to-end uni\ufb01ed-modal pre-training framework, namely UNIMO-2, for joint learning on both aligned image-caption data and unaligned image-only and text-only corpus. We build a uni\ufb01ed Transformer model to jointly learn visual representations, textual representations and semantic alignment between images and texts. In particular, we propose to conduct grounded learning on both images and texts via a sharing grounded space, which helps bridge unaligned images and texts, and align the visual and textual semantic spaces on different types of corpora. The experiments show that our grounded learning method can improve textual and visual semantic alignment for improving performance on various cross-modal tasks. Moreover, bene\ufb01ting from effective joint modeling of different types of corpora, our model also achieves impressive performance on single-modal visual and textual tasks. Our code and models are public at the UNIMO project page."},{"location":"Quantization-Survey-Paper-Notes/nlp/Unbounded_cache_model_for_online_language_modeling_with_open_vocabulary/","title":"Unbounded cache model for online language modeling with open vocabulary","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Unbounded_cache_model_for_online_language_modeling_with_open_vocabulary/#summary","title":"Summary","text":"<p>Summary: The paper proposes an extension to continuous cache models that can adapt predictions to local changes in data distribution. The extension can scale to larger contexts and uses a non-parametric memory component to store all the hidden activations seen in the past. The approach significantly improves the perplexity of pre-trained language models on new distributions and can scale efficiently to much larger contexts than previously proposed local cache models.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Unbounded_cache_model_for_online_language_modeling_with_open_vocabulary/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Unbounded_cache_model_for_online_language_modeling_with_open_vocabulary/#content","title":"Content","text":"<p> In this paper, the authors propose an extension to continuous cache models used to adapt predictions to local changes in data distribution. This extension can scale to larger contexts and uses a non-parametric memory component to store all the hidden activations seen in the past. The authors conduct experiments showing that this approach significantly improves the perplexity of pre-trained language models on new distributions and can scale efficiently to much larger contexts than previously proposed local cache models."},{"location":"Quantization-Survey-Paper-Notes/nlp/Understanding_and_overcoming_the_challenges_of_efficient_transformer_quantization/","title":"Understanding and overcoming the challenges of efficient transformer quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Understanding_and_overcoming_the_challenges_of_efficient_transformer_quantization/#summary","title":"Summary","text":"<p>Summary: The paper explores quantization techniques for transformer-based models in order to reduce their high latency and memory requirements for deployment on resource-limited devices. The authors present three quantization solutions and introduce a novel per-embedding-group quantization scheme for transformers. These methods achieve state-of-the-art results for post-training quantization on the GLUE benchmark using BERT and show that ultra-low bit-width quantization for transformer weights and embeddings can lead to significant memory savings with minimal accuracy loss.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Understanding_and_overcoming_the_challenges_of_efficient_transformer_quantization/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Understanding_and_overcoming_the_challenges_of_efficient_transformer_quantization/#content","title":"Content","text":"<p>Transformer-based models have become the standard for a broad range of Natural Language Processing (NLP) tasks, but their high latency and memory footprint make deployment on resource-limited devices inefficient. In this paper, we explore quantization for transformers and present three solutions based on post-training quantization and quantization-aware training. We show that transformers face unique quantization challenges due to their high dynamic activation ranges and introduce a novel quantization scheme, per-embedding-group quantization. Our methods achieve state-of-the-art results for post-training quantization on the GLUE benchmark using BERT. Additionally, we show that ultra-low bit-width quantization for transformer weights and embeddings can result in significant memory savings with minimum accuracy loss."},{"location":"Quantization-Survey-Paper-Notes/nlp/Unitab%3A_Unifying_text_and_box_outputs_for_grounded_vision-language_modeling/","title":"Unitab: Unifying text and box outputs for grounded vision-language modeling","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Unitab%3A_Unifying_text_and_box_outputs_for_grounded_vision-language_modeling/#summary","title":"Summary","text":"<p>Summary: UniTAB is proposed, a unified approach for grounded vision-language modeling that generates text and box outputs together while indicating word-box alignments in a shared token sequence, outperforming existing solutions in grounded captioning evaluations.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Unitab%3A_Unifying_text_and_box_outputs_for_grounded_vision-language_modeling/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Unitab%3A_Unifying_text_and_box_outputs_for_grounded_vision-language_modeling/#content","title":"Content","text":"<p> token to naturally indicate word-box alignments in the sequence. On grounded captioning, UniTAB presents a simpler solution with a single output head, and significantly outperforms state of the art in both grounding and captioning evaluations.&gt;"},{"location":"Quantization-Survey-Paper-Notes/nlp/Videobert%3A_A_joint_model_for_video_and_language_representation_learning/","title":"Videobert: A joint model for video and language representation learning","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Videobert%3A_A_joint_model_for_video_and_language_representation_learning/#summary","title":"Summary","text":"<p>The paper proposes a joint visual-linguistic model called VideoBERT to learn high-level features without any explicit supervision. The model builds upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. VideoBERT shows promising results in various tasks like action classification and video captioning. It outperforms the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features."},{"location":"Quantization-Survey-Paper-Notes/nlp/Videobert%3A_A_joint_model_for_video_and_language_representation_learning/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Videobert%3A_A_joint_model_for_video_and_language_representation_learning/#content","title":"Content","text":"<p>Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features."},{"location":"Quantization-Survey-Paper-Notes/nlp/Violet%3A_End-to-end_video-language_transformers_with_masked_visual-token_modeling/","title":"Violet: End-to-end video-language transformers with masked visual-token modeling","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Violet%3A_End-to-end_video-language_transformers_with_masked_visual-token_modeling/#summary","title":"Summary","text":"<p>The paper presents VIOLET, a video-language Transformer model that utilizes a video transformer to model temporal dynamics and introduces a new pre-training task for better video modeling. VIOLET achieves state-of-the-art performance on 5 video question answering tasks and 4 text-to-video retrieval tasks."},{"location":"Quantization-Survey-Paper-Notes/nlp/Violet%3A_End-to-end_video-language_transformers_with_masked_visual-token_modeling/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Violet%3A_End-to-end_video-language_transformers_with_masked_visual-token_modeling/#content","title":"Content","text":"<p>A great challenge in video-language (VidL) modeling lies in the disconnection between fixed video representations extracted from image/video understanding models and downstream VidL data. In this work, the authors present VIOLET, a fully end-to-end VIdeO-LanguagE Transformer, which adopts a video transformer to explicitly model the temporal dynamics of video inputs. They designed a new pre-training task, Masked Visual-token Modeling (MVM), for better video modeling. VIOLET achieves new state-of-the-art performance on 5 video question answering tasks and 4 text-to-video retrieval tasks."},{"location":"Quantization-Survey-Paper-Notes/nlp/Visually-augmented_language_modeling/","title":"Visually-augmented language modeling","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Visually-augmented_language_modeling/#summary","title":"Summary","text":"<p>Summary: The paper proposes a pre-training framework called VALM, which utilizes visually-augmented text tokens for language modeling by retrieving relevant images. This approach enables multimodal grounded language modeling by attending to both text and visual knowledge in images. The proposed VALM outperforms strong language-only and vision-language baselines on various visual knowledge-intensive commonsense reasoning tasks, demonstrating substantial gains in reasoning object commonsense including color, size, and shape.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Visually-augmented_language_modeling/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Visually-augmented_language_modeling/#content","title":"Content","text":"<p>Human language is grounded on multimodal knowledge including visual knowledge like colors, sizes, and shapes. However, current large-scale pre-trained language models rely on text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary. To address this, we propose a novel pre-training framework, named VALM, to visually-augment text tokens with retrieved relevant images for Language Modeling. Specifically, VALM builds on a novel latent text-image alignment method via an image retrieval module to fetch corresponding images given a textual context. With the visually-augmented context, VALM uses a visual knowledge fusion layer to enable multimodal grounded language modeling by attending to both text context and visual knowledge in images. We evaluate VALM on various visual knowledge-intensive commonsense reasoning tasks, which require visual information to excel. The experimental results illustrate that VALM outperforms all strong language-only and vision-language baselines with substantial gains in reasoning object commonsense including color, size, and shape."},{"location":"Quantization-Survey-Paper-Notes/nlp/What_Language_Model_to_Train_if_You_Have_One_Million_GPU_Hours%3F/","title":"What Language Model to Train if You Have One Million GPU Hours?","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/What_Language_Model_to_Train_if_You_Have_One_Million_GPU_Hours%3F/#summary","title":"Summary","text":"<p>The research aims to find an architecture and training setup that will efficiently and accurately train billion-parameter scale models. The study includes a comparison of different modeling practices and their impact on zero-shot generalization, as well as an analysis of popular pre-training corpora and their impact on zero-shot generalization. The study also examines the performance of a multilingual model and scaling behavior of Transformers in choosing the target model size, shape, and training setup."},{"location":"Quantization-Survey-Paper-Notes/nlp/What_Language_Model_to_Train_if_You_Have_One_Million_GPU_Hours%3F/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/What_Language_Model_to_Train_if_You_Have_One_Million_GPU_Hours%3F/#content","title":"Content","text":"<p> The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization. In addition, we study the impact of various popular pre-training corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the English-only one. Finally, we consider the scaling behavior of Transformers to choose the target model size, shape, and training setup."},{"location":"Quantization-Survey-Paper-Notes/nlp/Word2bits-quantized_word_vectors/","title":"Word2bits-quantized word vectors","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Word2bits-quantized_word_vectors/#summary","title":"Summary","text":"<p>The paper proposes a method called Word2Bits to learn high-quality quantized word vectors using 1-2 bits per parameter, which reduces the storage and acts as a regularizer against overfitting. The experiments show that their method outperforms full precision word vectors on standard word similarity tasks and question answering while taking up only 8-16x less space. Word2Bits introduces a quantization function into the Word2Vec loss formulation."},{"location":"Quantization-Survey-Paper-Notes/nlp/Word2bits-quantized_word_vectors/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Word2bits-quantized_word_vectors/#content","title":"Content","text":"<p>Word vectors are used extensively in deep learning models for natural language processing, but they require a significant amount of memory and storage. In this paper, we propose a method to learn high-quality quantized word vectors using 1-2 bits per parameter, which not only reduces the required storage but also acts as a regularizer against overfitting. Our experiments show that our quantized word vectors outperform full precision word vectors on standard word similarity tasks and question answering while taking up only 8-16x less space. Our method, called Word2Bits, introduces a quantization function into the Word2Vec loss formulation."},{"location":"Quantization-Survey-Paper-Notes/nlp/Zerogen%3A_Efficient_zero-shot_learning_via_dataset_generation/","title":"Zerogen: Efficient zero-shot learning via dataset generation","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Zerogen%3A_Efficient_zero-shot_learning_via_dataset_generation/#summary","title":"Summary","text":"<p>This paper proposes a method called ZER0GEN for zero-shot learning, which involves generating a dataset from scratch using pre-trained language models, and then training a task model using this dataset. Experiments on several NLP tasks demonstrate the effectiveness of the proposed method."},{"location":"Quantization-Survey-Paper-Notes/nlp/Zerogen%3A_Efficient_zero-shot_learning_via_dataset_generation/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Zerogen%3A_Efficient_zero-shot_learning_via_dataset_generation/#content","title":"Content","text":"<p>There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZER0GEN. Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference, show the effectiveness of ZER0GEN."},{"location":"Quantization-Survey-Paper-Notes/nlp/Zeroquant%3A_Efficient_and_affordable_post-training_quantization_for_large-scale_transformers/","title":"Zeroquant: Efficient and affordable post-training quantization for large-scale transformers","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/Zeroquant%3A_Efficient_and_affordable_post-training_quantization_for_large-scale_transformers/#summary","title":"Summary","text":"<p>Authors present a post-training quantization method called ZeroQuant for compressing large Transformer-based models. The approach involves an end-to-end quantization and inference pipeline with three components: hardware-friendly quantization scheme, affordable layer-by-layer knowledge distillation (LKD), and highly optimized quantization system backend support. ZeroQuant can reduce the precision for weights and activations to INT8 leading to 5.19x/4.16x speedup on BERT and GPT-3-style models compared to FP16 inference. ZeroQuant plus LKD can quantize weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations leading to a 3x memory footprint reduction compared to the FP16 model. ZeroQuant can be applied to GPT-J 6B and GPT-NeoX 20B language models, leading to similar accuracy as FP16 but achieving up to 5.2x better efficiency."},{"location":"Quantization-Survey-Paper-Notes/nlp/Zeroquant%3A_Efficient_and_affordable_post-training_quantization_for_large-scale_transformers/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/Zeroquant%3A_Efficient_and_affordable_post-training_quantization_for_large-scale_transformers/#content","title":"Content","text":"<p>In this work, the authors present an efficient and affordable post-training quantization approach called ZeroQuant to compress large Transformer-based models. ZeroQuant involves an end-to-end quantization and inference pipeline with three main components: a fine-grained hardware-friendly quantization scheme, a novel affordable layer-by-layer knowledge distillation algorithm (LKD), and a highly optimized quantization system backend support. The authors show that ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT-3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference. ZeroQuant plus LKD can affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in a 3x memory footprint reduction compared to the FP16 model. The authors demonstrate that ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J 6B and GPT-NeoX 20B, for which the INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency."},{"location":"Quantization-Survey-Paper-Notes/nlp/%5BPDF%5D%5BPDF%5D_Multilingual_Mix%3A_Example_Interpolation_Improves_Multilingual_Neural_Machine_Translation/","title":"[PDF][PDF] Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/%5BPDF%5D%5BPDF%5D_Multilingual_Mix%3A_Example_Interpolation_Improves_Multilingual_Neural_Machine_Translation/#summary","title":"Summary","text":"<p> The paper introduces multilingual crossover encoder-decoder (mXEncDec), which fuses language pairs at an instance level by interpolating instances from different language pairs into joint 'crossover examples'. The proposed approach encourages sharing input and output spaces across languages and includes several techniques to improve example interpolation across dissimilar languages under heavy data imbalance. Experiments on a larhe-scale WMT multilingual dataset showed a significant improvement in quality on English-to-Many, Many-to-English, and zero-shot translation tasks ranging from +0.5 BLEU up to +5.5 BLEU points."},{"location":"Quantization-Survey-Paper-Notes/nlp/%5BPDF%5D%5BPDF%5D_Multilingual_Mix%3A_Example_Interpolation_Improves_Multilingual_Neural_Machine_Translation/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/%5BPDF%5D%5BPDF%5D_Multilingual_Mix%3A_Example_Interpolation_Improves_Multilingual_Neural_Machine_Translation/#content","title":"Content","text":"<p> Multilingual neural machine translation models are trained to maximize the likelihood of a mix of examples drawn from multiple language pairs. In this paper, the authors introduce multilingual crossover encoder-decoder (mXEncDec) to fuse language pairs at an instance level. Their approach interpolates instances from different language pairs into joint \u2018crossover examples\u2019 in order to encourage sharing input and output spaces across languages. They propose several techniques to improve example interpolation across dissimilar languages under heavy data imbalance. Experiments on a large-scale WMT multilingual dataset demonstrate that their approach significantly improves quality on English-to-Many, Many-to-English and zero-shot translation tasks (from +0.5 BLEU up to +5.5 BLEU points)."},{"location":"Quantization-Survey-Paper-Notes/nlp/nuqmm%3A_Quantized_matmul_for_efficient_inference_of_large-scale_generative_language_models/","title":"nuqmm: Quantized matmul for efficient inference of large-scale generative language models","text":""},{"location":"Quantization-Survey-Paper-Notes/nlp/nuqmm%3A_Quantized_matmul_for_efficient_inference_of_large-scale_generative_language_models/#summary","title":"Summary","text":"<p>Summary: The paper proposes an efficient inference framework for large-scale generative language models by quantizing weights using a non-uniform quantization method and accelerating quantized matrix multiplications with the proposed nuQmm kernel. This reduces the model size and latency of inference for large LMs, allowing a wide trade-off between compression ratio and accuracy. This is achieved by reducing the minimum required number of GPUs through high compression ratios using low-bit quantization.</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/nuqmm%3A_Quantized_matmul_for_efficient_inference_of_large-scale_generative_language_models/#target-task","title":"Target Task","text":"<p>nlp</p>"},{"location":"Quantization-Survey-Paper-Notes/nlp/nuqmm%3A_Quantized_matmul_for_efficient_inference_of_large-scale_generative_language_models/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/4-bit_Conformer_with_native_quantization_aware_training_for_speech_recognition/","title":"4-bit Conformer with native quantization aware training for speech recognition","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/4-bit_Conformer_with_native_quantization_aware_training_for_speech_recognition/#summary","title":"Summary","text":"<p>Summary: The paper proposes the use of 4-bit ASR models with native quantization aware training to achieve higher compression rates without compromising on performance. The experiments performed on state-of-the-art Conformer-based ASR models on the LibriSpeech dataset and a practical ASR system trained with large-scale datasets produced lossless models with significant size reduction.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/4-bit_Conformer_with_native_quantization_aware_training_for_speech_recognition/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/4-bit_Conformer_with_native_quantization_aware_training_for_speech_recognition/#content","title":"Content","text":"<p>Reducing the latency and model size for live Automatic Speech Recognition (ASR) has been a significant research problem. To compress neural networks and reduce computation cost, model quantization has become an increasingly popular approach. To achieve a higher compression rate without introducing additional performance regression, in this study, we propose to develop 4-bit ASR models with native quantization aware training. We conducted two experiments on state-of-the-art Conformer-based ASR models to evaluate our proposed quantization technique. First, we explored the impact of different precisions for both weight and activation quantization on the LibriSpeech dataset and produced a lossless 4-bit Conformer model with 5.8x size reduction. Following this, we investigated and revealed the viability of 4-bit quantization on a practical ASR system that is trained with large-scale datasets and produced a lossless Conformer ASR model with mixed 4-bit and 8-bit weights that has 5x size reduction."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/4-bit_quantization_of_LSTM-based_speech_recognition_models/","title":"4-bit quantization of LSTM-based speech recognition models","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/4-bit_quantization_of_LSTM-based_speech_recognition_models/#summary","title":"Summary","text":"<p>Summary: The paper investigates the impact of low-precision representations on large LSTM-based architectures for Automatic Speech Recognition (ASR). The authors demonstrate that minimal accuracy loss is achievable through the use of appropriate quantizers and initializations that are customized according to the local properties of the network. The solution is demonstrated on the Switchboard and CallHome test sets of the NIST Hub5-2000 evaluation.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/4-bit_quantization_of_LSTM-based_speech_recognition_models/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/4-bit_quantization_of_LSTM-based_speech_recognition_models/#content","title":"Content","text":"<p> We investigate the impact of aggressive low-precision representations of weights and activations in large LSTM-based architectures for Automatic Speech Recognition (ASR). Using a 4-bit integer representation, a na\u00efve quantization approach applied to the LSTM portion of these models results in significant Word Error Rate (WER) degradation. However, we show that minimal accuracy loss is achievable with an appropriate choice of quantizers and initializations. We customize quantization schemes depending on the local properties of the network, improving recognition performance while limiting computational time. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH) test sets of the NIST Hub5-2000 evaluation."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_Single_Self-Supervised_Model_for_Many_Speech_Modalities_Enables_Zero-Shot_Modality_Transfer/","title":"A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_Single_Self-Supervised_Model_for_Many_Speech_Modalities_Enables_Zero-Shot_Modality_Transfer/#summary","title":"Summary","text":"<p>Summary: The paper introduces a self-supervised pre-training framework, known as u-HuBERT, which can utilize both unimodal and multimodal speech for a unified masked cluster prediction objective. The authors demonstrate that their modality dropout technique during pre-training can lead to a single fine-tuned model outperforming state-of-the-art modality-specific models. Additionally, their model can achieve zero-shot modality generalization for various speech processing tasks when fine-tuned only on audio.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_Single_Self-Supervised_Model_for_Many_Speech_Modalities_Enables_Zero-Shot_Modality_Transfer/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_Single_Self-Supervised_Model_for_Many_Speech_Modalities_Enables_Zero-Shot_Modality_Transfer/#content","title":"Content","text":"<p>In this paper, the authors present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. They utilize modality dropout during pre-training to demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, their model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_lottery_ticket_hypothesis_framework_for_low-complexity_device-robust_neural_acoustic_scene_classification/","title":"A lottery ticket hypothesis framework for low-complexity device-robust neural acoustic scene classification","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_lottery_ticket_hypothesis_framework_for_low-complexity_device-robust_neural_acoustic_scene_classification/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new approach to neural model compression named Acoustic Lottery, which compresses an acoustic scene classification model by combining data augmentation, knowledge transfer, pruning, and quantization. This approach achieves superior performance compared to its uncompressed seed model. The proposed system uses a data augmentation process, teach-student learning mechanism, Lottery Ticket Hypothesis based pruning, two-stage fusion technique, and quantization block to deliver a low-complexity model.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_lottery_ticket_hypothesis_framework_for_low-complexity_device-robust_neural_acoustic_scene_classification/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_lottery_ticket_hypothesis_framework_for_low-complexity_device-robust_neural_acoustic_scene_classification/#content","title":"Content","text":"<p> We propose a novel neural model compression strategy combining data augmentation, knowledge transfer, pruning, and quantization for device-robust acoustic scene classification (ASC). Our model, called Acoustic Lottery, largely compresses an ASC model and achieves superior performance (validation accuracy of 79.4% and Log loss of 0.64) compared to its not compressed seed model. The proposed Acoustic Lottery system consists of a data augmentation process to improve model generalization, a teach-student learning mechanism to transfer knowledge, from a large teacher model to a small student model, a Lottery Ticket Hypothesis based pruning method to deliver a low-complexity model, a two-stage fusion technique to improve model prediction, and finally, a quantization block to deploy a final model."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_study_on_speech_enhancement_using_exponent-only_floating_point_quantized_neural_network_%28eofp-qnn%29/","title":"A study on speech enhancement using exponent-only floating point quantized neural network (eofp-qnn)","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_study_on_speech_enhancement_using_exponent-only_floating_point_quantized_neural_network_%28eofp-qnn%29/#summary","title":"Summary","text":"<p>Summary: The study proposes a novel exponent-only floating-point quantized neural network (EOFP-QNN) consisting of two stages (mantissa-quantization and exponent-quantization) and evaluates its performance on two types of neural networks (BLSTM and FCN) for speech enhancement task. The study concludes that the proposed EOFP quantization technique significantly reduces model sizes while maintaining satisfactory speech-enhancement performance.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_study_on_speech_enhancement_using_exponent-only_floating_point_quantized_neural_network_%28eofp-qnn%29/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/A_study_on_speech_enhancement_using_exponent-only_floating_point_quantized_neural_network_%28eofp-qnn%29/#content","title":"Content","text":"<p>The present study investigates the performance of speech enhancement using a novel exponent-only floating-point quantized neural network (EOFP-QNN). The proposed EOFP-QNN consists of two stages: mantissa-quantization and exponent-quantization. We evaluated the proposed EOFP quantization technique on two types of neural networks, namely, bidirectional long short-term memory (BLSTM) and fully convolutional neural network (FCN), on a speech enhancement task. Experimental results showed that the model sizes can be significantly reduced while maintaining satisfactory speech-enhancement performance."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Avqvc%3A_One-shot_voice_conversion_by_vector_quantization_with_applying_contrastive_learning/","title":"Avqvc: One-shot voice conversion by vector quantization with applying contrastive learning","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Avqvc%3A_One-shot_voice_conversion_by_vector_quantization_with_applying_contrastive_learning/#summary","title":"Summary","text":"<p>This paper proposes a novel framework called AVQVC that combines contrastive learning techniques and vector quantization voice conversion to improve the sound quality and performance of voice conversion models, without increasing complexity."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Avqvc%3A_One-shot_voice_conversion_by_vector_quantization_with_applying_contrastive_learning/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Avqvc%3A_One-shot_voice_conversion_by_vector_quantization_with_applying_contrastive_learning/#content","title":"Content","text":"<p>Voice Conversion (VC) is a widely researched subject that aims to convert speech from a source speaker to speech of a target speaker while keeping the discourse content the same. In this paper, a novel framework called A VQVC is proposed, which stands for AutoVC and vector quantization voice conversion. The A VQVC framework applies contrastive learning techniques to effectively separate the content and timbre information from speech, which yields improved sound quality of the generated speech. The approach outperforms the traditional VQVC model without increasing the model's complexity."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Bit-level_optimized_neural_network_for_multi-antenna_channel_quantization/","title":"Bit-level optimized neural network for multi-antenna channel quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Bit-level_optimized_neural_network_for_multi-antenna_channel_quantization/#summary","title":"Summary","text":"<p>The paper proposes a deep learning based CSI quantization method using a joint convolutional residual network (JC-ResNet) to reduce the overhead of CSI feedback and improve the bit-level quantization performance for MIMO channel feature extraction and recovery, and experimental results show significant improvement in performance."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Bit-level_optimized_neural_network_for_multi-antenna_channel_quantization/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Bit-level_optimized_neural_network_for_multi-antenna_channel_quantization/#content","title":"Content","text":"<p>Quantized channel state information (CSI) plays a critical role in precoding design which helps reap the merits of multiple-input multiple-output (MIMO) technology. In order to reduce the overhead of CSI feedback, we propose a deep learning based CSI quantization method by developing a joint convolutional residual network (JC-ResNet) which benefits MIMO channel feature extraction and recovery from the perspective of bit-level quantization performance. Experiments show that our proposed method substantially improves the performance."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/CCC-wav2vec_2.0%3A_Clustering_aided_Cross_Contrastive_Self-supervised_learning_of_speech_representations/","title":"CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/CCC-wav2vec_2.0%3A_Clustering_aided_Cross_Contrastive_Self-supervised_learning_of_speech_representations/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new pre-training strategy called ccc-wav2vec 2.0 that utilizes clustering and augmentation based cross-contrastive loss as its self-supervised objective. The proposed method achieves up to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on the test-clean and test-other sets respectively of LibriSpeech without the use of any language model. The paper also states that the proposed method shows up to 14.9% relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on Switchboard data.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/CCC-wav2vec_2.0%3A_Clustering_aided_Cross_Contrastive_Self-supervised_learning_of_speech_representations/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/CCC-wav2vec_2.0%3A_Clustering_aided_Cross_Contrastive_Self-supervised_learning_of_speech_representations/#content","title":"Content","text":"<p>While Self-Supervised Learning has helped reap the benefit of the scale from the available unlabeled data, the learning paradigms are continuously being bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which uses clustering and an augmentation based cross-contrastive loss as its self-supervised objective. ccc-wav2vec 2.0 achieves up to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on the test-clean and test-other sets respectively of LibriSpeech, without the use of any language model. The proposed method also achieves up to 14.9% relative WER improvement over the baseline wav2vec 2.0, when fine-tuned on Switchboard data."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Comparison_of_nonuniform_optimal_quantizer_designs_for_speech_coding_with_adaptive_critics_and_particle_swarm/","title":"Comparison of nonuniform optimal quantizer designs for speech coding with adaptive critics and particle swarm","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Comparison_of_nonuniform_optimal_quantizer_designs_for_speech_coding_with_adaptive_critics_and_particle_swarm/#summary","title":"Summary","text":"<p>Summary: The paper compares two different methods for designing optimal quantizers for speech coding - one based on adaptive critics and the other based on particle swarm optimization.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Comparison_of_nonuniform_optimal_quantizer_designs_for_speech_coding_with_adaptive_critics_and_particle_swarm/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Comparison_of_nonuniform_optimal_quantizer_designs_for_speech_coding_with_adaptive_critics_and_particle_swarm/#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/DGC-vector%3A_A_new_speaker_embedding_for_zero-shot_voice_conversion/","title":"DGC-vector: A new speaker embedding for zero-shot voice conversion","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/DGC-vector%3A_A_new_speaker_embedding_for_zero-shot_voice_conversion/#summary","title":"Summary","text":"<p>The paper proposes a new method, DGC-vector for speaker representation in zero-shot voice conversion algorithms. The method combines the benefits of D-vector, GST-based speaker representation, and auxiliary supervision to improve speaker similarity in zero-shot VC. The authors demonstrate through objective and subjective evaluations that the proposed method achieves decent performance and outperforms other baseline speaker representations. The paper also covers related works on speaker embeddings, including D-vector."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/DGC-vector%3A_A_new_speaker_embedding_for_zero-shot_voice_conversion/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/DGC-vector%3A_A_new_speaker_embedding_for_zero-shot_voice_conversion/#content","title":"Content","text":"<p> In this research paper, the authors propose a novel method called DGC-vector for speaker representation in zero-shot voice conversion algorithms. The paper studies the impact of speaker embeddings on zero-shot voice conversion performance and aims to improve the speaker similarity of zero-shot VC. The proposed method combines the advantages of D-vector, GST-based speaker representation, and auxiliary supervision. The authors show through objective and subjective evaluations that the proposed method achieves a decent performance on zero-shot VC and improves the speaker similarity significantly over other baseline speaker representations. The paper also briefly discusses related works in the field of speaker embeddings, including D-vector."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning-based_quantization_of_L-values_for_Gray-coded_modulation/","title":"Deep learning-based quantization of L-values for Gray-coded modulation","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning-based_quantization_of_L-values_for_Gray-coded_modulation/#summary","title":"Summary","text":"<p> The paper introduces a deep learning-based quantization scheme for log-likelihood ratio (L-value) storage, which is shown to be competitive with state-of-the-art schemes, reducing memory footprint by up to two and achieving a loss of performance smaller than 0.1dB with less than two effective bits per L-value. The authors analyze the dependency between average magnitudes of L-values and show that they follow a consistent ordering, independent of channel coefficient or interference distribution."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning-based_quantization_of_L-values_for_Gray-coded_modulation/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning-based_quantization_of_L-values_for_Gray-coded_modulation/#content","title":"Content","text":"<p> In this work, a deep learning-based quantization scheme for log-likelihood ratio (L-value) storage in general fading scenarios affected by interference is introduced. We analyze the dependency between the average magnitudes of different L-values and show they follow a consistent ordering, regardless of the channel coefficient or interference distribution. Our method is shown to be competitive with state-of-the-art maximum mutual information quantization schemes, reducing the required memory footprint by a ratio of up to two and achieving a loss of performance smaller than 0.1dB with less than two effective bits per L-value or smaller than 0.04dB with 2.25 effective bits."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning_for_massive_MIMO_with_1-bit_ADCs%3A_When_more_antennas_need_fewer_pilots/","title":"Deep learning for massive MIMO with 1-bit ADCs: When more antennas need fewer pilots","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning_for_massive_MIMO_with_1-bit_ADCs%3A_When_more_antennas_need_fewer_pilots/#summary","title":"Summary","text":"<p>This paper proposes a deep-learning based channel estimation framework for uplink massive MIMO systems with 1-bit ADCs, which utilizes prior channel estimation observations and deep neural networks to learn the mapping from quantized received measurements to channels. Increasing the number of base station antennas leads to improved channel estimation performance and requires fewer pilots. Previous works did not address deep learning-based channel estimation for massive MIMO systems with 1-bit ADCs or the observation that more antennas need fewer pilots."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning_for_massive_MIMO_with_1-bit_ADCs%3A_When_more_antennas_need_fewer_pilots/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning_for_massive_MIMO_with_1-bit_ADCs%3A_When_more_antennas_need_fewer_pilots/#content","title":"Content","text":"<p>This paper proposes a deep-learning based channel estimation framework for uplink massive MIMO systems with 1-bit ADCs. The framework leverages prior channel estimation observations and deep neural networks to learn the mapping from quantized received measurements to channels. The paper analytically proves that increasing the number of base station antennas leads to better channel estimation performance for the same pilot sequence length, and hence fewer pilots are required. Simulation results confirm these observations and show that more antennas lead to better channel estimation in terms of normalized mean squared error and signal-to-noise ratio per antenna. Prior work has not addressed deep learning-based channel estimation for massive MIMO systems with 1-bit ADCs or the counterintuitive observation that more antennas need fewer pilots."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning_vector_quantization_for_acoustic_information_retrieval/","title":"Deep learning vector quantization for acoustic information retrieval","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning_vector_quantization_for_acoustic_information_retrieval/#summary","title":"Summary","text":"<p> The paper proposes a deep learning vector quantization (DLVQ) algorithm based on deep neural networks that is capable of learning a codebook and improves over conventional VQ in classification problems. The proposed DLVQ is tested on an audio information retrieval task and shows promising results with a 10.5% relative gain in mean average precision (MAP) when initialized with the k-means VQ technique."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning_vector_quantization_for_acoustic_information_retrieval/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_learning_vector_quantization_for_acoustic_information_retrieval/#content","title":"Content","text":"<p>We propose a novel deep learning vector quantization (DLVQ) algorithm based on deep neural networks (DNNs). Utilizing a strong representation power of this deep learning framework, with any vector quantization (VQ) method as an initializer, the proposed DLVQ technique is capable of learning a code-constrained codebook and thus improves over conventional VQ to be used in classification problems. Tested on an audio information retrieval task, the proposed DLVQ achieves a quite promising performance when it is initialized by the k-means VQ technique. A 10.5% relative gain in mean average precision (MAP) is obtained after fusing the k-means and DLVQ results together."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_neural_network-based_quantized_signal_reconstruction_for_DOA_estimation/","title":"Deep neural network-based quantized signal reconstruction for DOA estimation","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_neural_network-based_quantized_signal_reconstruction_for_DOA_estimation/#summary","title":"Summary","text":"<p> The paper proposes a DNN-based recovery technique with low-cost ADC in an IRS for DOA estimation in a massive MIMO system. The proposed DNN estimates the quantization noise caused by the ADC and provides better reconstruction performance than existing methods. The network's reconstruction performance is compared with state-of-the-art methods, showing an improvement in DOA estimation using 1-bit ADC over 2-bit ADC."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_neural_network-based_quantized_signal_reconstruction_for_DOA_estimation/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_neural_network-based_quantized_signal_reconstruction_for_DOA_estimation/#content","title":"Content","text":"<p>For a massive multiple-input-multiple-output (MIMO) system using intelligent re\ufb02ecting surface (IRS) equipped with radio frequency (RF) chains, the multi-channel RF chains can be expensive, particularly when high-resolution and high-speed analog to digital converters (ADC) are used in each RF channel. In this paper, a direction of angle (DOA) estimation problem is investigated with low-cost ADC in IRS, and a deep neural network (DNN) is proposed as a recovery method for the low-resolution sampled signal. The proposed DNN estimates the quantization noise caused by the ADC and yields better reconstruction performance than state-of-the-art methods. Simulation results show that the performance of the DOA estimation using 1-bit ADC is improved to exceed that using 2-bit ADC. The paper presents a signal and quantization noise model and proposes a scheme for signal reconstruction. The reconstruction performance of the proposed network is also compared with state-of-the-art methods."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_neural_network_for_respiratory_sound_classification_in_wearable_devices_enabled_by_patient_specific_model_tuning/","title":"Deep neural network for respiratory sound classification in wearable devices enabled by patient specific model tuning","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_neural_network_for_respiratory_sound_classification_in_wearable_devices_enabled_by_patient_specific_model_tuning/#summary","title":"Summary","text":"<p> The paper proposes a deep CNN-RNN model that classifies respiratory sounds based on Mel-spectrograms, along with a patient-specific model tuning strategy for reliable anomaly detection. A local log quantization strategy is suggested to reduce the memory footprint while deploying on memory-constrained systems. The proposed model achieves state-of-the-art score on the ICBHI\u201917 dataset, and the patient-specific re-training strategy can be useful for wearable healthcare solutions."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_neural_network_for_respiratory_sound_classification_in_wearable_devices_enabled_by_patient_specific_model_tuning/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Deep_neural_network_for_respiratory_sound_classification_in_wearable_devices_enabled_by_patient_specific_model_tuning/#content","title":"Content","text":"<p>The primary objective of this paper is to build classi\ufb01cation models and strategies to identify breathing sound anomalies (wheeze, crackle) for automated diagnosis of respiratory and pulmonary diseases. In this work we propose a deep CNN-RNN model that classi\ufb01es respiratory sounds based on Mel-spectrograms. We also implement a patient speci\ufb01c model tuning strategy that \ufb01rst screens respiratory patients and then builds patient speci\ufb01c classi\ufb01cation models using limited patient data for reliable anomaly detection. Moreover, we devise a local log quantization strategy for model weights to reduce the memory footprint for deployment in memory constrained systems such as wearable devices. The proposed hybrid CNN-RNN model achieves a score of 66:31% on four-class classi\ufb01cation of breathing cycles for ICBHI\u201917 scienti\ufb01c challenge respiratory sound database. When the model is re-trained with patient speci\ufb01c data, it produces a score of 71:81% for leave-one-out validation. The proposed weight quantization technique achieves \u00194reduction in total memory cost without loss of performance. The main contribution of the paper is as follows: Firstly, the proposed model is able to achieve state of the art score on the ICBHI\u201917 dataset. Secondly, deep learning models are shown to successfully learn domain speci\ufb01c knowledge when pre-trained with breathing data and produce signi\ufb01cantly superior performance compared to generalized models. Finally, local log quantization of trained weights is shown to be able to reduce the memory requirement signi\ufb01cantly. This type of patient-speci\ufb01c re-training strategy can be very useful in developing reliable long-term automated patient monitoring systems particularly in wearable healthcare solutions."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Dithered_Quantization_for_Frequency-Domain_Speech_and_Audio_Coding./","title":"Dithered Quantization for Frequency-Domain Speech and Audio Coding.","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Dithered_Quantization_for_Frequency-Domain_Speech_and_Audio_Coding./#summary","title":"Summary","text":"<p>Summary: The paper discusses the use of dithered quantization in improving the perceived sound quality in speech and audio codecs that use discrete cosine transform (DCT) or modified discrete cosine transform (MDCT) techniques. The paper also explores how dithering can be applied in the frequency domain to further enhance the quality of MDCT-based codecs, especially in achieving high quality at low bit rates.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Dithered_Quantization_for_Frequency-Domain_Speech_and_Audio_Coding./#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Dithered_Quantization_for_Frequency-Domain_Speech_and_Audio_Coding./#content","title":"Content","text":"<p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/End-to-end_keyword_spotting_using_neural_architecture_search_and_quantization/","title":"End-to-end keyword spotting using neural architecture search and quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/End-to-end_keyword_spotting_using_neural_architecture_search_and_quantization/#summary","title":"Summary","text":"<p>Summary: This paper proposes a neural architecture search method to optimize convolutional neural networks for keyword spotting on raw audio waveforms in low-resource environments. They explore methods of quantization for reducing memory footprint and achieve a highly efficient KWS model with high accuracy using fixed and trained bit-width quantization.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/End-to-end_keyword_spotting_using_neural_architecture_search_and_quantization/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/End-to-end_keyword_spotting_using_neural_architecture_search_and_quantization/#content","title":"Content","text":"<p> This paper introduces a differentiable neural architecture search (NAS) method for optimizing the structure of convolutional neural networks (CNNs) for keyword spotting (KWS) on raw audio waveforms in resource-constrained environments. We conduct experiments on the Google speech commands dataset, comparing our approach to mel-frequency cepstral coefficient (MFCC)-based systems. We also investigate different methods of quantization for reducing memory footprint, including fixed and trained bit-width quantization. We obtain a highly efficient KWS model with an accuracy of 95.55% using 75.7k parameters and 13.6M operations with NAS alone. The same model achieves a test accuracy of 93.76% using trained bit-width quantization while using only 2.91 bits per activation and 2.51 bits per weight on average."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Finite_alphabet_iterative_decoding_of_LDPC_codes_with_coarsely_quantized_neural_networks/","title":"Finite alphabet iterative decoding of LDPC codes with coarsely quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Finite_alphabet_iterative_decoding_of_LDPC_codes_with_coarsely_quantized_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper presents a method of using Quantized Neural Networks (QNN) to design Finite Alphabet Message Passing Decoders (FAID) for Low-Density Parity Check (LDPC) codes. The authors train these networks to minimize the bit error rate by using straight-through estimators (STE) to handle the zero derivatives. The simulations show that training a QNN can obtain a FAID with 3-bit message and 4-bit channel output, which performs better than the floating-point min-sum decoding algorithm while maintaining good error performance in a flexible and efficient manner.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Finite_alphabet_iterative_decoding_of_LDPC_codes_with_coarsely_quantized_neural_networks/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Finite_alphabet_iterative_decoding_of_LDPC_codes_with_coarsely_quantized_neural_networks/#content","title":"Content","text":"<p>In this paper, the authors introduce a method of using quantized neural networks (QNN) to design finite alphabet message passing decoders (FAID) for Low-Density Parity Check (LDPC) codes. They train such networks while minimizing the bit error rate, which is a widely used and accurate metric to measure the performance of iterative decoders. They use straight-through estimators (STE) to handle the zero derivatives in the backward propagation caused by the low precision activations. Examples and simulations show that by training a QNN, a FAID with 3-bit of message and 4-bit of channel output can be obtained, which performs better than the more complex floating-point min-sum decoding algorithm. This methodology is promising in the sense that it facilitates designing low-precision FAID for LDPC codes while maintaining good error performance in a flexible and efficient manner."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Fixed-point_performance_analysis_of_recurrent_neural_networks/","title":"Fixed-point performance analysis of recurrent neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Fixed-point_performance_analysis_of_recurrent_neural_networks/#summary","title":"Summary","text":"<p>Summary: This paper proposes a quantization method based on retraining for analyzing the fixed-point performance of recurrent neural networks (RNNs). The paper studies the quantization sensitivity of each layer of RNNs and presents optimization results to minimize the capacity of weights while maintaining performance. A language model and a phoneme recognition example are used to demonstrate the proposed scheme. Layer-wise fixed-point sensitivity analysis results are also provided.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Fixed-point_performance_analysis_of_recurrent_neural_networks/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Fixed-point_performance_analysis_of_recurrent_neural_networks/#content","title":"Content","text":"<p> Recurrent neural networks (RNNs) have been successful in many applications, however, their implementation often requires increased complexity in hardware and software. This paper presents an analysis of the fixed-point performance of recurrent neural networks using a quantization method based on retraining. The work studies the quantization sensitivity of each layer in RNNs and presents overall fixed-point optimization results that minimize the capacity of weights while maintaining performance. A language model and a phoneme recognition example are used to demonstrate the proposed scheme. The paper provides an overview of the proposed quantization procedure for weights and signals, as well as layer-wise fixed-point sensitivity analysis results."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/GenerSpeech%3A_Towards_Style_Transfer_for_Generalizable_Out-Of-Domain_Text-to-Speech/","title":"GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/GenerSpeech%3A_Towards_Style_Transfer_for_Generalizable_Out-Of-Domain_Text-to-Speech/#summary","title":"Summary","text":"<p>The paper proposes GenerSpeech, a text-to-speech model for zero-shot style transfer of out-of-domain custom voices. It decomposes speech variations into style-agnostic and style-specific parts using a multi-level style adaptor and generalizable content adaptor with Mix-Style Layer Normalization. Evaluation shows that GenerSpeech outperforms state-of-the-art models in terms of audio quality and style similarity, and performs robustly in the few-shot data setting."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/GenerSpeech%3A_Towards_Style_Transfer_for_Generalizable_Out-Of-Domain_Text-to-Speech/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/GenerSpeech%3A_Towards_Style_Transfer_for_Generalizable_Out-Of-Domain_Text-to-Speech/#content","title":"Content","text":"<p>Style transfer for out-of-domain (OOD) speech synthesis aims to generate speech samples with unseen style derived from an acoustic reference, while facing the following challenges: 1) The highly dynamic style features in expressive voice are difficult to model and transfer; and 2) the TTS models should be robust enough to handle diverse OOD conditions that differ from the source data. This paper proposes GenerSpeech, a text-to-speech model towards high-fidelity zero-shot style transfer of OOD custom voice. GenerSpeech decomposes the speech variation into the style-agnostic and style-specific parts by introducing two components: 1) a multi-level style adaptor to efficiently model a large range of style conditions, including global speaker and emotion characteristics, and the local (utterance, phoneme, and word-level) fine-grained prosodic representations; and 2) a generalizable content adaptor with Mix-Style Layer Normalization to eliminate style information in the linguistic content representation and thus improve model generalization. Our evaluations on zero-shot style transfer demonstrate that GenerSpeech surpasses the state-of-the-art models in terms of audio quality and style similarity. The extension studies to adaptive style transfer further show that GenerSpeech performs robustly in the few-shot data setting."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Hardware_aware_training_for_efficient_keyword_spotting_on_general_purpose_and_specialized_hardware/","title":"Hardware aware training for efficient keyword spotting on general purpose and specialized hardware","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Hardware_aware_training_for_efficient_keyword_spotting_on_general_purpose_and_specialized_hardware/#summary","title":"Summary","text":"<p>This paper presents a study that utilizes Hardware Aware Training (HAT) to create new Keyword Spotting (KWS) neural networks based on the Legendre Memory Unit (LMU) that achieve state-of-the-art accuracy and have low parameter counts while operating efficiently on standard hardware. The study also identifies the power requirements of custom-designed accelerator hardware that outperforms general-purpose low power hardware and one designed for a specific goal."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Hardware_aware_training_for_efficient_keyword_spotting_on_general_purpose_and_specialized_hardware/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Hardware_aware_training_for_efficient_keyword_spotting_on_general_purpose_and_specialized_hardware/#content","title":"Content","text":"<p>Keyword spotting (KWS) is a crucial interface for various mobile and edge applications like phones, wearables,and cars. As KWS systems are \"always on,\" maximizing the accuracy and power efficiency of these systems is crucial for their success. This study uses Hardware Aware Training (HAT) to build new KWS neural networks based on the Legendre Memory Unit (LMU) that attain state-of-the-art (SotA) accuracy and have low parameter counts. Furthermore, this neural network can operate efficiently on standard hardware (212 \u00b5W). This study also identifies the power requirements of custom-designed accelerator hardware that achieves SotA power efficiency of 8.79 \u00b5W, outclassing general-purpose low power hardware (a microcontroller) by 24\u00d7 and one designed for a specific goal (ASICs) by 16\u00d7.  </p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Hitnet%3A_Hybrid_ternary_recurrent_neural_network/","title":"Hitnet: Hybrid ternary recurrent neural network","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Hitnet%3A_Hybrid_ternary_recurrent_neural_network/#summary","title":"Summary","text":"<p> The paper investigates the accuracy degradation of RNNs under different quantization schemes and proposes a hybrid ternary RNN called HitNet which can quantize RNN models into ternary values of {-1, 0, 1} and outperforms state-of-the-art methods towards extremely quantized RNNs. The authors develop a hybrid quantization method to quantize weights and activations and introduce a sloping factor into the activation functions to address the error-sensitive problem. The proposed method improves the perplexity per word (PPW) of a ternary LSTM on Penn Tree Bank from 126 to 110.3 and a ternary GRU from 142 to 113.5."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Hitnet%3A_Hybrid_ternary_recurrent_neural_network/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Hitnet%3A_Hybrid_ternary_recurrent_neural_network/#content","title":"Content","text":"<p>Quantization is a promising technique to reduce the model size, memory footprint, and computational cost of neural networks for employment on embedded devices with limited resources. In this paper, the authors investigate the accuracy degradation of RNNs under different quantization schemes and propose HitNet, a hybrid ternary RNN, which bridges the accuracy gap between the full precision model and the quantized model with ternary weights and activations. They develop a hybrid quantization method to quantize weights and activations, and introduce a sloping factor into the activation functions to address the error-sensitive problem. They test their method on typical RNN models and show that HitNet can quantize RNN models into ternary values of {-1, 0, 1} and significantly outperform the state-of-the-art methods towards extremely quantized RNNs. They improve the perplexity per word (PPW) of a ternary LSTM on Penn Tree Bank (PTB) corpus from 126 to 110.3 and a ternary GRU from 142 to 113.5."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Improving_the_speed_of_neural_networks_on_CPUs/","title":"Improving the speed of neural networks on CPUs","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Improving_the_speed_of_neural_networks_on_CPUs/#summary","title":"Summary","text":"<p> This paper provides a tutorial for minimizing computational cost in neural networks on modern x86 CPUs through techniques such as data layout, computation batching, and the use of SSE2, SSSE3, and SSE4 instructions. The paper uses speech recognition as an example task and shows a 10x speedup over an unoptimized baseline and a 4x speedup over an optimized floating-point baseline with no accuracy loss. The techniques presented can be applied to neural network training and provide a cost-effective alternative to specialized hardware."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Improving_the_speed_of_neural_networks_on_CPUs/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Improving_the_speed_of_neural_networks_on_CPUs/#content","title":"Content","text":"<p>Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 \ufb01xed-point instructions which provide a 3improvement over an optimized \ufb02oating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10speedup over an unoptimized baseline and a 4speedup over an aggressively optimized \ufb02oating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Increasing_compactness_of_deep_learning_based_speech_enhancement_models_with_parameter_pruning_and_quantization_techniques/","title":"Increasing compactness of deep learning based speech enhancement models with parameter pruning and quantization techniques","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Increasing_compactness_of_deep_learning_based_speech_enhancement_models_with_parameter_pruning_and_quantization_techniques/#summary","title":"Summary","text":"<p>Summary: The paper proposes a novel parameter pruning technique to remove redundant channels in a neural network and applies a parameter quantization technique to reduce its size. By integrating both techniques, the resulting SE model size was reduced by 10.03% with minor performance losses of 1.43% for STOI and 3.24% for PESQ. The proposed techniques can be useful for SE systems in devices with limited resources.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Increasing_compactness_of_deep_learning_based_speech_enhancement_models_with_parameter_pruning_and_quantization_techniques/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Increasing_compactness_of_deep_learning_based_speech_enhancement_models_with_parameter_pruning_and_quantization_techniques/#content","title":"Content","text":"<p>Most recent studies on deep learning based speech enhancement (SE) focused on improving denoising performance. However, successful SE applications require striking a desirable balance between denoising performance and computational cost in real scenarios. In this study, we propose a novel parameter pruning (PP) technique, which removes redundant channels in a neural network. In addition, a parameter quantization (PQ) technique was applied to reduce the size of a neural network by representing weights with fewer cluster centroids. Because the techniques are derived based on different concepts, the PP and PQ can be integrated to provide even more compact SE models. The experimental results show that the PP and PQ techniques produce a compacted SE model with a size of only 10.03 % compared to that of the original model, resulting in minor performance losses of 1.43% (from 0.70 to 0.69) for STOI and 3.24% (from 1.85 to 1.79) for PESQ. The promising results suggest that the PP and PQ techniques can be used in an SE system in devices with limited storage and computation resources."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Integer-only_zero-shot_quantization_for_efficient_speech_recognition/","title":"Integer-only zero-shot quantization for efficient speech recognition","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Integer-only_zero-shot_quantization_for_efficient_speech_recognition/#summary","title":"Summary","text":"<p> The paper proposes an integer-only, zero-shot quantization scheme for ASR models that achieves significant speedup and compression rate. The approach is applied to QuartzNet, Jasper, and Conformer without any access to training and/or validation data and results in a modest WER degradation of &lt;1% with INT8 quantization."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Integer-only_zero-shot_quantization_for_efficient_speech_recognition/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Integer-only_zero-shot_quantization_for_efficient_speech_recognition/#content","title":"Content","text":"<p>End-to-end neural network models for speech recognition tasks often perform poorly on edge hardware due to large memory and computation requirements. While quantizing model weights and/or activations to low-precision can be a promising solution, previous research lacks integer-only and zero-shot quantization methods for ASR models. In this paper, we propose an integer-only, zero-shot quantization scheme for ASR models that achieves negligible WER degradation even without any access to training and/or validation data. We apply our method to QuartzNet, Jasper, and Conformer and demonstrate significant speedup and compression rate, with a modest WER degradation of &lt;1% with INT8 quantization."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Joint_antenna_selection_and_hybrid_beamformer_design_using_unquantized_and_quantized_deep_learning_networks/","title":"Joint antenna selection and hybrid beamformer design using unquantized and quantized deep learning networks","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Joint_antenna_selection_and_hybrid_beamformer_design_using_unquantized_and_quantized_deep_learning_networks/#summary","title":"Summary","text":"<p> The paper proposes a deep learning-based approach for joint antenna selection and hybrid beamformer design in millimeter wave communications. This approach uses convolutional neural networks (CNNs) trained with several noisy channel matrices to achieve robust performance. The proposed CNN framework provides an order better spectral efficiency and is 10 times faster than conventional techniques. Additionally, the proposed network, saved in no more than 5 bits, is suitable for digital mobile devices."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Joint_antenna_selection_and_hybrid_beamformer_design_using_unquantized_and_quantized_deep_learning_networks/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Joint_antenna_selection_and_hybrid_beamformer_design_using_unquantized_and_quantized_deep_learning_networks/#content","title":"Content","text":"<p> In this paper, the authors propose a deep learning-based approach to joint antenna selection and hybrid beamformer design for massive MIMO systems in millimeter wave communications. They formulate the problem as a classification/prediction task for convolutional neural networks (CNNs). The CNNs are trained with several noisy channel matrices to achieve robust performance. Numerical experiments reveal that the proposed CNN framework provides an order better spectral efficiency and is 10 times faster than conventional techniques. The authors also demonstrate that the proposed network, saved in no more than 5 bits, is suited for digital mobile devices."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Linear_receivers_for_massive_MIMO_systems_with_one-bit_ADCs/","title":"Linear receivers for massive MIMO systems with one-bit ADCs","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Linear_receivers_for_massive_MIMO_systems_with_one-bit_ADCs/#summary","title":"Summary","text":"<p>Summary: The authors propose three linear receivers for massive MIMO systems with 1-bit ADCs which are Bussgang-based Maximal Ratio Combining (BMRC), Bussgang-based Zero-Forcing (BZF), and Bussgang-based Minimum Mean Squared Error (BMMSE). These receivers are obtained by using the Bussgang decomposition to handle the non-linear effect of one-bit ADCs. Simulation results indicate that the newly proposed receivers have lower bit error rate floors compared to conventional linear receivers.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Linear_receivers_for_massive_MIMO_systems_with_one-bit_ADCs/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Linear_receivers_for_massive_MIMO_systems_with_one-bit_ADCs/#content","title":"Content","text":"<p> In this letter, the authors propose three linear receivers for massive MIMO systems with 1-bit ADCs: Bussgang-based Maximal Ratio Combining (BMRC), Bussgang-based Zero-Forcing (BZF), and Bussgang-based Minimum Mean Squared Error (BMMSE). The proposed receivers are obtained by using the Bussgang decomposition to cope with the non-linear effect of the one-bit ADCs. Simulation results show significantly lower bit error rate floors obtained by the proposed receivers than those of conventional linear receivers."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Low-bit_quantization_and_quantization-aware_training_for_small-footprint_keyword_spotting/","title":"Low-bit quantization and quantization-aware training for small-footprint keyword spotting","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Low-bit_quantization_and_quantization-aware_training_for_small-footprint_keyword_spotting/#summary","title":"Summary","text":"<p> The paper proposes a new method called dynamic quantization to reduce memory and computation footprint of deep neural network (DNN) based Keyword Spotters (KWS). Dynamic quantization performs column-wise quantization of DNN weight matrices using each column's exact individual min-max range. The paper also proposes a quantization-aware training approach that incorporates quantization errors into the KWS model during training. These approaches together achieve accuracy close to full precision models while reducing the models' on-device memory footprint up to 80%."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Low-bit_quantization_and_quantization-aware_training_for_small-footprint_keyword_spotting/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Low-bit_quantization_and_quantization-aware_training_for_small-footprint_keyword_spotting/#content","title":"Content","text":"<p> In this paper, the authors propose a novel method for reducing the memory and computational footprint of deep neural network (DNN) based keyword spotters (KWS). This method, called dynamic quantization, performs column-wise quantization of DNN weight matrices using each column's exact individual min-max range. Additionally, the authors propose a quantization-aware training approach that incorporates quantization errors into the KWS model during training. Together, these approaches significantly improve the performance of KWS in 4-bit and 8-bit quantized precision, achieving end-to-end accuracy close to that of full precision models while reducing the models' on-device memory footprint by up to 80%."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Memory_requirement_reduction_of_deep_neural_networks_for_field_programmable_gate_arrays_using_low-bit_quantization_of_parameters/","title":"Memory requirement reduction of deep neural networks for field programmable gate arrays using low-bit quantization of parameters","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Memory_requirement_reduction_of_deep_neural_networks_for_field_programmable_gate_arrays_using_low-bit_quantization_of_parameters/#summary","title":"Summary","text":"<p>Summary: This paper proposes a method that uses non-uniform fixed-point quantization and virtual bit shift to improve the accuracy of quantization of deep neural network weights. The method is tested in a speech enhancement application where a fully connected DNN is used. The low-bit quantization method reduces DNN memory requirements by 50% while STOI performance only drops by 2.7%.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Memory_requirement_reduction_of_deep_neural_networks_for_field_programmable_gate_arrays_using_low-bit_quantization_of_parameters/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Memory_requirement_reduction_of_deep_neural_networks_for_field_programmable_gate_arrays_using_low-bit_quantization_of_parameters/#content","title":"Content","text":"<p>Effective employment of deep neural networks (DNNs) in mobile devices and embedded systems, like field programmable gate arrays, is hampered by requirements for memory and computational power. In this paper we propose a method that employs a non-uniform fixed-point quantization and a virtual bit shift (VBS) to improve the accuracy of the quantization of the DNN weights. We evaluate our method in a speech enhancement application, where a fully connected DNN is used to predict the clean speech spectrum from the input noisy speech spectrum. A DNN is optimized, its memory requirement is calculated, and its performance is evaluated using the short-time objective intelligibility (STOI) metric. The application of the low-bit quantization leads to a 50% reduction of the DNN memory requirement while the STOI performance drops only by 2.7%."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Neural_Codec_Language_Models_are_Zero-Shot_Text_to_Speech_Synthesizers/","title":"Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Neural_Codec_Language_Models_are_Zero-Shot_Text_to_Speech_Synthesizers/#summary","title":"Summary","text":"<p>Summary: The paper proposes a language modeling approach for TTS using a neural codec language model called VALL-E. It involves training a model using discrete codes from a neural audio codec model and regarding TTS as a conditional language modeling task. During pre-training, the English speech database is scaled up to 60K hours. VALL-E can synthesize high-quality personalized speech with only a 3-second recording of an unseen speaker as an acoustic prompt. The experimental results show that VALL-E outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. Additionally, VALL-E preserves the speaker's emotion and acoustic environment of the acoustic prompt in synthesis.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Neural_Codec_Language_Models_are_Zero-Shot_Text_to_Speech_Synthesizers/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Neural_Codec_Language_Models_are_Zero-Shot_Text_to_Speech_Synthesizers/#content","title":"Content","text":"<p>We introduce a language modeling approach for text to speech synthesis (TTS). Speci\ufb01cally, we train a neural codec language model (called VALL-E ) using discrete codes derived from an off-the-shelf neural audio codec model, and re- gard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that VALL-E signi\ufb01cantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we \ufb01nd VALL-E could preserve the speaker\u2019s emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Neural_analysis_and_synthesis%3A_Reconstructing_speech_from_self-supervised_representations/","title":"Neural analysis and synthesis: Reconstructing speech from self-supervised representations","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Neural_analysis_and_synthesis%3A_Reconstructing_speech_from_self-supervised_representations/#summary","title":"Summary","text":"<p>The paper introduces a neural analysis and synthesis framework called NANSY that can manipulate voice, speed and pitch of speech signals with high reconstruction quality and controllability. Unlike previous works, this framework does not use any bottleneck structures for disentangling analysis features. It uses a novel training strategy based on information perturbation and features including wav2vec and newly proposed pitch feature called Yingram for self-supervised training. NANSY can also be extended to a multilingual setting. The framework performs well in several applications such as pitch shift, time-scale modification, and zero-shot voice conversion."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Neural_analysis_and_synthesis%3A_Reconstructing_speech_from_self-supervised_representations/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Neural_analysis_and_synthesis%3A_Reconstructing_speech_from_self-supervised_representations/#content","title":"Content","text":"<p>We present a neural analysis and synthesis (NANSY) framework for manipulating voice, pitch and speed of any given speech signal. We address the poor reconstruction quality obtained from previous works that used information bottleneck to disentangle analysis features for controllable synthesis, by proposing a novel training strategy based on information perturbation. NANSY does not require any bottleneck structures and enjoys high reconstruction quality and controllability. The framework utilizes a new set of analysis features \u2013 wav2vec feature and newly proposed pitch feature, Yingram, which allows for fully self-supervised training. NANSY can be extended to a multilingual setting by training on a multilingual dataset. The experiments show that NANSY achieves significant improvement in performance in several applications such as zero-shot voice conversion, pitch shift, and time-scale modification."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Noisevc%3A_Towards_high_quality_zero-shot_voice_conversion/","title":"Noisevc: Towards high quality zero-shot voice conversion","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Noisevc%3A_Towards_high_quality_zero-shot_voice_conversion/#summary","title":"Summary","text":"<p> The paper proposes NoiseVC, an approach for zero-shot voice conversion using Vector Quantization (VQ) and Contrastive Predictive Coding (CPC) with Noise Augmentation to enhance disentanglement capability. The approach doesn't rely on parallel data, pre-trained models or text transcripts, making it suitable for low-resourced languages. The authors conduct several experiments and demonstrate that NoiseVC has strong disentanglement ability with a small quality sacrifice."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Noisevc%3A_Towards_high_quality_zero-shot_voice_conversion/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Noisevc%3A_Towards_high_quality_zero-shot_voice_conversion/#content","title":"Content","text":"<p> In this paper, the authors propose NoiseVC, an approach for zero-shot voice conversion that can disentangle contents based on Vector Quantization (VQ) and Contrastive Predictive Coding (CPC). They perform Noise Augmentation to further enhance the disentanglement capability. The authors conduct several experiments and demonstrate that NoiseVC has a strong disentanglement ability with a small sacrifice of quality. They compare their approach with previous methods and show that it does not rely on parallel data, pre-trained models or text transcripts, making it particularly suitable for low-resourced languages."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Normalization_helps_training_of_quantized_LSTM/","title":"Normalization helps training of quantized LSTM","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Normalization_helps_training_of_quantized_LSTM/#summary","title":"Summary","text":"<p> The paper discusses how quantization can reduce the computational demands of long-short-term memory (LSTM) weights but the exploding gradient problem makes training difficult, especially with large weight matrices. The paper proposes that normalization techniques, such as weight normalization, layer normalization, and batch normalization, can help train quantized LSTMs by stabilizing the gradient magnitude. Normalized quantized LSTMs perform significantly better than unnormalized ones and are comparable to full-precision LSTMs in terms of performance while being smaller in size. The paper also identifies the limitations of existing quantization methods and the inefficiencies of the batch normalization extension."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Normalization_helps_training_of_quantized_LSTM/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Normalization_helps_training_of_quantized_LSTM/#content","title":"Content","text":"<p> The paper discusses the use of quantization on long-short-term memory (LSTM) weights to reduce their computational demands. However, training a quantized LSTM is difficult due to the exploding gradient problem, especially when weight matrices are large. The paper shows that normalization techniques, such as weight normalization, layer normalization, and batch normalization, can stabilize the gradient magnitude and help train quantized LSTMs. Normalized quantized LSTMs perform significantly better than their unnormalized counterparts and are comparable to full-precision LSTMs in terms of performance while being much smaller in size. The paper also highlights the inefficiencies of existing quantization methods and the limitations of the batch normalization extension in storage and computational cost."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/On_the_efficient_representation_and_execution_of_deep_acoustic_models/","title":"On the efficient representation and execution of deep acoustic models","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/On_the_efficient_representation_and_execution_of_deep_acoustic_models/#summary","title":"Summary","text":"<p> The paper proposes a quantization scheme to reduce the resolution of a neural network's parameters from 32-bit floating point values to 8-bit integer values for memory savings and optimized hardware instructions. A 'quantization aware' training process is proposed to recover most of the accuracy loss caused by quantization. The techniques are validated on a long short-term memory-based acoustic model for a speech recognition task and can be applied to other deep learning models and domains."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/On_the_efficient_representation_and_execution_of_deep_acoustic_models/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/On_the_efficient_representation_and_execution_of_deep_acoustic_models/#content","title":"Content","text":"<p> In this paper, we propose a simple and computationally efficient quantization scheme to reduce the resolution of the parameters of a neural network from 32-bit floating point values to 8-bit integer values. This enables significant memory savings and the use of optimized hardware instructions for integer arithmetic, thus reducing the cost of inference. We also propose a 'quantization aware' training process to recover most of the loss in accuracy introduced by quantization. We validate our techniques by applying them to a long short-term memory-based acoustic model on a large vocabulary speech recognition task. The proposed techniques can be applied to other deep learning models and domains."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/On_the_quantization_of_recurrent_neural_networks/","title":"On the quantization of recurrent neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/On_the_quantization_of_recurrent_neural_networks/#summary","title":"Summary","text":"<p>The paper talks about the challenges related to integer quantization of neural networks that reduce memory consumption and ML execution time, but may negatively impact the model's quality. The authors present an integer-only quantization strategy for LSTM neural network topologies that is accurate, efficient, and uses 8-bit integer weights and mostly 8-bit activations. The strategy targets various hardware and leverages instruction sets available in CPU architectures and neural accelerators. The paper discusses various factors that influence quantization, including representations in the integer domain, precision used for computation results, and conversion of intermediate results for efficient computation."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/On_the_quantization_of_recurrent_neural_networks/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/On_the_quantization_of_recurrent_neural_networks/#content","title":"Content","text":"<p>Integer quantization of neural networks is an important technique that reduces memory consumption and machine learning (ML) execution time. However, quantization may negatively affect the model's quality. Therefore, devising quantization strategies that preserve the quality of the original neural networks while providing its benefits is a challenging area of research. In this work, the authors present an integer-only quantization strategy for Long Short-Term Memory (LSTM) neural network topologies, which are the foundation of many production ML systems. This quantization strategy is accurate, efficient, and fast to execute, utilizing 8-bit integer weights and mostly 8-bit activations. Additionally, it is able to target various hardware by leveraging instruction sets available in common CPU architectures and available neural accelerators. The researchers address the challenging problem of fully quantizing RNNs, particularly LSTM topologies, which are extensively employed in state-of-the-art production systems such as speech recognition, text translation, and text-to-speech, among others. They discuss various factors that influence quantization, including representations in the integer domain, the precision used to represent the results of computations, and how the intermediate results are converted for subsequent computations to occur efficiently."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Phonological_vocoding_using_artificial_neural_networks/","title":"Phonological vocoding using artificial neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Phonological_vocoding_using_artificial_neural_networks/#summary","title":"Summary","text":"<p>Summary: The paper proposes a new approach for the task of phonological vocoding based on artificial neural networks (ANNs). They investigate the use of ANNs for the compression of speech in the context of low bit-rate speech coding. The approach is evaluated on the TIMIT speech corpus using two different vocoding schemes. The results show that the proposed ANNs-based vocoding approach outperforms the state-of-the-art MELP vocoder in terms of spectral distortion, cepstral distortion, and perceptual quality. Additionally, the proposed approach can facilitate better speaker recognition performance compared to MELP.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Phonological_vocoding_using_artificial_neural_networks/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Phonological_vocoding_using_artificial_neural_networks/#content","title":"Content","text":"<p>Abstract: In this paper, we present an approach for the task of phonological vocoding based on artificial neural networks (ANNs). Specifically, we investigate the use of ANNs for the compression of speech in the context of low bit-rate speech coding. The approach is evaluated on the TIMIT speech corpus using two different vocoding schemes. We present a quantitative analysis of the results in terms of spectral distortion, cepstral distortion, and perceptual quality. The results show that the proposed ANNs-based vocoding approach can achieve a significant improvement in terms of spectral distortion and perceptual quality compared to the state-of-the-art MELP vocoder. Additionally, the proposed approach can facilitate better speaker recognition performance compared to MELP.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Precision_scaling_of_neural_networks_for_efficient_audio_processing/","title":"Precision scaling of neural networks for efficient audio processing","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Precision_scaling_of_neural_networks_for_efficient_audio_processing/#summary","title":"Summary","text":"<p>The paper presents a study on the impact of scaling the precision of neural networks for efficient audio processing like voice-activity detection and single-channel speech enhancement. Lower bit precision in neural networks results in a significant reduction in processing time (up to 30x) with a negligible (&lt; 3.14%) impact on performance only in the case of classification tasks. The authors have also presented a table with computation/memory demand and performance of DNNs with baseline/reduced bit width."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Precision_scaling_of_neural_networks_for_efficient_audio_processing/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Precision_scaling_of_neural_networks_for_efficient_audio_processing/#content","title":"Content","text":"<p>In this paper, the authors present a study on the impact of scaling the precision of neural networks for efficient audio processing, specifically on voice-activity detection and single-channel speech enhancement. Through experiments conducted with real user data, they demonstrate that deep neural networks that use lower bit precision can significantly reduce the processing time (up to 30x), while their performance impact is low (&lt; 3.14%) only in the case of classification tasks such as those present in voice activity detection. The authors also provide computation/memory demand and performance of DNNs with baseline/reduced bit width in a table."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/QTI_submission_to_DCASE_2021%3A_Residual_normalization_for_device-imbalanced_acoustic_scene_classification_with_efficient_design/","title":"QTI submission to DCASE 2021: Residual normalization for device-imbalanced acoustic scene classification with efficient design","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/QTI_submission_to_DCASE_2021%3A_Residual_normalization_for_device-imbalanced_acoustic_scene_classification_with_efficient_design/#summary","title":"Summary","text":"<p>The paper discusses the design of an audio scene classification system named Residual Normalization under the constraints of model complexity, with four proposed methods. These methods include Residual Normalization, BC-ResNet-Mod, spectrogram-to-spectrogram translation, and model compression schemes, thereby achieving an average test accuracy of 76.3% in TAU Urban Acoustic Scenes 2020 Mobile dataset containing 315k parameters. The accuracy remained stable at 75.3% even after the model was compressed to 61.0KB."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/QTI_submission_to_DCASE_2021%3A_Residual_normalization_for_device-imbalanced_acoustic_scene_classification_with_efficient_design/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/QTI_submission_to_DCASE_2021%3A_Residual_normalization_for_device-imbalanced_acoustic_scene_classification_with_efficient_design/#content","title":"Content","text":"<p> This technical report describes the details of our TASK1A submission of the DCASE2021 challenge. The goal of the task is to design an audio scene classification system for device-imbalanced datasets under the constraints of model complexity. This report introduces four methods to achieve the goal. First, we propose Residual Normalization, a novel feature normalization method that uses instance normalization with a shortcut path to discard unnecessary device-specific information without losing useful information for classification. Second, we design an efficient architecture, BC-ResNet-Mod, a modified version of the baseline architecture with a limited receptive field. Third, we exploit spectrogram-to-spectrogram translation from one to multiple devices to augment training data. Finally, we utilize three model compression schemes: pruning, quantization, and knowledge distillation to reduce model complexity. The proposed system achieves an average test accuracy of 76.3% in TAU Urban Acoustic Scenes 2020 Mobile, development dataset with 315k parameters, and average test accuracy of 75.3% after compression to 61.0KB of non-zero parameters."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Quantization_of_acoustic_model_parameters_in_automatic_speech_recognition_framework/","title":"Quantization of acoustic model parameters in automatic speech recognition framework","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Quantization_of_acoustic_model_parameters_in_automatic_speech_recognition_framework/#summary","title":"Summary","text":"<p>This paper studies the impact of parameter quantization on the word recognition accuracy in hybrid automatic speech recognition systems that use deep neural network based models trained with the LF-MMI criterion and n-gram language models. The study was conducted on the standard Librispeech setup and the results provide an overview of recognition accuracy with respect to the applied quantization scheme."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Quantization_of_acoustic_model_parameters_in_automatic_speech_recognition_framework/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Quantization_of_acoustic_model_parameters_in_automatic_speech_recognition_framework/#content","title":"Content","text":"<p>State-of-the-art hybrid automatic speech recognition (ASR) system exploits deep neural network (DNN) based acoustic models (AM) trained with Lattice Free-Maximum Mutual Information (LF-MMI) criterion and n-gram language models. The impact of parameter quantization on the overall word recognition performance is studied in this paper. Results obtained on standard Librispeech setup provide an interesting overview of recognition accuracy w.r.t. applied quantization scheme."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Resource-efficient_dnns_for_keyword_spotting_using_neural_architecture_search_and_quantization/","title":"Resource-efficient dnns for keyword spotting using neural architecture search and quantization","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Resource-efficient_dnns_for_keyword_spotting_using_neural_architecture_search_and_quantization/#summary","title":"Summary","text":"<p>This paper proposes using neural architecture search to optimize CNNs for keyword spotting in low-resource environments. A highly efficient model was obtained with 95.4% accuracy using only 494.8 kB of memory and 19.6 million operations. Weight quantization was used to reduce memory consumption. Experimental results and comparisons with related work are presented."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Resource-efficient_dnns_for_keyword_spotting_using_neural_architecture_search_and_quantization/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Resource-efficient_dnns_for_keyword_spotting_using_neural_architecture_search_and_quantization/#content","title":"Content","text":"<p>This paper presents a neural architecture search approach to optimize convolutional neural networks for keyword spotting in low-resource environments. The approach was able to obtain a highly efficient model with 95.4% accuracy on the Google speech commands dataset using only 494.8 kB of memory and 19.6 million operations. The model was further optimized using weight quantization, which reduced memory consumption. The paper also shows that increasing the number of input features substantially improves performance. The paper provides experimental results and comparisons with related work. Code related to the paper is available online."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Robust_speech_recognition_via_large-scale_weak_supervision/","title":"Robust speech recognition via large-scale weak supervision","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Robust_speech_recognition_via_large-scale_weak_supervision/#summary","title":"Summary","text":"<p>Summary: The paper discusses the study of speech processing systems that are trained to predict transcripts of audio on the internet, which resulted in models that generalize well and approach human-level accuracy and robustness, even in zero-shot transfer settings without fine-tuning. The researchers are releasing the models and inference code for further research on robust speech processing.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Robust_speech_recognition_via_large-scale_weak_supervision/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Robust_speech_recognition_via_large-scale_weak_supervision/#content","title":"Content","text":"<p> We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero- shot transfer setting without the need for any \ufb01ne- tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/SIG-VC%3A_A_Speaker_Information_Guided_Zero-Shot_Voice_Conversion_System_for_Both_Human_Beings_and_Machines/","title":"SIG-VC: A Speaker Information Guided Zero-Shot Voice Conversion System for Both Human Beings and Machines","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/SIG-VC%3A_A_Speaker_Information_Guided_Zero-Shot_Voice_Conversion_System_for_Both_Human_Beings_and_Machines/#summary","title":"Summary","text":"<p> The paper presents a novel method called SIG-VC for zero-shot voice conversion, which disentangles speaker and content information by extracting pre-trained speaker verification and acoustic models to remove speaker information from speech. The proposed system outperforms previous state-of-the-art systems in terms of subjective and objective metrics, indicating a significant reduction in the trade-off problem in zero-shot voice conversion."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/SIG-VC%3A_A_Speaker_Information_Guided_Zero-Shot_Voice_Conversion_System_for_Both_Human_Beings_and_Machines/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/SIG-VC%3A_A_Speaker_Information_Guided_Zero-Shot_Voice_Conversion_System_for_Both_Human_Beings_and_Machines/#content","title":"Content","text":"<p> Nowadays, zero-shot voice conversion is gaining attention in extreme conditions where data of source or target speakers is scarce and unseen in the training stage. In this paper, a novel method called SIG-VC is proposed for zero-shot voice conversion. The proposed system disentangles speaker and content information to remove the residual speaker information from the speech. A pre-trained speaker verification system is used for the speaker information extraction, and a pre-trained acoustic model is applied to extract the linguistic feature. By sharing the parameters of some modules in the system and forcing the intermediate representation in the vector space of the selected acoustic feature, the intermediate representation is supervised to remove the speaker information from the linguistic information. Conditioning on the speaker information of the target speaker, the system manages to generate the target speech while adding feedback loss control on the output, resulting in high spoofing capability to speaker verification systems. The proposed system outperforms the previous state-of-the-art system, AGAIN-VC. Subjective and objective metrics are used for evaluation, indicating the significant reduction in the trade-off problem in zero-shot voice conversion."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Small-footprint_open-vocabulary_keyword_spotting_with_quantized_LSTM_networks/","title":"Small-footprint open-vocabulary keyword spotting with quantized LSTM networks","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Small-footprint_open-vocabulary_keyword_spotting_with_quantized_LSTM_networks/#summary","title":"Summary","text":"<p> The paper presents an open-vocabulary keyword spotting technique using a quantized LSTM neural network trained with CTC. The model is lightweight, weighing less than 500KB and can run on tiny devices. It can handle any user-defined keywords and does not require specific training data for them. The confidence scores are calibrated using predictions of CTC-trained networks and a fast detection algorithm is implemented. The proposed system performs better than standard keyword-filler models."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Small-footprint_open-vocabulary_keyword_spotting_with_quantized_LSTM_networks/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Small-footprint_open-vocabulary_keyword_spotting_with_quantized_LSTM_networks/#content","title":"Content","text":"<p> We present a small-footprint, open-vocabulary keyword spotting method based on a quantized long short-term memory (LSTM) neural network trained with connectionist temporal classification (CTC). Our approach is designed to be able to run on tiny devices and handle any arbitrary set of user-defined keywords without requiring training data specific to those keywords. The model weighs less than 500KB and takes advantage of some properties of the predictions of CTC-trained networks to calibrate the confidence scores and implement a fast detection algorithm. Our proposed system outperforms standard keyword-filler models."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/SpeechCLIP%3A_Integrating_Speech_with_Pre-Trained_Vision_and_Language_Model/","title":"SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/SpeechCLIP%3A_Integrating_Speech_with_Pre-Trained_Vision_and_Language_Model/#summary","title":"Summary","text":"<p>Summary:  The paper proposes a framework called SpeechCLIP that connects speech and text through images to enhance speech models without transcriptions. The model aligns pre-trained HuBERT and CLIP models using paired images and spoken captions with minimal fine-tuning. The proposed model outperforms the prior state-of-the-art on image-speech retrieval and performs zero-shot speech-text retrieval without direct supervision from transcriptions. Visually grounded speech models (VGS) that utilize paired image-speech data to enhance speech processing are also discussed, and it is shown that VGS models trained with retrieval objectives can extract semantic and word-level information from speech.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/SpeechCLIP%3A_Integrating_Speech_with_Pre-Trained_Vision_and_Language_Model/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/SpeechCLIP%3A_Integrating_Speech_with_Pre-Trained_Vision_and_Language_Model/#content","title":"Content","text":"<p>Data-driven speech processing models often require transcription data which is expensive to collect. In this paper, the authors propose SpeechCLIP, a framework that connects speech and text through images to enhance speech models without transcriptions. They align state-of-the-art pre-trained HuBERT and CLIP models using paired images and spoken captions with minimal fine-tuning. The proposed model outperforms prior state-of-the-art on image-speech retrieval and performs zero-shot speech-text retrieval without direct supervision from transcriptions. The model can extract semantically related keywords directly from speech. The authors also discuss the benefits of visually grounded speech models (VGS), which utilize paired image-speech data to enhance speech processing. They show that VGS models trained with retrieval objectives can extract semantic and word-level information from speech."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Streaming_end-to-end_speech_recognition_for_mobile_devices/","title":"Streaming end-to-end speech recognition for mobile devices","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Streaming_end-to-end_speech_recognition_for_mobile_devices/#summary","title":"Summary","text":"<p>The paper presents a novel approach for end-to-end speech recognition using a recurrent neural network transducer with layer normalization, large batch size, word-piece targets, time reduction and quantization. Synthetic data is used to train the model with better accuracy, while shallow fusion is used for bias towards user-specific context. The proposed approach outperforms the conventional CTC-based model in terms of latency and accuracy in voice search and dictation."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Streaming_end-to-end_speech_recognition_for_mobile_devices/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Streaming_end-to-end_speech_recognition_for_mobile_devices/#content","title":"Content","text":"<p>End-to-end models are promising for on-device speech recognition, but require streaming decoding, robustness to diverse use cases, and user-specific context in addition to high accuracy. We present a recurrent neural network transducer-based end-to-end speech recognizer with layer normalization, large batch size, word-piece targets, time reduction and quantization. Shallow fusion is also used for bias towards user-specific context. Training the model on synthetic data generated using a text-to-speech system addresses the model's inability to accurately model numeric sequences. Results show that our proposed approach outperforms a conventional CTC-based model in terms of latency and accuracy in several evaluation categories, including voice search and dictation."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Sub-8-bit_quantization_aware_training_for_8-bit_neural_network_accelerator_with_on-device_speech_recognition/","title":"Sub-8-bit quantization aware training for 8-bit neural network accelerator with on-device speech recognition","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Sub-8-bit_quantization_aware_training_for_8-bit_neural_network_accelerator_with_on-device_speech_recognition/#summary","title":"Summary","text":"<p>Summary: The paper proposes a sub-8-bit quantization-aware training scheme called S8BQAT for 8-bit neural network accelerators. The scheme uses quantization centroids derived from a 32-bit baseline and a Multi-Regional Absolute Cosine regularizer to augment training loss. Additionally, the paper introduces a hard compressor to improve convergence rate. The proposed scheme was applied on speech recognition tasks using RNN-T architecture and resulted in a reduction of word error rate by 4-16% while improving latency by 5%.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Sub-8-bit_quantization_aware_training_for_8-bit_neural_network_accelerator_with_on-device_speech_recognition/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Sub-8-bit_quantization_aware_training_for_8-bit_neural_network_accelerator_with_on-device_speech_recognition/#content","title":"Content","text":"<p> We present a novel sub-8-bit quantization-aware training (S8BQAT) scheme for 8-bit neural network accelerators. With the quantization centroids derived from a 32-bit baseline, we augment training loss with a Multi-Regional Absolute Cosine (MRACos) regularizer that aggregates weights towards their nearest centroid, effectively acting as a pseudo compressor. Additionally, a periodically invoked hard compressor is introduced to improve the convergence rate by emulating runtime model weight quantization. We apply S8BQAT on speech recognition tasks using Recurrent Neural Network-Transducer (RNN-T) architecture. With S8BQAT, we are able to increase the model parameter size to reduce the word error rate by 4-16% relatively, while still improving latency by 5%."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/TGAVC%3A_Improving_autoencoder_voice_conversion_with_text-guided_and_adversarial_training/","title":"TGAVC: Improving autoencoder voice conversion with text-guided and adversarial training","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/TGAVC%3A_Improving_autoencoder_voice_conversion_with_text-guided_and_adversarial_training/#summary","title":"Summary","text":"<p>The paper proposes a new framework called TextGuided AutoVC (TGA VC) for non-parallel many-to-many voice conversion. The proposed framework uses an expected content embedding based on text transcriptions to guide the extraction of voice content and adversarial training to eliminate the speaker identity information from estimated content embedding. The content encoder is trained to extract speaker-independent content embedding from speech. Experimental results on the AIShell-3 dataset show that the TGA VC outperforms AutoVC in terms of naturalness and similarity of converted speech."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/TGAVC%3A_Improving_autoencoder_voice_conversion_with_text-guided_and_adversarial_training/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/TGAVC%3A_Improving_autoencoder_voice_conversion_with_text-guided_and_adversarial_training/#content","title":"Content","text":"<p>Non-parallel many-to-many voice conversion remains an interesting but challenging speech processing task. Recently, AutoVC, a conditional autoencoder based method, achieved excellent conversion results by disentangling the speaker identity and the speech content using information-constraining bottlenecks. In this paper, a novel voice conversion framework, named TextGuided AutoVC(TGA VC), is proposed to more effectively separate content and timbre from speech, where an expected content embedding produced based on the text transcriptions is designed to guide the extraction of voice content. In addition, the adversarial training is applied to eliminate the speaker identity information in the estimated content embedding extracted from speech. Under the guidance of the expected content embedding and the adversarial training, the content encoder is trained to extract speaker-independent content embedding from speech. Experiments on AIShell-3 dataset show that the proposed model outperforms AutoVC in terms of naturalness and similarity of converted speech."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Towards_improved_zero-shot_voice_conversion_with_conditional_dsvae/","title":"Towards improved zero-shot voice conversion with conditional dsvae","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Towards_improved_zero-shot_voice_conversion_with_conditional_dsvae/#summary","title":"Summary","text":"<p>Summary: The paper proposes an enhanced framework for zero-shot non-parallel voice conversion utilizing disentangled sequential variational autoencoder (DSV AE) for information decomposition. The authors suggest that disentangling content and speaking style is crucial for zero-shot non-parallel voice conversion. The proposed conditional DSV AE (C-DSV AE) model uses content bias as a condition to reshape the content embedding for better content embeddings with more phonetic information, resulting in better phoneme classification accuracy, stabilized vocalization, and improved zero-shot VC performance compared to the DSV AE baseline.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Towards_improved_zero-shot_voice_conversion_with_conditional_dsvae/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Towards_improved_zero-shot_voice_conversion_with_conditional_dsvae/#content","title":"Content","text":"<p> The paper proposes an improved zero-shot non-parallel voice conversion framework with disentangled sequential variational autoencoder (DSV AE) as the backbone for information decomposition. The authors suggest that the disentangling of content and speaking style information is essential for zero-shot non-parallel voice conversion. The proposed conditional DSV AE (C-DSV AE) model enables the use of content bias as a condition to the prior modeling and reshapes the content embedding sampled from the posterior distribution for a better content embedding with more phonetic information preserved. The paper demonstrates that content embeddings derived from the C-DSV AE overcome the randomness and achieve a much better phoneme classification accuracy, a stabilized vocalization, and a better zero-shot VC performance compared to the competitive DSV AE baseline."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Towards_unsupervised_phone_and_word_segmentation_using_self-supervised_vector-quantized_neural_networks/","title":"Towards unsupervised phone and word segmentation using self-supervised vector-quantized neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Towards_unsupervised_phone_and_word_segmentation_using_self-supervised_vector-quantized_neural_networks/#summary","title":"Summary","text":"<p> The paper introduces a method for segmenting and clustering speech into low-bitrate phone-like sequences without supervision using pretrained self-supervised vector-quantized (VQ) neural networks. The method assigns blocks of contiguous feature vectors to the same code, resulting in variable-rate segmentation of speech into discrete units for unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized dynamic programming method performs the best. The proposed method outperforms the state-of-the-art in some cases at a substantially lower bitrate."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Towards_unsupervised_phone_and_word_segmentation_using_self-supervised_vector-quantized_neural_networks/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Towards_unsupervised_phone_and_word_segmentation_using_self-supervised_vector-quantized_neural_networks/#content","title":"Content","text":"<p> We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We speci\ufb01cally con- strain pretrained self-supervised vector-quantized (VQ) neural networks so that blocks of contiguous feature vectors are as- signed to the same code, thereby giving a variable-rate segmenta- tion of the speech into discrete units. Two segmentation methods are considered. In the \ufb01rst, features are greedily merged until a prespeci\ufb01ed number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The pe- nalized dynamic programming method generally performs best. While performance on individual tasks is only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Training_robust_zero-shot_voice_conversion_models_with_self-supervised_features/","title":"Training robust zero-shot voice conversion models with self-supervised features","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Training_robust_zero-shot_voice_conversion_models_with_self-supervised_features/#summary","title":"Summary","text":"<p>Summary: The paper proposes an unsupervised zero-shot voice conversion model with a length resampling decoder that works with length-incompatible input and output features. It is trained on pairs of segments from the same utterance and adds a cycle-consistency loss following joint training with a speaker encoder and classification loss to improve audio quality. The proposed model shows significant performance improvement on VCTK and LibriTTS datasets.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Training_robust_zero-shot_voice_conversion_models_with_self-supervised_features/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Training_robust_zero-shot_voice_conversion_models_with_self-supervised_features/#content","title":"Content","text":"<p>Unsupervised Zero-Shot Voice Conversion (VC) aims to modify the speaker characteristic of an utterance to match an unseen target speaker without relying on parallel training data. In this paper, we propose a VC model with a length resampling decoder that works with length-incompatible input and output features, allowing the self-supervised acoustic model and the vocoder to be trained on their task-optimal speech features. We also demonstrate that training on pairs of segments from the same utterance and adding a cycle-consistency loss can help to learn a lower variant speaker embedding, which leads to higher audio quality. Moreover, we show that training a speaker encoder jointly with a speaker classification loss produces utterances with better-transferred voice characteristics. We combine these training techniques and demonstrate performance improvement in terms of objective and subjective evaluation, on the VCTK dataset and the LibriTTS dataset."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Transferring_neural_speech_waveform_synthesizers_to_musical_instrument_sounds_generation/","title":"Transferring neural speech waveform synthesizers to musical instrument sounds generation","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Transferring_neural_speech_waveform_synthesizers_to_musical_instrument_sounds_generation/#summary","title":"Summary","text":"<p>Summary: The study explores the use of three neural speech synthesizers in generating musical instrument sounds trained under three scenarios and compared their performance. Pre-training speech synthesizers on speech data, and fine-tuning on music data significantly improves their performance. The results of the perceptual test prove that WaveGlow is best for zero-shot learning, while NSF is most suitable for other scenarios and generates samples similar to natural audio.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Transferring_neural_speech_waveform_synthesizers_to_musical_instrument_sounds_generation/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Transferring_neural_speech_waveform_synthesizers_to_musical_instrument_sounds_generation/#content","title":"Content","text":"<p> Recent neural waveform synthesizers like WaveNet, WaveGlow and the neural-source-filter model have shown impressive performance in speech synthesis. This study aims to explore a similar approach in music production. Three neural speech synthesizers are compared for generating musical instruments sounds, and they are trained under three different scenarios: (1) training on only music data, (2) zero-shot learning from the speech domain and (3) fine-tuning-based adaptation from the speech to the music domain. The results of a perceptual test prove that pre-training the speech synthesizers on speech data, and fine-tuning them on music data significantly improves their performance. The study concludes that WaveGlow performs best in zero-shot learning, while NSF is most suitable for the other scenarios and generates samples close to natural audio."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vector-quantized_neural_networks_for_acoustic_unit_discovery_in_the_zerospeech_2020_challenge/","title":"Vector-quantized neural networks for acoustic unit discovery in the zerospeech 2020 challenge","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vector-quantized_neural_networks_for_acoustic_unit_discovery_in_the_zerospeech_2020_challenge/#summary","title":"Summary","text":"<p>Summary: The paper proposes two neural models to discover acoustic units from unlabelled speech data. These models use vector quantization to map continuous features to a finite set of codes. The first model is a type of vector-quantized variational autoencoder(VQ-VAE) and the second model combines vector quantization with contrastive predictive coding (VQ-CPC). Both models outperformed all submissions to the 2019 and 2020 challenges for ABX phone discrimination tests with a relative improvement of more than 30%. VQ-CPC performs slightly better and is simpler and faster to train than VQ-VAE. The paper concludes that vector quantization is an effective bottleneck that forces the models to discard speaker information.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vector-quantized_neural_networks_for_acoustic_unit_discovery_in_the_zerospeech_2020_challenge/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vector-quantized_neural_networks_for_acoustic_unit_discovery_in_the_zerospeech_2020_challenge/#content","title":"Content","text":"<p>In this paper, we explore vector quantization for acoustic unit discovery. Leveraging unlabelled data, we aim to learn discrete representations of speech that separate phonetic content from speaker-specific details. We propose two neural models to tackle this challenge \u2013 both use vector quantization to map continuous features to a finite set of codes. The first model is a type of vector-quantized variational autoencoder (VQ-VAE). The VQ-VAE encodes speech into a sequence of discrete units before reconstructing the audio waveform. Our second model combines vector quantization with contrastive predictive coding (VQ-CPC). The idea is to learn a representation of speech by predicting future acoustic units. We evaluate the models on English and Indonesian data for the ZeroSpeech 2020 challenge. In ABX phone discrimination tests, both models outperform all submissions to the 2019 and 2020 challenges, with a relative improvement of more than 30%. The models also perform competitively on a downstream voice conversion task. Of the two, VQ-CPC performs slightly better in general and is simpler and faster to train. Finally, probing experiments show that vector quantization is an effective bottleneck, forcing the models to discard speaker information."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vqmivc%3A_Vector_quantization_and_mutual_information-based_unsupervised_speech_representation_disentanglement_for_one-shot_voice_conversion/","title":"Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vqmivc%3A_Vector_quantization_and_mutual_information-based_unsupervised_speech_representation_disentanglement_for_one-shot_voice_conversion/#summary","title":"Summary","text":"<p>The paper introduces a one-shot voice conversion approach that uses vector quantization and mutual information-based VC (VQMIVC) to reduce inter-dependencies of speech representations. VQCPC and MI are applied during training to achieve SRD. Experimental results show that VQMIVC outperforms current state-of-the-art one-shot VC systems by retaining source content and intonation variations, while capturing target speaker characteristics, leading to higher speech naturalness and speaker similarity."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vqmivc%3A_Vector_quantization_and_mutual_information-based_unsupervised_speech_representation_disentanglement_for_one-shot_voice_conversion/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vqmivc%3A_Vector_quantization_and_mutual_information-based_unsupervised_speech_representation_disentanglement_for_one-shot_voice_conversion/#content","title":"Content","text":"<p>One-shot voice conversion (VC) can disentangle speech representations to perform conversion across arbitrary speakers with only a single target-speaker utterance for reference. The vector quantization and mutual information-based VC (VQMIVC) approach reduces the inter-dependencies of content, speaker and pitch representations in an unsupervised manner. VQCPC and MI are used during training to achieve SRD. Results show that VQMIVC retains source linguistic content and intonation variations, while capturing target speaker characteristics, achieving higher speech naturalness and speaker similarity than current state-of-the-art one-shot VC systems."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vqvc%2B%3A_One-shot_voice_conversion_by_vector_quantization_and_u-net_architecture/","title":"Vqvc+: One-shot voice conversion by vector quantization and u-net architecture","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vqvc%2B%3A_One-shot_voice_conversion_by_vector_quantization_and_u-net_architecture/#summary","title":"Summary","text":"<p>Summary: The paper proposes a Voice Conversion (VC) method called VQVC+ which combines a Vector Quantization (VQ) based method with a U-Net architecture to address the issue of imperfect disentanglement of speaker and content in input speech during VC. The VQ serves as an information bottleneck to prevent overfitting, leading to excellent results in terms of audio naturalness and speaker similarity as compared to existing methods such as AutoVC and Chou.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vqvc%2B%3A_One-shot_voice_conversion_by_vector_quantization_and_u-net_architecture/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Vqvc%2B%3A_One-shot_voice_conversion_by_vector_quantization_and_u-net_architecture/#content","title":"Content","text":"<p> Voice Conversion (VC) is a challenging task that transforms the voice of the source speaker while retaining the linguistic information. The disentangling of the speaker and content in input speech is carried out by vector quantization (VQ), adversarial training, or instance normalization (IN). However, the output speech quality is negatively affected due to imperfect disentanglement. To address this issue, the VQ-based method is combined with a U-Net architecture entitled VQVC+ in the proposed method. The VQ-based method serves as a strong information bottleneck that prevents overfitting. The proposed method shows excellent performance in both audio naturalness and speaker similarity over AutoVC and Chou by subjective evaluations."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Zero-shot_long-form_voice_cloning_with_dynamic_convolution_attention/","title":"Zero-shot long-form voice cloning with dynamic convolution attention","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Zero-shot_long-form_voice_cloning_with_dynamic_convolution_attention/#summary","title":"Summary","text":"<p>This paper proposes an attention-based text-to-speech system that can reproduce a target voice from just a few seconds of reference speech and generalize to long utterances. It uses an energy-based attention mechanism for generalization and conditional pretraining on a large corpus of diverse data to achieve effective zero-shot speaker adaptation. The proposed model outperforms several implementations of voice cloning systems in terms of speech naturalness, speaker similarity, alignment consistency, and ability to synthesize long utterances."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Zero-shot_long-form_voice_cloning_with_dynamic_convolution_attention/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/Zero-shot_long-form_voice_cloning_with_dynamic_convolution_attention/#content","title":"Content","text":"<p>With recent advancements in voice cloning, the performance of speech synthesis for a target speaker has been rendered similar to the human level. However, autoregressive voice cloning systems still suffer from text alignment failures, resulting in an inability to synthesize long sentences. In this work, we propose a variant of attention-based text-to-speech system that can reproduce a target voice from a few seconds of reference speech and generalize to very long utterances as well. The proposed system is based on three independently trained components: a speaker encoder, synthesizer and universal vocoder. Generalization to long utterances is realized using an energy-based attention mechanism known as Dynamic Convolution Attention, in combination with a set of modifications proposed for the synthesizer based on Tacotron 2. Moreover, effective zero-shot speaker adaptation is achieved by conditioning both the synthesizer and vocoder on a speaker encoder that has been pretrained on a large corpus of diverse data. We compare several implementations of voice cloning systems in terms of speech naturalness, speaker similarity, alignment consistency and ability to synthesize long utterances, and conclude that the proposed model can produce intelligible synthetic speech for extremely long utterances, while preserving a high extent of naturalness and similarity for short texts."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/%5BPDF%5D%5BPDF%5D_Small-footprint_acoustic_scene_classification_through_8-bit_quantization-aware_training_and_pruning_of_ResNet_models/","title":"[PDF][PDF] Small-footprint acoustic scene classification through 8-bit quantization-aware training and pruning of ResNet models","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/%5BPDF%5D%5BPDF%5D_Small-footprint_acoustic_scene_classification_through_8-bit_quantization-aware_training_and_pruning_of_ResNet_models/#summary","title":"Summary","text":"<p>Summary: This report discusses the IDLab submissions for Task 1a of the DCASE Challenge 2021, focused on creating an acoustic scene classification model of size under 128 KB. Techniques such as ResNet with SE blocks, temporal cropping, time domain mixup, and speed-change augmentation were used. Parameter reduction techniques were utilized, and the model achieved an accuracy of 71.2% on the standard test set of the TAU Urban Acoustic Scenes 2020 Mobile development dataset.</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/%5BPDF%5D%5BPDF%5D_Small-footprint_acoustic_scene_classification_through_8-bit_quantization-aware_training_and_pruning_of_ResNet_models/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/%5BPDF%5D%5BPDF%5D_Small-footprint_acoustic_scene_classification_through_8-bit_quantization-aware_training_and_pruning_of_ResNet_models/#content","title":"Content","text":"<p> This report describes the IDLab submissions for Task 1a of the DCASE Challenge 2021, focused on constructing an acoustic scene classification model with a size of less than 128 KB. The submitted systems consisted of a ResNet based model enhanced with Squeeze-and-Excitation (SE) blocks trained with temporal cropping, time domain mixup and speed-change augmentation strategies. Parameter reduction techniques were explored, such as weight quantization, grouped convolutions and pruning, as well as prediction score post-processing that directly optimizes the log loss criterion through logistic regression-based calibration. The uncalibrated 8-bit model achieved a log loss of 0.82 and an accuracy of 71.2% on the standard test set of the TAU Urban Acoustic Scenes 2020 Mobile development dataset."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/%5BPDF%5D%5BPDF%5D_Wavelet_transform_speech_recognition_using_vector_quantization%2C_dynamic_time_warping_and_artificial_neural_networks/","title":"[PDF][PDF] Wavelet transform speech recognition using vector quantization, dynamic time warping and artificial neural networks","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/%5BPDF%5D%5BPDF%5D_Wavelet_transform_speech_recognition_using_vector_quantization%2C_dynamic_time_warping_and_artificial_neural_networks/#summary","title":"Summary","text":"<p> This paper explores the use of Discrete Wavelet Transform (DWT) along with Dynamic Time Warping (DTW), Vector Quantization (VQ) and Artificial Neural Networks (ANN) for recognizing isolated words for a speaker-dependent system. It concludes that DWT can be an effective feature extraction tool for speech recognition as it can distribute specific features of the signal into different frequency bands without assuming any model for input speech signal. VQ and DTW with wavelets performed well, but the results for ANN were less promising because it is sensitive to time alignment and frame synchronization."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/%5BPDF%5D%5BPDF%5D_Wavelet_transform_speech_recognition_using_vector_quantization%2C_dynamic_time_warping_and_artificial_neural_networks/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/%5BPDF%5D%5BPDF%5D_Wavelet_transform_speech_recognition_using_vector_quantization%2C_dynamic_time_warping_and_artificial_neural_networks/#content","title":"Content","text":"<p>In this paper we investigate the performance of the Discrete Wavelet Transform (DWT) with Dynamic Time Warping, Vector Quantization and Artificial Neural Networks for speaker-dependent, isolated word recognition. Our experiments and simulation results indicate that the DWT is a potential contender to be a feature extraction tool for speech recognition. While vector quantization and dynamic time-warping with wavelets have yielded results comparable to those obtained with LPC, preliminary results suggest that the neural networks are sensitive to time alignment and frame synchronism. The Wavelet Transform is an elegant tool for the analysis of non-stationary signals like speech, distributing specific features of the signal into different frequency bands. It does not assume any model for the input speech signal and may be implemented as a fast, pyramidal algorithm."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/u-HuBERT%3A_Unified_Mixed-Modal_Speech_Pretraining_And_Zero-Shot_Transfer_to_Unlabeled_Modality/","title":"u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality","text":""},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/u-HuBERT%3A_Unified_Mixed-Modal_Speech_Pretraining_And_Zero-Shot_Transfer_to_Unlabeled_Modality/#summary","title":"Summary","text":"<p>The paper introduces u-HuBERT, a pre-training framework that uses both multimodal and unimodal speech with a common masked cluster prediction objective. The paper demonstrates that a single model with modality dropout during pre-training can perform better or similar to state-of-the-art modality-specific models. The fine-tuned model can also generalize well to different speech processing tasks, achieving zero-shot modality generalization for multiple speech processing tasks."},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/u-HuBERT%3A_Unified_Mixed-Modal_Speech_Pretraining_And_Zero-Shot_Transfer_to_Unlabeled_Modality/#target-task","title":"Target Task","text":"<p>speech recognition</p>"},{"location":"Quantization-Survey-Paper-Notes/speech_recognition/u-HuBERT%3A_Unified_Mixed-Modal_Speech_Pretraining_And_Zero-Shot_Transfer_to_Unlabeled_Modality/#content","title":"Content","text":"<p>In this paper, the authors present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, they demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, their model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks."}]}