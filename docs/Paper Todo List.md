
# Pre-Transformer
TODO


# Transformer 

## Pre-LLM:
TODO 

### Next: 
1. I-BERT: Integer-only BERT Quantization https://arxiv.org/pdf/2101.01321.pdf
2. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT https://ojs.aaai.org/index.php/AAAI/article/view/6409
3. Efficient 8-Bit Quantization of Transformer Neural Machine Language Translation Model https://arxiv.org/pdf/1906.00532.pdf


## LLM: 

### Finished:
1. Survery of PTQ for LLM (Microsoft): https://arxiv.org/pdf/2303.08302.pdf 
2. GPTQ https://arxiv.org/pdf/2210.17323.pdf
3. ZeroQuant https://arxiv.org/pdf/2206.01861.pdf
4. LLM.int8() https://arxiv.org/pdf/2208.07339.pdf
5. RPTQ https://arxiv.org/pdf/2304.01089.pdf
6. SmoothQuant https://arxiv.org/pdf/2211.10438.pdf

### Next: 
1. Intriguing Properties of Quantization at Scale (Cohere): publish yesterday https://arxiv.org/pdf/2305.19268v1.pdf

QAT: 
1. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models https://arxiv.org/pdf/2305.17888.pdf

Finetuning: 
1. QLORA: Efficient Finetuning of Quantized LLMs:  https://arxiv.org/pdf/2305.14314.pdf