# Tent: Efficient quantization of neural networks on the tiny edge with tapered fixed point

## Summary

Summary: The paper presents a new low-precision framework, TENT, for TinyML models. The authors propose a tapered fixed-point quantization algorithm that matches the numerical format’s dynamic range and distribution to that of the deep neural network model at each layer. The accuracy on classification tasks improves up to ≈31%, with an energy overhead of ≈17-30% as compared to fixed-point, for ConvNet and ResNet-18 models.


## Target Task

computer vision

## Content

<Abstract: >In this research, we propose a new low-precision framework, TENT, to leverage the benefits of a tapered fixed-point numerical format in TinyML models. We introduce a tapered fixed-point quantization algorithm that matches the numerical format’s dynamic range and distribution to that of the deep neural network model’s parameter distribution at each layer. Results show that the accuracy on classification tasks improves up to ≈31% with an energy overhead of ≈17-30% as compared to fixed-point, for ConvNet and ResNet-18 models.



---

