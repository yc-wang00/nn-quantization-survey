# Quantized neural networks: Characterization and holistic optimization

## Summary

Summary: This paper proposes a holistic approach for the optimization of quantized deep neural networks (QDNNs) that includes training methods and quantization-friendly architecture design. Results suggest that wider models are better suited for weight and activation quantization while deeper models are more susceptible to activation quantization. This study can aid in the advancement of QDNN optimization techniques.


## Target Task

computer vision

## Content

Abstract: Quantized deep neural networks (QDNNs) are necessary for low-power, high throughput, and embedded applications. This study proposes a holistic approach for the optimization of QDNNs, which contains QDNN training methods as well as quantization-friendly architecture design. The results indicate that deeper models are more prone to activation quantization, while wider models improve the resiliency to both weight and activation quantization. This study can provide insight into better optimization of QDNNs.



---

