# SDQ: Stochastic Differentiable Quantization with Mixed Precision

## Summary

<Summary: >The paper discusses a Stochastic Differentiable Quantization method (SDQ) that can automatically learn the Mixed Precision Quantization (MPQ) strategy in a more flexible and globally optimized space with a smoother gradient approximation. The approach outperforms all state-of-the-art mixed or single precision quantization with a lower bitwidth, even better than the full-precision counterparts across various ResNet and MobileNet families.


## Target Task

computer vision

## Content

<Abstract: >In order to efficiently deploy deep models, model quantization approaches have been used. Mixed precision quantization (MPQ) has recently been researched, but previous studies mainly search the MPQ strategy in a costly scheme, or utilize partial prior knowledge for bitwidth assignment. This work presents a Stochastic Differentiable Quantization (SDQ) method that automatically learns the MPQ strategy in a more flexible and globally-optimized space with smoother gradient approximation. Extensive evaluations have shown that SDQ outperforms all state-of-the-art mixed or single precision quantization with a lower bitwidth and is even better than the full-precision counterparts across various ResNet and MobileNet families, demonstrating its effectiveness and superiority.



---

