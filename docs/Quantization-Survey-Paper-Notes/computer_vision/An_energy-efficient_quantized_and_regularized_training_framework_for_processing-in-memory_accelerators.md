# An energy-efficient quantized and regularized training framework for processing-in-memory accelerators

## Summary

Summary: The paper proposes an energy-efficient quantized and regularized training framework for PIM accelerators, which involves a PIM-based non-uniform activation quantization scheme and an energy-aware weight regularization method. The proposed framework can improve energy efficiency by reducing ADC resolution requirements and training low energy consumption CNN models for PIM without much accuracy loss. Experimental results show that the proposed framework can reduce the resolution of ADCs by 2 bits and computing energy consumption in the analog domain by 35%.


## Target Task

computer vision

## Content

<Abstract: >Convolutional Neural Networks (CNNs) have made breakthroughs in various fields, while the energy consumption becomes enormous. Processing-In-Memory (PIM) architectures based on emerging non-volatile memory have demonstrated great potential in improving the energy efficiency of CNN computing. In this paper, we propose an energy-efﬁcient quantized and regularized training framework for PIM accelerators, which consists of a PIM-based non-uniform activation quantization scheme and an energy-aware weight regularization method. The proposed framework can improve the energy efﬁciency of PIM architectures by reducing the Analog-to-Digital Converters (ADCs) resolution requirements and training low energy consumption CNN models for PIM, with little accuracy loss. The experimental results show that the proposed training framework can reduce the resolution of ADCs by 2bits and the computing energy consumption in the analog domain by 35%. The energy efficiency can be enhanced by 3.4× in our proposed training framework.



---

