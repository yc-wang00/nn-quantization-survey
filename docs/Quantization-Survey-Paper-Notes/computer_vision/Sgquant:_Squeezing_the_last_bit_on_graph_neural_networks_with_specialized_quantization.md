# Sgquant: Squeezing the last bit on graph neural networks with specialized quantization

## Summary

<Summary: > The paper proposes a specialized Graph Neural Network (GNN) quantization scheme called SGQuant to systematically reduce the GNN memory consumption. SGQuant can effectively reduce the memory footprint while limiting the accuracy drop to 0.4% on average.


## Target Task

computer vision

## Content

<Abstract: >With the increasing popularity of graph-based learning, Graph Neural Networks (GNNs) win lots of attention from research and industry field because of their high accuracy. However, existing GNNs suffer from high memory footprints (e.g., node embedding features). To this end, we propose a specialized GNN quantization scheme, SGQuant, to systematically reduce the GNN memory consumption. Intensive experiments show that SGQuant can effectively reduce the memory footprint from 4.25 to 31.9 compared with the original full-precision GNNs while limiting the accuracy drop to 0.4% on average.



---

