# [PDF][PDF] Soft-to-hard vector quantization for end-to-end learned compression of images and neural networks

## Summary

<Summary: >The paper proposes a novel approach for learning compressible representations in deep architectures through the use of a soft relaxation of quantization and entropy, which is eventually annealed to their discrete counterparts during training. The method is demonstrated to be effective in two applications, namely image compression and neural network compression, surpassing existing techniques.


## Target Task

computer vision

## Content

<Abstract: >In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.



---

