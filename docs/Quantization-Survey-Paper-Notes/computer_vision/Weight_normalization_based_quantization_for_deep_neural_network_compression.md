# Weight normalization based quantization for deep neural network compression

## Summary

<Summary:> 
The paper proposes a new method called weight normalization based quantization (WNQ) for deep learning model compression. WNQ uses weight normalization to reduce the quantization error and improve performance. The experiments conducted on CIFAR-100 and ImageNet datasets show that WNQ outperforms other baseline methods and achieves state-of-the-art performance.


## Target Task

computer vision

## Content

<Abstract: >In this paper, we propose a novel quantization method, called weight normalization based quantization (WNQ), for model compression. WNQ adopts weight normalization to avoid the long-tail distribution of network weights and subsequently reduces the quantization error. Experiments on CIFAR-100 and ImageNet show that WNQ can outperform other baselines to achieve state-of-the-art performance.



---

