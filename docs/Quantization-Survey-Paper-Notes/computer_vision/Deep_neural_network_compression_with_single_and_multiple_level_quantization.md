# Deep neural network compression with single and multiple level quantization

## Summary

Summary: The paper proposes two new network quantization methods, SLQ and MLQ, for high-bit and extremely low-bit quantization. These approaches consider network quantization from both the width and depth level and demonstrated impressive results through experiments using state-of-the-art neural networks including AlexNet, VGG-16, GoogleNet, and ResNet-18. Network compression is crucial and is an effective solution for decreasing storage and computation costs for DNN models.


## Target Task

computer vision

## Content

<Abstract:>
In this paper, the authors propose two novel network quantization approaches, single-level network quantization (SLQ) for high-bit quantization and multi-level network quantization (MLQ) for extremely low-bit quantization (ternary). They are the first to consider the network quantization from both width and depth level. The proposed approaches are validated with extensive experiments based on the state-of-the-art neural networks including AlexNet, VGG-16, GoogleNet and ResNet-18. Both SLQ and MLQ achieve impressive results. Network compression is critical and has become an effective solution to reduce the storage and computation costs for DNN models.



---

