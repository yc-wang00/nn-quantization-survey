# Structured compression by weight encryption for unstructured pruning and quantization

## Summary

<Summary: >The paper presents a new weight representation scheme for Sparse Quantized Neural Networks, which uses a fine-grained and unstructured pruning method for structured regular format during inference. The proposed format has high compression ratio without compromising the model accuracy and allows for performance enhancement on devices with irregular representations. The paper also discusses the challenges of achieving performance enhancement without an inherently parallel sparse-matrix decoding process during inference.


## Target Task

computer vision

## Content

<Abstract: >This paper proposes a new weight representation scheme for Sparse Quantized Neural Networks using a fine-grained and unstructured pruning method, which achieves a structured regular format for efficient decoding during inference through an XOR-gate network. The proposed format shows high compression ratio for various deep learning models without compromising on the model accuracy, enabling performance enhancement on devices with irregular representations of sparse matrix formats. This paper also discusses the challenges of achieving performance enhancement without an inherently parallel sparse-matrix decoding process during inference.



---

