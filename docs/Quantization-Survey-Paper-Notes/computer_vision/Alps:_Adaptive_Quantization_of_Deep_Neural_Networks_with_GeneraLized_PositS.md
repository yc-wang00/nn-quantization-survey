# Alps: Adaptive Quantization of Deep Neural Networks with GeneraLized PositS

## Summary

<Summary: > The paper presents a new adaptive quantization algorithm using generalized posit format to represent dynamic range and distribution of deep neural network parameters. The algorithm is achieved by minimizing the intra-layer posit quantization error using a compander. The efficacy of the proposed quantization algorithm is studied on ResNet-50 and EfficientNet models for classification tasks within a new low-precision framework called ALPS. Results indicate that the new algorithm outperforms other numerical formats, including standard posits, in terms of accuracy and energy dissipation.


## Target Task

computer vision

## Content

<Abstract: >In this paper, a new adaptive quantization algorithm for generalized posit format is presented, to optimally represent the dynamic range and distribution of deep neural network parameters. Adaptation is achieved by minimizing the intra-layer posit quantization error with a compander. The efficacy of the proposed quantization algorithm is studied within a new low-precision framework, ALPS, on ResNet-50 and EfficientNet models for classification tasks. Results assert that the accuracy and energy dissipation of low-precision DNNs using generalized posits outperform other well-known numerical formats, including standard posits.



---

