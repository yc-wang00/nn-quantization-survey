# Same, same but different: Recovering neural network quantization error through weight factorization

## Summary

Summary: The paper proposes a new method for improving the performance of quantized neural networks by utilizing a degree of freedom of scaling output channels. The approach significantly decreases the degradation caused by quantization and achieves state-of-the-art results for MobileNets. The authors also explore equivalent weight arrangements to improve network interpretability, network-pruning, and neural nets regularization.


## Target Task

computer vision

## Content

<Abstract: >
Quantization of neural networks is essential for implementing deep neural networks on embedded devices. In this paper, the authors propose a novel approach for improving the performance of a quantized network while utilizing an overlooked degree of freedom of scaling output channels. They present a simple and easy-to-implement method that significantly decreases the degradation caused by quantization and achieve state-of-the-art degradation results for MobileNets. Furthermore, the authors explore equivalent weight arrangements that make the network less sensitive to quantization to improve network interpretability, network-pruning, neural nets regularization, and other domains.



---

