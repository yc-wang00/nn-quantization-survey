# Quantized neural network design under weight capacity constraint

## Summary

<Summary: >The paper evaluates the effect of changing the network complexity and word-length of weights on the performance of fully-connected deep neural networks and convolutional neural networks. The authors propose a metric called effective compression ratio (ECR) to compare the complexity of floating-point and fixed-point networks with the same performance. The goal is to provide a guide for determining network size and word-length for efficient hardware implementation of DNNs.


## Target Task

computer vision

## Content

<Abstract: >The complexity of deep neural network algorithms can be lowered either by scaling the number of units or reducing the word-length of weights. In this paper, the performances of fully-connected deep neural networks (FCDNNs) and convolutional neural networks (CNNs) are evaluated while changing the network complexity and the word-length of weights. Based on the experiments, the authors propose a metric called the effective compression ratio (ECR) that compares the complexity of floating-point and fixed-point networks showing the same performance. This analysis intends to provide a guideline to network size and word-length determination for efficient hardware implementation of deep neural networks (DNN).



---

