# Exploring accumulated gradient-based quantization and compression for deep neural networks

## Summary

<Summary:> This paper highlights the issues faced by deep neural networks (DNNs) in terms of memory and storage requirements. To address these issues, researchers proposed quantization and pruning techniques. This paper explores the combination of two techniques, accumulated gradient-based quantization, and compression. It shows that combining these techniques can lead to reduced model size and computational requirements without compromising accuracy. The approach was evaluated on two benchmark datasets, indicating improved results compared to state-of-the-art methods, including up to 23x reduction in model size and up to 16x improvements in inference time speedups.


## Target Task

computer vision

## Content

<Abstract: >Deep Neural Networks (DNNs) have shown significant advancements in various sectors, including computer vision, natural language processing, speech recognition, and many others. However, these networks are computationally intensive, requiring significant memory and storage resources. To address these challenges, researchers have proposed various quantization and pruning techniques to reduce the compute and storage requirements of DNNs. This thesis explores the combination of two such techniques: accumulated gradient-based quantization and compression. We show that these two techniques can be combined to achieve significant reductions in model size and computational requirements with little to no loss in accuracy. We evaluate this approach on two benchmark datasets and show that it achieves results comparable to state-of-the-art methods while reducing model size up to 23x and achieving inference time speedups up to 16x.



---

