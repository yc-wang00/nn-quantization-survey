# Resource-aware object classification and segmentation for semi-autonomous grasping with prosthetic hands

## Summary

Summary: The paper presents a visual perception system that extracts scene information for semi-autonomous hand control using electromyographic (EMG) signals captured by two surface electrodes attached to the human body. The system allows for intuitive and effortless control with few simple user commands, reduces the complexity of manual control, and increases classification and segmentation accuracy up to 96.5% and 89.5% respectively. The system runs on an in-hand Arm Cortex-H7 microcontroller at only 400 MHz and captures visual environmental information for real-time evaluation, enabling offline design with online parametrization.


## Target Task

computer vision

## Content

<Abstract: Myoelectric control of prosthetic hands relies on electromyographic (EMG) signals captured by usually two surface electrodes attached to the human body in different setups. In this paper, we present a visual perception system to extract scene information for semi-autonomous hand-control that allows minimizing required command complexity and leads to more intuitive and effortless control. Our evaluation shows classification accuracy of 96.5% and segmentation accuracy of up to 89.5% on an in-hand Arm Cortex-H7 microcontroller running at only 400 MHz. We address the problem of semi-autonomous grasping with hand prosthesis, which should be controlled by a few simple user commands instead of complete manual control which could be complex and increases the cognitive burden for the user, especially for control of multi-DoF hands. The in-hand embedded system captures visual environmental information, that is intended to be evaluated in real-time. The captured visual scene information allows selecting a suitable offline design with possible online parametrization.>



---

