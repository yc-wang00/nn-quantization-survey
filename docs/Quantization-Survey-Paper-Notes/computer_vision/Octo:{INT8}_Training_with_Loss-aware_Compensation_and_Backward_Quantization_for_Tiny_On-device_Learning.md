# Octo:{INT8} Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning

## Summary

<Summary: > The paper proposes a unified scheme called Octo to automatically learn INT8 calibration parameters, compensate for losses caused by quantization and perform backward quantization to train DNN models with INT8 weights and activations. Octo is shown to achieve higher accuracy, lower latency, lower memory usage and faster convergence than state-of-the-art methods while preserving model interpretability. The proposed method aims to enable the deployment of tiny DNN models on edge devices.


## Target Task

computer vision

## Content

<Abstract: >In this paper, we propose Octo, a unified scheme that automatically learns INT8  calibration parameters, compensates for losses caused by quantization, and performs backward quantization to train DNN models with INT8 weights and activations. Our experiments on datasets and networks of various sizes show that Octo can achieve higher accuracy, lower latency, lower memory usage, and faster convergence than state-of-the-art methods while preserving model interpretability. With Octo, we aim to facilitate the deployment of tiny DNN models on edge devices.



---

