# Harmonious coexistence of structured weight pruning and ternarization for deep neural networks

## Summary

<Summary: > This paper proposes a PE-wise structured pruning approach and optimized weight ternarization to compress deep neural networks. The proposed techniques can achieve up to 21 times compression rate with minimal accuracy degradation.


## Target Task

computer vision

## Content

<Abstract: >Deep convolutional neural networks have exhibited tremendous success in computer vision, but their large model sizes and high computing complexity hinder their deployment on resource-limited embedded systems. Weight pruning and quantization have been used to compress DNN models, but combining the two techniques exhibits disharmony, especially when more aggressive compression schemes are applied. This work introduces a PE-wise structured pruning scheme and integrates it with an optimized weight ternarization approach to quantize weights into ternary values. The proposed techniques achieve up to 21 times PE-wise structured compression rate with only a 1.74%/0.94% accuracy degradation of ResNet-18 on the ImageNet dataset.



---

