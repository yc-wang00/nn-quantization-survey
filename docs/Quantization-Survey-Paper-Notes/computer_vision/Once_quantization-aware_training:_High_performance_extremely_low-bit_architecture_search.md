# Once quantization-aware training: High performance extremely low-bit architecture search

## Summary

<Summary:> The paper proposes a joint training method combining Network Architecture Search and quantization with a shared step size to achieve accurate quantization in extremely low-bit cases. The method also introduces a bit-inheritance scheme to transfer quantized models to lower bits, reducing time cost and improving accuracy. State-of-the-art results are achieved compared to various architectures under different bit-widths, and quantization-friendly architectures are identified. The interaction between quantization and neural architectures is analyzed, and the authors release codes and models to the public.


## Target Task

computer vision

## Content

<Abstract:> In this paper, the authors propose a method for combining Network Architecture Search with quantization to enhance quantization accuracy in extremely low-bit cases. They introduce the joint training of architecture and quantization with a shared step size to acquire a large number of quantized models. A bit-inheritance scheme is also introduced to transfer quantized models to lower bits, reducing time cost and improving quantization accuracy. The proposed method achieves state-of-the-art results compared to various architectures under different bit-widths. The authors identify quantization-friendly architectures and analyze the interaction between quantization and neural architectures. Codes and models are released to the public.



---

