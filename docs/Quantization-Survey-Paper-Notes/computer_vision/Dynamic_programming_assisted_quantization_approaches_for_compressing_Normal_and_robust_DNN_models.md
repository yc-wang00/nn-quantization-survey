# Dynamic programming assisted quantization approaches for compressing Normal and robust DNN models

## Summary

Summary: The authors propose two weight quantization approaches, DPR and DPQ, for compressing DNNs using dynamic programming based algorithms. These approaches produce models with higher inference accuracy than recently proposed counterparts while achieving same or larger compression. They are also extended for compressing robust DNNs, achieving significant compression with less than 3% accuracy drop on both natural and adversarial examples.


## Target Task

computer vision

## Content

<Abstract: In this work, the authors present effective quantization approaches for compressing deep neural networks (DNNs). They propose two weight quantization approaches, DPR and DPQ, for compressing normal DNNs using dynamic programming based algorithms. The approaches with regularization and quantization function produce models with higher inference accuracy than recently proposed counterparts while achieving same or larger compression. They are also extended for compressing robust DNNs, and the relevant experiments show 16X compression of the robust ResNet-18 model with less than 3% accuracy drop on both natural and adversarial examples.>



---

