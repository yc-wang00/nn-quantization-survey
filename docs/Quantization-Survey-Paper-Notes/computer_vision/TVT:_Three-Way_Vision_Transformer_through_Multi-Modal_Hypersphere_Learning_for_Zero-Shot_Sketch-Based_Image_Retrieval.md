# TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval

## Summary

Summary: The paper proposes a novel token-based strategy called Three-Way Vision Transformer (TVT) to retrieve natural images related to sketch queries from unseen categories in zero-shot sketch-based image retrieval (ZS-SBIR) task. They integrate three pre-trained Vision Transformers (ViTs) into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. They also propose a distillation process to supervise fusion ViT with soft targets from modality-specific ViTs. The method learns a multi-modal hypersphere to bridge the modal gap between modalities of sketch and image without losing uniformity. The proposed TVT method outperforms other ZS-SBIR methods on three benchmark datasets.


## Target Task

computer vision

## Content

Abstract: 
In this paper, the authors propose a novel approach for zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. They propose a token-based strategy called Three-Way Vision Transformer (TVT) that integrates three pre-trained Vision Transformers (ViTs) into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. They also propose a distillation process to supervise fusion ViT with soft targets from modality-specific ViTs to prevent catastrophic forgetting. Their method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments demonstrate the superiority of their TVT method over the state-of-the-art ZS-SBIR methods on three benchmark datasets.



---

