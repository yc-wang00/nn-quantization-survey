# [PDF][PDF] Towards a deeper understanding of training quantized neural networks

## Summary

<Summary: >This paper investigates the theory of training quantized neural networks using high-precision representations that have an annealing property, which is lacking in purely quantized training methods. The aim is to learn on embedded platforms with limited resources, memory capacity, and power consumption. The authors analyze the convergence properties of commonly used methods, with the main result explaining observed empirical differences between the training algorithms.


## Target Task

computer vision

## Content

<Abstract: >Training neural networks with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. In this work, the authors investigate the theory of training quantized neural networks by analyzing the convergence properties of some commonly used methods. The main result shows that training algorithms that exploit high-precision representations have an important annealing property that purely quantized training methods lack, which explains many of the observed empirical differences between these types of algorithms.



---

