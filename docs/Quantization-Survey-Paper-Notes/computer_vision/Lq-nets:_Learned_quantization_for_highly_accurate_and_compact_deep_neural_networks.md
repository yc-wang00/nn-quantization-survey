# Lq-nets: Learned quantization for highly accurate and compact deep neural networks

## Summary

Summary: The paper proposes to jointly train a quantized DNN and its associated quantizers to address the accuracy gap between quantized and full-precision models, instead of using fixed quantization schemes.


## Target Task

computer vision

## Content

<Abstract: >Although weight and activation quantization is an eﬀective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using ﬁxed, handcrafted quantization schemes such as uniform or logarithmic quantization.



---

