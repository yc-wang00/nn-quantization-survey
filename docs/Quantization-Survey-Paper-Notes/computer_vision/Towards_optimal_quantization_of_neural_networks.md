# Towards optimal quantization of neural networks

## Summary

<Summary: >The paper proposes an approach to quantizing deep networks for use in mobile and in-sensor applications using functional high-rate quantization theory, which leads to an optimal quantizer that is computed using backpropagation algorithm. The paper focuses on fixed-rate scalar quantization of edge weights that can be used directly for inferential computation. The proposed approach provides a theoretical foundation for quantization, and even in cases where technical conditions are not met, a heuristic quantizer can still be computed with certain regularization guarantees.


## Target Task

computer vision

## Content

<Abstract: >Due to the need for compressing deep networks for use in mobile and in-sensor applications, there has been a growing interest in quantizing synaptic weights. However, most prior work has been heuristic, lacking theoretical foundations. This study develops an approach to quantizing deep networks using functional high-rate quantization theory, which leads to an optimal quantizer that is computed using the backpropagation algorithm. In cases where certain technical conditions are not met, a heuristic quantizer can still be computed with certain regularization guarantees. The focus is on fixed-rate scalar quantization of edge weights that can be used directly for inferential computation. </Abstract:>



---

