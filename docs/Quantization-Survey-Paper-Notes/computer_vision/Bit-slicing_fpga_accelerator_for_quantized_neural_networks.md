# Bit-slicing fpga accelerator for quantized neural networks

## Summary

<Summary: > This paper proposes a new Quantized Neural Network (QNN) accelerator architecture that supports arbitrary low-precision fixed-point formats for efficient hardware performance of neural networks. The architecture enables processing of the inference pass in large neural networks in a flexible pipelined scheme that minimizes the movement of data to off-chip memory. The design implemented on a modest-sized FPGA offers high computational throughput and energy efficiency.


## Target Task

computer vision

## Content

<Abstract: >Deep Neural Networks (DNNs) are commonly used in various applications such as speech recognition and computer vision. Despite their efficiency, they require huge computational resources and thus are not suitable for embedded applications. Extreme quantization, which involves the use of low-precision and fixed-point arithmetic, is a promising path for improving the hardware performance of neural networks. In this paper, we propose a new Quantized Neural Network (QNN) accelerator architecture that supports arbitrary low-precision fixed-point formats. Our architecture enables processing of the inference pass in large neural networks while implementing an efficient streaming data flow in a flexible pipelined scheme that minimizes the movement of data to off-chip memory. The design is implemented on a modest-sized FPGA and offers high computational throughput and energy efficiency.



---

