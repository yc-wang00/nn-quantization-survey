# Harnessing object and scene semantics for large-scale video understanding

## Summary

<Summary: >This paper proposes a semantic fusion network to combine frame-based low-level CNN features with object features and scene features to improve supervised activity and video categorization on two large-scale datasets - ActivityNet and FCVID. The fusion network has the ability to discover semantic relationships between video classes and objects/scenes by examining and back propagating information through the network. Zero-shot action/video classification and clustering experiments show the effectiveness of this semantic representation.


## Target Task

computer vision

## Content

<Abstract: >Large-scale action recognition and video categorization are important computer vision problems. In this paper, the authors propose a semantic fusion network that combines three streams of information: (i) frame-based low-level CNN features, (ii) object features from a state-of-the-art large-scale CNN object-detector trained to recognize 20K classes, and (iii) scene features from a state-of-the-art CNN scene-detector trained to recognize 205 scenes. The fusion network achieves improvements in supervised activity and video categorization on two large-scale datasets - ActivityNet and FCVID. By examining and back propagating information through the fusion network, semantic relationships between video classes and objects/scenes can be discovered. The authors illustrate the effectiveness of this semantic representation through experiments on zero-shot action/video classification and clustering.



---

