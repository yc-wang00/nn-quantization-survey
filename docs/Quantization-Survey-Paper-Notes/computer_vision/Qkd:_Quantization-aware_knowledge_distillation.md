# Qkd: Quantization-aware knowledge distillation

## Summary

<Summary:> The paper proposes a new method called Quantization-aware Knowledge Distillation (QKD) that coordinates quantization and knowledge distillation in three phases to reduce memory and power consumption of deep neural networks on edge devices. The proposed method outperformed existing state-of-the-art methods and achieved full-precision accuracy at low quantization levels on ResNet and MobilenetV2.


## Target Task

computer vision

## Content

<Abstract:> Quantization and Knowledge distillation (KD) methods are used to reduce memory and power consumption of deep neural networks (DNNs) for resource-constrained edge devices. We propose Quantization-aware Knowledge Distillation (QKD) wherein quantization and KD are carefully coordinated in three phases. QKD outperformed existing state-of-the-art methods and could recover the full-precision accuracy at as low as W3A3 quantization on ResNet and W6A6 quantization on MobilenetV2.



---

