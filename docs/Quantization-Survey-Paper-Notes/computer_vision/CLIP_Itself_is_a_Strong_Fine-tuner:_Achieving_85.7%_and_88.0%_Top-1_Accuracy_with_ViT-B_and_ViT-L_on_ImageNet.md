# CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1 Accuracy with ViT-B and ViT-L on ImageNet

## Summary

<Summary: > The paper demonstrates through a comprehensive study that fine-tuning performance of CLIP is impacted by hyper-parameter choices and improves fine-tuning CLIP for classification tasks by evaluating the impact of the hyper-parameters. Equipped with hyperparameter refinement, CLIP itself is better or competitive compared with large-scale supervised pre-training approaches or latest works that use CLIP as prediction targets in Masked Image Modeling. Additionally, the observations challenge the conventional conclusion that CLIP is not suitable for fine-tuning and motivate researchers to rethink recently proposed improvements based on CLIP.


## Target Task

computer vision

## Content

<Abstract: >Recent studies have shown that CLIP has achieved remarkable success in performing zero-shot inference while its fine-tuning performance is not satisfactory. In this paper, we identify that fine-tuning performance is significantly impacted by hyper-parameter choices. We examine various key hyper-parameters and empirically evaluate their impact in fine-tuning CLIP for classification tasks through a comprehensive study. Equipped with hyper-parameter refinement, we demonstrate CLIP itself is better or at least competitive in fine-tuning compared with large-scale supervised pre-training approaches or latest works that use CLIP as prediction targets in Masked Image Modeling. CLIP ViT-Base/16 and CLIP ViT-Large/14 can achieve 85:7%;88:0% fine-tuning Top-1 accuracy on the ImageNet-1K dataset. These observations challenge the conventional conclusion that CLIP is not suitable for fine-tuning, and motivate us to rethink recently proposed improvements based on CLIP.



---

