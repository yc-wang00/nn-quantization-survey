# Monte Carlo gradient quantization

## Summary

Summary: The paper proposes Monte Carlo Gradient Quantization (MCGQ) method to leverage both sparsity and quantization for compressing gradients of neural networks during training. The proposed method shows faster convergence and better performance than existing quantization methods in image classification and language modeling. MCGQ with low-bit-width quantization and high sparsity levels surpasses existing compression methods' rates.


## Target Task

computer vision

## Content

<Abstract: We propose Monte Carlo methods to leverage both sparsity and quantization to compress gradients of neural networks throughout training. Our method, called Monte Carlo Gradient Quantization (MCGQ), shows faster convergence and higher performance than existing quantization methods on image classification and language modeling. Using both low-bit-width-quantization and high sparsity levels, our method more than doubles the rates of existing compression methods from 200× to 520× and 462× to more than 1200× on different language modeling tasks.>



---

