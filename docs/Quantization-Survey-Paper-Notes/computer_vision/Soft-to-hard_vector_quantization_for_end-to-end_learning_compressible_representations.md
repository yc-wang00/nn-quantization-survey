# Soft-to-hard vector quantization for end-to-end learning compressible representations

## Summary

<Summary: > The paper presents a new approach to deep learning model compression using a soft-to-hard quantization method that shows competitive results for both image and neural network compression tasks. The approach involves continuous relaxation of quantization and entropy, which gradually transitions to their discrete counterparts throughout the training process.


## Target Task

computer vision

## Content

<Abstract: >We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.



---

