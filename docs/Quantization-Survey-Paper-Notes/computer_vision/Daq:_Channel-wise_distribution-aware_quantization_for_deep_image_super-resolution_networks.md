# Daq: Channel-wise distribution-aware quantization for deep image super-resolution networks

## Summary

<Summary: > This paper presents a new ultra-low precision distribution-aware quantization approach called Distribution-Aware Quantization (DAQ) that is designed for image super-resolution (SR). The proposed quantization method reduces the computational and resource costs without significantly sacrificing SR performance compared to other quantization methods. The authors observed that in recent SR networks, each channel has different distribution characteristics and thus proposed a channel-wise distribution-aware quantization scheme. Results from experiments were provided to demonstrate the effectiveness of the approach.


## Target Task

computer vision

## Content

<Abstract: > Since the resurgence of deep neural networks (DNNs), image super-resolution (SR) has recently seen a huge progress in improving the quality of low resolution images, however at the great cost of computations and resources. Recently, there has been several efforts to make DNNs more efficient via quantization. However, SR demands pixel-level accuracy in the system, it is more difficult to perform quantization without significantly sacrificing SR performance. To this end, we introduce a new ultra-low precision yet effective quantization approach specifically designed for SR. In particular, we observe that in recent SR networks, each channel has different distribution characteristics. Thus we propose a channel-wise distribution-aware quantization scheme. Experimental results demonstrate that our proposed quantization, dubbed Distribution-Aware Quantization (DAQ), manages to greatly reduce the computational and resource costs without significant sacrifice in SR performance, compared to other quantization methods.



---

