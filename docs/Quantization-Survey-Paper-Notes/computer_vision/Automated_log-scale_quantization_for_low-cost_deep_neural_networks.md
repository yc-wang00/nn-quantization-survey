# Automated log-scale quantization for low-cost deep neural networks

## Summary

Summary: The paper proposes a new STLQ-aware training method for logarithmic quantization, which outperforms the previous state-of-the-art method. The proposed method has been applied to ResNet-18 weight parameters, achieving performance similar to APoT at 3-bit precision. It has also been tested on various DNNs in image enhancement and semantic segmentation and produces competitive results.


## Target Task

computer vision

## Content

<Abstract: Quantization plays an important role in deep neural network (DNN) hardware. In particular, logarithmic quantization has multiple advantages for DNN hardware implementations. This paper proposes a novel STLQ-aware training method, which significantly outperforms the previous state-of-the-art training method for STLQ. With this new training method, STLQ applied to weight parameters of ResNet-18 can achieve the same level of performance as state-of-the-art quantization method, APoT, at 3-bit precision. The method is also applied to various DNNs in image enhancement and semantic segmentation, showing competitive results.>



---

