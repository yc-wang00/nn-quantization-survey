# Mr. BiQ: Post-Training Non-Uniform Quantization Based on Minimizing the Reconstruction Error

## Summary

Summary: The paper proposes a post-training non-uniform quantization method, termed Mr.BiQ, for compressing neural networks, including Transformer models. Mr.BiQ optimizes quantization parameters jointly with the weights and allows for multiple data formats for activations. Results show improved accuracy, with up to 5.35 p.p. improvement in CNNs, up to 4.23 p.p. improvement in Vision Transformers, and up to 3.37 point improvement in Transformers for NLP.


## Target Task

computer vision

## Content

<Abstract:>
Post-training quantization is an effective method to compress a neural network, but it has only been discussed and demonstrated in uniform quantization on convolutional neural networks. This paper proposes a new post-training non-uniform quantization method, called Mr.BiQ, for low bit-width quantization even on Transformer models. The proposed method optimizes the quantization parameters directly and jointly with the weights, allowing for multiple data formats for activations. Experimental results show significant improvement in accuracy for various models, with up to 5.35 p.p. improvement in CNNs, up to 4.23 p.p. improvement in Vision Transformers, and up to 3.37 point improvement in Transformers for NLP.



---

