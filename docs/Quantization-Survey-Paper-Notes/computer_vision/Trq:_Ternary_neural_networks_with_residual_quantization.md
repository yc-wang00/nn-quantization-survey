# Trq: Ternary neural networks with residual quantization

## Summary

<Summary: >The paper proposes Ternary Residual Quantization (TRQ) which is a stem-residual framework that performs quantization on full-precision weights for a refined reconstruction by combining the binarized stem and residual parts. TRQ has high flexibility and precision which can be extended to multiple bits. The TRQ model provides for great recognition accuracy while being accelerated.


## Target Task

computer vision

## Content

<Abstract: >Ternary neural networks (TNNs) have the potential to accelerate network operations by reducing full-precision weights to ternary ones. However, existing TNNs have a significant accuracy loss due to simple thresholding operations. In this paper, we propose Ternary Residual Quantization (TRQ) to achieve more powerful TNNs with a stem-residual framework. TRQ recursively performs quantization on full-precision weights for a refined reconstruction by combining the binarized stem and residual parts. This unique quantization process endows TRQ with high flexibility and precision, and it can be easily extended to multiple bits. Experimental results demonstrate that TRQ yields great recognition accuracy while being accelerated.



---

