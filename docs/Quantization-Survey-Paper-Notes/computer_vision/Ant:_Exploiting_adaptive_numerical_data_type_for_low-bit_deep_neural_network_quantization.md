# Ant: Exploiting adaptive numerical data type for low-bit deep neural network quantization

## Summary

Summary: The paper proposes a fixed-length adaptive numerical data type called ANT that uses flint - a specific data type combining the advantages of float and int to adapt to the importance of different values within a tensor, and an adaptive framework to select the best type for each tensor based on its distribution characteristics. The unified processing element architecture for ANT shows significant improvement in speed and energy efficiency compared to the existing state-of-the-art quantization accelerators.


## Target Task

computer vision

## Content

<Abstract: >Quantization techniques are used to reduce the computation and memory cost of deep neural network (DNN) models. However, existing quantization solutions have limited benefits and require more bits to maintain the accuracy of original models. To address this, this paper proposes a fixed-length adaptive numerical data type called ANT, which leverages two key innovations to exploit the intra-tensor and inter-tensor adaptive opportunities in DNN models. First, a particular data type called flint is proposed, which combines the advantages of float and int for adapting to the importance of different values within a tensor. Second, an adaptive framework that selects the best type for each tensor according to its distribution characteristics is proposed. The authors design a unified processing element architecture for ANT and show its ease of integration with existing DNN accelerators. Their design results in 2.8 speedup and 2.5 energy efficiency improvement over the state-of-the-art quantization accelerators.



---

