# A survey of model compression and acceleration for deep neural networks

## Summary

Summary: The paper reviews recent techniques for compressing and accelerating Deep Neural Networks (DNNs) into four categories, namely parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization were thoroughly discussed before introducing the other techniques. The paper offers analysis of the performance, advantages, and drawbacks of each category and concludes by discussing the challenges and possible directions for future work.


## Target Task

computer vision

## Content

<Abstract: >Deep neural networks (DNNs) have recently achieved great success in many visual recognition tasks. However, existing deep neural network models are computationally expensive and memory intensive, hindering their deployment in devices with low memory resources or in applications with strict latency requirements. Therefore, a natural thought is to perform model compression and acceleration in deep networks without significantly decreasing the model performance. In this paper, we review the recent techniques for compacting and accelerating DNN models, divided into four categories: parameter pruning and quantization, low-rank factorization, transferred/compact convolutional filters, and knowledge distillation. Methods of parameter pruning and quantization are described first, after which the other techniques are introduced. For each category, insightful analysis about the performance, related applications, advantages and drawbacks are provided. Finally, we conclude the paper, discuss remaining the challenges and possible directions for future work.



---

