# Defend deep neural networks against adversarial examples via fixed and dynamic quantized activation functions

## Summary

<Summary: >This paper proposes a method for defending against adversarial attacks by using quantization of activation functions. The authors also introduce a technique called Dynamic Quantized Activation (DQA) for training robust neural networks. Experiments using the MNIST and CIFAR-10 datasets show that the proposed method significantly improves the robustness of DNNs against various attack methods. This is the first work to apply quantization of activation functions to defend against adversarial examples.


## Target Task

computer vision

## Content

<Abstract: >Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks. In this work, we propose to defend DNNs against adversarial examples using quantization of activation functions. We also propose a Dynamic Quantized Activation (DQA) technique for training robust neural networks. Our experiments with the MNIST and CIFAR-10 datasets under different white-box and black-box attack methods including FGSM, PGD, and C&W attacks demonstrate that the robustness of DNNs can be greatly improved using the proposed DQA. This is the first work to use quantization of activation functions to defend against adversarial examples.



---

