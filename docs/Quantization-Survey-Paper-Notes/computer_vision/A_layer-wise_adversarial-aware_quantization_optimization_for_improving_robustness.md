# A layer-wise adversarial-aware quantization optimization for improving robustness

## Summary

<Summary: >The paper proposes a layer-wise adversarial-aware quantization method to improve the robustness of quantized adversarially-trained neural networks by minimizing adversarial and quantization losses simultaneously. The method uses Lipschitz constant to choose the best quantization parameter settings for a neural network. The proposed method is consistent with proven metrics and effectively improves the robustness of quantized adversarially-trained neural networks.


## Target Task

computer vision

## Content

<Abstract: >In this work, we propose a layer-wise adversarial-aware quantization method to improve the robustness of quantized adversarially-trained neural networks by simultaneously minimizing adversarial and quantization losses. Recent research has found that adversarially-trained neural networks are more vulnerable to quantization loss than plain models. Our method uses Lipschitz constant to select the best quantization parameter settings for a neural network. We derive the losses and prove the consistency of our metric selection theoretically. Experimental results show that our method can effectively and efficiently improve the robustness of quantized adversarially-trained neural networks.



---

