# A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer

## Summary

Summary: The paper introduces a self-supervised pre-training framework, known as u-HuBERT, which can utilize both unimodal and multimodal speech for a unified masked cluster prediction objective. The authors demonstrate that their modality dropout technique during pre-training can lead to a single fine-tuned model outperforming state-of-the-art modality-specific models. Additionally, their model can achieve zero-shot modality generalization for various speech processing tasks when fine-tuned only on audio.


## Target Task

speech recognition

## Content

<Abstract: >In this paper, the authors present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. They utilize modality dropout during pre-training to demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, their model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for multiple speech processing tasks.



---

