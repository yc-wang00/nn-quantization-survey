# Fixed-point performance analysis of recurrent neural networks

## Summary

Summary: This paper proposes a quantization method based on retraining for analyzing the fixed-point performance of recurrent neural networks (RNNs). The paper studies the quantization sensitivity of each layer of RNNs and presents optimization results to minimize the capacity of weights while maintaining performance. A language model and a phoneme recognition example are used to demonstrate the proposed scheme. Layer-wise fixed-point sensitivity analysis results are also provided.


## Target Task

speech recognition

## Content

<Abstract:>
Recurrent neural networks (RNNs) have been successful in many applications, however, their implementation often requires increased complexity in hardware and software. This paper presents an analysis of the fixed-point performance of recurrent neural networks using a quantization method based on retraining. The work studies the quantization sensitivity of each layer in RNNs and presents overall fixed-point optimization results that minimize the capacity of weights while maintaining performance. A language model and a phoneme recognition example are used to demonstrate the proposed scheme. The paper provides an overview of the proposed quantization procedure for weights and signals, as well as layer-wise fixed-point sensitivity analysis results.



---

