# CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations

## Summary

Summary: The paper proposes a new pre-training strategy called ccc-wav2vec 2.0 that utilizes clustering and augmentation based cross-contrastive loss as its self-supervised objective. The proposed method achieves up to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on the test-clean and test-other sets respectively of LibriSpeech without the use of any language model. The paper also states that the proposed method shows up to 14.9% relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on Switchboard data.


## Target Task

speech recognition

## Content

<Abstract: >While Self-Supervised Learning has helped reap the benefit of the scale from the available unlabeled data, the learning paradigms are continuously being bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which uses clustering and an augmentation based cross-contrastive loss as its self-supervised objective. ccc-wav2vec 2.0 achieves up to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on the test-clean and test-other sets respectively of LibriSpeech, without the use of any language model. The proposed method also achieves up to 14.9% relative WER improvement over the baseline wav2vec 2.0, when fine-tuned on Switchboard data.



---

