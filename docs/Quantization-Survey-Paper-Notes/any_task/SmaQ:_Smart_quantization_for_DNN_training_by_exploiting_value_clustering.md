# SmaQ: Smart quantization for DNN training by exploiting value clustering

## Summary

Summary: The paper discusses the issue of memory requirement during neural network training and introduces Smart Quantization (SmaQ), a quantization scheme that exploits the observed normal distribution to quantize data structures resulting in up to 6.7x reduction in memory usage during training with minimal accuracy loss.


## Target Task

any task

## Content

<Abstract: Advancements in modern deep learning have shown that deeper networks with larger datasets can achieve state of the art results in many different tasks. As networks become deeper, the memory requirement of neural network training proves to be the primary bottleneck of single-machine training. In this paper, we ﬁrst study the characteristics of neural network weight, gradient, feature map, gradient map, and optimizer state distributions for some popular neural network architectures. Our investigation shows that the majority of the data structures used by neural networks can have their value distributions be approximated with normal distributions. We then introduce Smart Quantization (SmaQ), a quantization scheme that exploits this observed normal distribution to quantize the data structures. Our dynamic quantization method calculates the sampled mean and standard deviation of tensors and quantizes each tensor element to 6 or 8 bits based on the z-score of that value. Our scheme reduces the memory usage during training by up to 6.7x with minor losses in accuracy.>



---

