# Loss-aware weight quantization of deep networks

## Summary

<Summary: >This paper suggests a method for compressing deep neural networks by weight quantization. They propose a scheme that extends loss-aware weight binarization to ternarization and m-bit quantization. Experimental results demonstrate that this approach outperforms existing weight quantization algorithms while maintaining accuracy comparable to full-precision networks.


## Target Task

any task

## Content

<Abstract: >The huge size of deep networks hinders their use in small computing devices. In this paper, we consider compressing the network by weight quantization. We extend a recently proposed loss-aware weight binarization scheme to ternarization, with possibly different scaling parameters for the positive and negative weights, and m-bit (where m > 2) quantization. Experiments on feedforward and recurrent neural networks show that the proposed scheme outperforms state-of-the-art weight quantization algorithms, and is as accurate (or even more accurate) than the full-precision network.



---

