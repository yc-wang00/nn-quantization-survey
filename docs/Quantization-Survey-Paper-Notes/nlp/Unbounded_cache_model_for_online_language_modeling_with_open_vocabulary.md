# Unbounded cache model for online language modeling with open vocabulary

## Summary

Summary: The paper proposes an extension to continuous cache models that can adapt predictions to local changes in data distribution. The extension can scale to larger contexts and uses a non-parametric memory component to store all the hidden activations seen in the past. The approach significantly improves the perplexity of pre-trained language models on new distributions and can scale efficiently to much larger contexts than previously proposed local cache models.


## Target Task

nlp

## Content

<Abstract: >
In this paper, the authors propose an extension to continuous cache models used to adapt predictions to local changes in data distribution. This extension can scale to larger contexts and uses a non-parametric memory component to store all the hidden activations seen in the past. The authors conduct experiments showing that this approach significantly improves the perplexity of pre-trained language models on new distributions and can scale efficiently to much larger contexts than previously proposed local cache models.



---

