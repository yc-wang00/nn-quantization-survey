# SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning

## Summary

Summary: The paper proposes SPT, a semi-parametric prompt tuning method for multitask prompted learning, which uses a memory bank for memory prompts based on discrete prompts. The effectiveness of SPT is demonstrated through experiments in fine-tuning a full language model with SPT on 31 different tasks and evaluating zero-shot generalization on 9 heldout datasets and pretraining SPT on the GLUE datasets and evaluating fine-tuning on the SuperGLUE datasets.


## Target Task

nlp

## Content

<Abstract:>
Pre-trained large language models can interpolate human-written prompts in a natural way. Multitask prompted learning can enhance the potential for more effective downstream fine-tuning. Prompt tuning has been proposed for efficient multitask-inference in the same batch but existing methods may lack generalization. This paper proposes SPT, a semi-parametric prompt tuning method for multitask prompted learning, which uses a memory bank for memory prompts based on discrete prompts. The effectiveness of SPT is demonstrated through experiments in fine-tuning a full language model with SPT on 31 different tasks and evaluating zero-shot generalization on 9 heldout datasets and pretraining SPT on the GLUE datasets and evaluating fine-tuning on the SuperGLUE datasets.



---

