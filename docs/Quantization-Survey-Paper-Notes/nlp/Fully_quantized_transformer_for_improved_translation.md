# Fully quantized transformer for improved translation

## Summary

<Summary: > The paper proposes a quantization strategy inclusive of all components of the Transformer, and it is the first to show that it is possible to avoid any loss in translation quality with a fully quantized network. The 8-bit models consistently score equal or higher BLEU than the full-precision variant on multiple translation datasets, and achieved state-of-the-art quantization results when compared to all previously proposed methods.


## Target Task

nlp

## Content

<Abstract: > State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To the best of our knowledge, we are the ﬁrst to propose a quantization strategy inclusive of all components of the Transformer (Vaswani et al., 2017). We are also the ﬁrst to show that it is possible to avoid any loss in translation quality with a fully quantized network. Indeed, our 8-bit models consistently score equal or higher BLEU than the full-precision variant on multiple translation datasets. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results.



---

