# A compact and language-sensitive multilingual translation method

## Summary

Summary: The paper proposes a compact and language-sensitive method for multilingual translation using a universal representor to replace both encoder and decoder models, and introducing language-sensitive embedding, attention, and discriminator. This method outperforms strong standard multilingual translation systems in various scenarios, including one-to-many, many-to-many, and zero-shot, especially in low-resource and zero-shot translation scenarios.


## Target Task

nlp

## Content

<Abstract: Multilingual neural machine translation (Multi-NMT) has made remarkable progress due to its simple deployment. However, this paradigm has limitations as it does not make full use of language commonality and parameter sharing between encoder and decoder. In this paper, we propose a compact and language-sensitive method for multilingual translation, using a universal representor to replace both encoder and decoder models, and introducing language-sensitive embedding, attention, and discriminator. Our proposed methods outperform strong standard multilingual translation systems on various translation scenarios, including one-to-many, many-to-many, and zero-shot, especially in low-resource and zero-shot translation scenarios.>



---

