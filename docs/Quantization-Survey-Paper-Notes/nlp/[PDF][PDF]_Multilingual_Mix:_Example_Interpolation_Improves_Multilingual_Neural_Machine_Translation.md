# [PDF][PDF] Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation

## Summary

<Summary: > The paper introduces multilingual crossover encoder-decoder (mXEncDec), which fuses language pairs at an instance level by interpolating instances from different language pairs into joint 'crossover examples'. The proposed approach encourages sharing input and output spaces across languages and includes several techniques to improve example interpolation across dissimilar languages under heavy data imbalance. Experiments on a larhe-scale WMT multilingual dataset showed a significant improvement in quality on English-to-Many, Many-to-English, and zero-shot translation tasks ranging from +0.5 BLEU up to +5.5 BLEU points.


## Target Task

nlp

## Content

<Abstract: > Multilingual neural machine translation models are trained to maximize the likelihood of a mix of examples drawn from multiple language pairs. In this paper, the authors introduce multilingual crossover encoder-decoder (mXEncDec) to fuse language pairs at an instance level. Their approach interpolates instances from different language pairs into joint ‘crossover examples’ in order to encourage sharing input and output spaces across languages. They propose several techniques to improve example interpolation across dissimilar languages under heavy data imbalance. Experiments on a large-scale WMT multilingual dataset demonstrate that their approach significantly improves quality on English-to-Many, Many-to-English and zero-shot translation tasks (from +0.5 BLEU up to +5.5 BLEU points).



---

