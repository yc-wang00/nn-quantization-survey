# The case for 4-bit precision: k-bit Inference Scaling Laws

## Summary

<Summary: > This paper studies the trade-off between reduced memory footprint and inference latency against accuracy for Large Language Models (LLMs) through quantization methods. The study found that 4-bit precision is almost always optimal for total model bits and zero-shot accuracy across all model scales and model families tested, indicating that reducing model bits through quantization can lower latency while maintaining optimal accuracy.


## Target Task

nlp

## Content

<Abstract: > Quantization methods are widely used to reduce the number of bits required to represent each parameter in a model. In this paper, the trade-off between reduced memory footprint and inference latency against accuracy is studied for Large Language Models (LLMs) using inference scaling laws of zero-shot performance. The study found that 4-bit precision is almost universally optimal for total model bits and zero-shot accuracy across all model scales and model families tested. The findings suggest that reducing the model bits through quantization can potentially reduce the latency of the model while maintaining optimal accuracy.



---

