# Massive Language Models Can Be Accurately Pruned in One-Shot

## Summary

Summary: Large-scale generative pre-trained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot without any retraining, and minimal loss of accuracy. A new pruning method called SparseGPT is designed to efficiently and accurately prune massive GPT-family models.


## Target Task

nlp

## Content

<Abstract:>
We show for the Ô¨Årst time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining , at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. The code is available at: https://github.com/IST-DASLab/sparsegpt.



---

