# Compression of deep learning models for text: A survey

## Summary

Summary: This survey discusses the compression of deep learning models used for Natural Language Processing (NLP) and Information Retrieval (IR) tasks for real-world applications. The authors mention various models such as BERT, GPT-2, MT-DNN, XLNet, T5, T-NLG, and GShard and list six different methods for compression such as pruning, quantization, knowledge distillation, parameter sharing, tensor decomposition, and sub-quadratic transformer based methods in order to achieve small model size and low response times. The survey aims to present a coherent story of the work done by the deep learning for NLP community in the past few years.


## Target Task

nlp

## Content

<Abstract: > In this survey, the authors discuss the methods of compressing deep learning models used for natural language processing (NLP) and information retrieval (IR) tasks. The authors mention the various types of models which have made significant contributions to this field such as BERT, GPT-2, MT-DNN, XLNet, T5, T-NLG, and GShard. But, in real-world applications, small model size and low response times are required. Hence, the authors discuss six different types of methods for compression of such models, which include pruning, quantization, knowledge distillation, parameter sharing, tensor decomposition, and sub-quadratic transformer based methods. The authors believe that this survey organizes the plethora of work done by the ‘deep learning for NLP’ community in the past few years and presents it as a coherent story.



---

