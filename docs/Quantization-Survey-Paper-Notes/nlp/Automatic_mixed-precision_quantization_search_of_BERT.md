# Automatic mixed-precision quantization search of BERT

## Summary

Summary: The paper proposes an automatic mixed-precision quantization framework for BERT which conducts quantization and pruning in a subgroup-wise level. This approach leads to a much smaller model size without sacrificing performance, making it more practical for deployment on resource-constrained devices.


## Target Task

nlp

## Content

<Abstract: Pre-trained language models such as BERT have shown remarkable effectiveness in various natural language processing tasks. However, these models usually contain millions of parameters, which prevents them from practical deployment on resource-constrained devices. In this paper, we proposed an automatic mixed-precision quantization framework designed for BERT that can simultaneously conduct quantization and pruning in a subgroup-wise level. Our proposed method outperforms baselines by providing the same performance with much smaller model size.>



---

