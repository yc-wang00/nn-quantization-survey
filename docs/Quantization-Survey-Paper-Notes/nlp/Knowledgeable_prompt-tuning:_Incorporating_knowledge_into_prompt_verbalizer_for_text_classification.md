# Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification

## Summary

Summary: The paper proposes knowledgeable prompt-tuning (KPT) to improve and stabilize prompt-tuning for text classification by incorporating external knowledge into the verbalizer through expansion and refinement of the label word space. The approach is evaluated through extensive experiments on zero and few-shot text classification tasks, showing its effectiveness.


## Target Task

nlp

## Content

Abstract: Tuning pre-trained language models with task-specific prompts has been a promising approach for text classification. In this work, the authors propose a knowledgeable prompt-tuning (KPT) approach to improve and stabilize prompt-tuning by incorporating external knowledge into the verbalizer. The authors expand the label word space of the verbalizer using external knowledge bases (KBs) and refine the expanded label word space with the PLM itself before predicting with the expanded label word space. Extensive experiments on zero and few-shot text classification tasks demonstrate the effectiveness of knowledgeable prompt-tuning.



---

