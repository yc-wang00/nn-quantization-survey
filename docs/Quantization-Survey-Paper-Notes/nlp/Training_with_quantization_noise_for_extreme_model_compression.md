# Training with quantization noise for extreme model compression

## Summary

Summary: The paper proposes an extension to Quantization Aware Training approach to produce compact models with high accuracy, allowing unbiased gradients to flow through different weights during each forward. This approach achieves a new state-of-the-art compromise between accuracy and model size in natural language processing and image classification by demonstrating it through Transformer and ConvNet architectures.


## Target Task

nlp

## Content

<Abstract: >We tackle the problem of producing compact models with high accuracy and propose an extension to the Quantization Aware Training approach. Our approach allows for unbiased gradients to flow through different weights during each forward, enabling extreme compression rates while maintaining performance. The approach is demonstrated through state-of-the-art Transformer and ConvNet architectures, achieving a new state-of-the-art compromise between accuracy and model size in natural language processing and image classification.



---

