# Beyond preserved accuracy: Evaluating loyalty and robustness of BERT compression

## Summary

Summary: This paper proposes two new metrics for evaluating compressed language models that measure how closely the compressed model mimics the original model. The paper also examines the effects of compression on robustness under adversarial attacks.


## Target Task

nlp

## Content

<Abstract: Recent studies on compression of pretrained language models (e.g., BERT) usually use preserved accuracy as the metric for evaluation. In this paper, we propose two new metrics, label loyalty and probability loyalty that measure how closely a compressed model (i.e., student) mimics the original model (i.e., teacher). We also explore the effect of compression with regard to robustness under adversarial attacks.>



---

