# Meta-KD: A meta knowledge distillation framework for language model compression across domains

## Summary

<Summary: >This paper proposes a Meta-Knowledge Distillation (Meta-KD) framework for compressing pre-trained language models (PLMs) with acceptable performance loss. The Meta-KD framework uses a meta-teacher model with transferable knowledge across domains to distill cross-domain knowledge to students. The experiments demonstrate the effectiveness and superiority of Meta-KD on public multi-domain NLP tasks even when the training data is scarce.


## Target Task

nlp

## Content

<Abstract: >Pre-trained language models have been applied to various NLP tasks, but their large sizes and long inference times limit their deployment in real-time applications. Knowledge distillation (KD) is one of the promising ways to compress PLMs with acceptable performance loss; however, most KD approaches focus on single-domain KD without considering cross-domain knowledge. In this paper, we propose a Meta-Knowledge Distillation (Meta-KD) framework that builds a meta-teacher model with transferable knowledge across domains to distill such knowledge to students. Experiments show the effectiveness and superiority of Meta-KD on public multi-domain NLP tasks even when the training data is scarce.



---

