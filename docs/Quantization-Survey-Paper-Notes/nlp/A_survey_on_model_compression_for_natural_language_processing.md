# A survey on model compression for natural language processing

## Summary

Summary: This paper discusses the challenges in using Transformer-based pretrained language models (PLMs) for edge and mobile computing due to high energy cost and inference delay. The paper reviews the state of model compression and acceleration for PLMs, highlighting benchmarks, metrics and methodology, as part of efforts to consider computation, time and carbon emission in NLP.


## Target Task

nlp

## Content

<Abstract: Despite achieving state-of-the-art performance on many NLP
tasks, the high energy cost and long inference delay prevent
Transformer-based pretrained language models (PLMs) from
seeing broader adoption including for edge and mobile com-
puting. Efﬁcient NLP research aims to comprehensively con-
sider computation, time and carbon emission for the entire
life-cycle of NLP, including data preparation, model training
and inference. In this survey, we focus on the inference stage
and review the current state of model compression and accel-
eration for pretrained language models, including benchmarks,
metrics and methodology.>



---

