# ERNIE-ViLG: Unified generative pre-training for bidirectional vision-language generation

## Summary

<Summary: >The paper proposes ERNIE-ViLG, a generative pre-training framework for bidirectional image-text generation using transformer models. The framework explores large-scale pre-training, achieving state-of-the-art performance on text-to-image synthesis and image captioning tasks, with an FID of 7.9 on MS-COCO.


## Target Task

nlp

## Content

<Abstract: >In this paper, we propose ERNIE-ViLG, a unified generative pre-training framework for bidirectional image-text generation with transformer model. The bidirectional image-text generative modeling eases the semantic alignments across vision and language. To explore the landscape of large-scale pre-training for bidirectional text-image generation, we train a 10-billion parameter ERNIE-ViLG model on a large-scale dataset of 145 million (Chinese) image-text pairs which achieves state-of-the-art performance for both text-to-image and image-to-text tasks, obtaining an FID of 7.9 on MS-COCO for text-to-image synthesis and best results on COCO-CN and AIC-ICC for image captioning.



---

