# Hptq: Hardware-friendly post training quantization

## Summary

<Summary: > This paper proposes a hardware-friendly post-training quantization (HPTQ) framework for neural network quantization. The goal is to deploy models on edge devices, and the success of the deployment depends largely on hardware-friendly quantizers with uniform, symmetric and power-of-two thresholds. HPTQ leverages several known quantization methods to satisfy these constraints and is evaluated on four tasks across different network architectures.


## Target Task

Target: Computer vision and edge device deployment.

## Content

<Abstract: > Neural network quantization is an effective approach to deploy models on edge devices. However, the success of such deployment largely depends on the quantizers being hardware-friendly with uniform, symmetric, and power-of-two thresholds. Current post-training quantization methods do not support all these constraints simultaneously. In this paper, the authors propose a hardware-friendly post-training quantization (HPTQ) framework which leverages several known quantization methods to obtain competitive results while adhering to these constraints. The effectiveness of HPTQ is evaluated on four tasks across a variety of network architectures.



---

