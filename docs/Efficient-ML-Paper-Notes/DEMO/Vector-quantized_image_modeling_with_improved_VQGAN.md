# Vector-quantized image modeling with improved VQGAN

## Summary

<Summary: > The paper proposes a Vector-quantized Image Modeling approach, where a Transformer is pre-trained to predict image tokens autoregressively using a learned Vision-Transformer-based VQGAN to encode discrete image tokens. The approach is inspired by the success of pretraining language models on massive text corpora for zero-shot and few-shot capabilities in language tasks.


## Target Task

Target: NLP (Natural Language Processing)

## Content

<Abstract: > Pretraining language models with next-token prediction on massive text corpora has delivered phenomenal zero-shot, few-shot, transfer learning and multi-tasking capabilities on both generative and discriminative language tasks. Motivated by this success, we explore a Vector-quantized Image Modeling ( VIM ) approach that involves pretraining a Transformer to predict rasterized image tokens autoregressively. The discrete image tokens are encoded from a learned Vision-Transformer-based VQGAN ( ViT-VQGAN ).



---

