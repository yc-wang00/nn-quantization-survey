# Unbounded cache model for online language modeling with open vocabulary

## Summary

<Summary: >The paper proposes an extended version of cache models for language modeling that can capture more significant contexts. They use a non-parametric memory component and quantization algorithms to store all hidden activations and improve the perplexity of pre-trained language models on various distributions while scaling to more significant contexts. The approach has been validated through extensive experiments.


## Target Task

Target: NLP (language modeling)

## Content

<Abstract: >In this paper, the authors propose an extension of continuous cache models for language modeling, which can capture larger contexts. They use a large scale non-parametric memory component that stores all the hidden activations seen in the past and leverages recent advances in approximate nearest neighbor search and quantization algorithms. The authors conduct extensive experiments to show that their approach improves the perplexity of pre-trained language models on new distributions and can scale efficiently to much larger contexts.



---

