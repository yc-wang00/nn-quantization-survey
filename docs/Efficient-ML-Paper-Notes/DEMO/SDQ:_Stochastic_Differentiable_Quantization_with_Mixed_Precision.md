# SDQ: Stochastic Differentiable Quantization with Mixed Precision

## Summary

<Summary: > The paper presents a new Stochastic Differentiable Quantization (SDQ) method that can automatically learn the MPQ strategy, which outperforms all state-of-the-art mixed or single precision quantization with a lower bitwidth and even better than the full-precision counterparts across various ResNet and MobileNet families. SDQ has been extensively tested on several networks on different hardware (GPUs and FPGA) and datasets.


## Target Task

Target: Computer Vision

## Content

<Abstract: > In this work, we present a novel Stochastic Differentiable Quantization (SDQ) method that can automatically learn the MPQ strategy in a more flexible and globally-optimized space with smoother gradient approximation. We extensively evaluate our method for several networks on different hardware (GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or single precision quantization with a lower bitwidth and is even better than the full-precision counterparts across various ResNet and MobileNet families, demonstrating its effectiveness and superiority.



---

